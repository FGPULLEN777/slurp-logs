{"nick":"PerilousApricot","reason":"Remote host closed the connection","date":"2016-07-21T00:04:54.001Z","type":"quit"}
{"nick":"PerilousApricot","date":"2016-07-21T00:06:48.172Z","type":"join"}
{"nick":"PerilousApricot","reason":"Ping timeout: 240 seconds","date":"2016-07-21T00:11:11.435Z","type":"quit"}
{"nick":"importantshock","date":"2016-07-21T00:25:54.756Z","type":"join"}
{"nick":"qard-appneta","reason":"Quit: (╯°□°）╯︵ pɹɐb","date":"2016-07-21T00:31:58.823Z","type":"quit"}
{"nick":"rmg_","date":"2016-07-21T00:37:09.749Z","type":"join"}
{"nick":"importantshock","reason":"Remote host closed the connection","date":"2016-07-21T00:41:48.750Z","type":"quit"}
{"nick":"rmg_","reason":"Ping timeout: 276 seconds","date":"2016-07-21T00:42:09.703Z","type":"quit"}
{"nick":"zhangyq","date":"2016-07-21T00:53:49.046Z","type":"join"}
{"nick":"brson","reason":"Quit: leaving","date":"2016-07-21T00:57:54.053Z","type":"quit"}
{"nick":"zju_x","reason":"Remote host closed the connection","date":"2016-07-21T01:06:47.446Z","type":"quit"}
{"nick":"tunniclm_","reason":"Ping timeout: 244 seconds","date":"2016-07-21T01:16:10.065Z","type":"quit"}
{"nick":"importantshock","date":"2016-07-21T01:42:38.858Z","type":"join"}
{"nick":"importantshock","reason":"Ping timeout: 276 seconds","date":"2016-07-21T01:48:27.745Z","type":"quit"}
{"nick":"roxlu","reason":"Ping timeout: 276 seconds","date":"2016-07-21T02:02:45.719Z","type":"quit"}
{"nick":"rgrinberg","reason":"Ping timeout: 240 seconds","date":"2016-07-21T02:04:31.169Z","type":"quit"}
{"nick":"roxlu","date":"2016-07-21T02:09:22.170Z","type":"join"}
{"nick":"happy-dude","reason":"Quit: Connection closed for inactivity","date":"2016-07-21T02:21:03.596Z","type":"quit"}
{"nick":"rgrinberg","date":"2016-07-21T02:32:21.714Z","type":"join"}
{"nick":"rmg_","date":"2016-07-21T02:38:39.222Z","type":"join"}
{"nick":"rmg_","reason":"Ping timeout: 272 seconds","date":"2016-07-21T02:43:44.289Z","type":"quit"}
{"nick":"Ralith_","message":"Dirkson: I strongly suspect your program will have undesirable behavior even in the unlikely event that you do get all the mutex wrangling exactly correct","date":"2016-07-21T02:53:02.138Z","type":"message"}
{"nick":"Ralith_","message":"you need to fundamentally redesign it so that you don't have several threads trying to munge the same data simultaneously.","date":"2016-07-21T02:53:21.866Z","type":"message"}
{"nick":"Ralith_","message":"because indeed if they're blocking on eachother all the time then they *aren't* munging data simultaneously, so it might as well be a single thread.","date":"2016-07-21T02:54:10.636Z","type":"message"}
{"nick":"Ralith_","message":"I don't know exactly what problem you did this in an attempt to solve, but you should strongly consider moving all operations on this data into a single thread and then, if and only if necessary, dividing up non-overlapping segments of work using a thread pool","date":"2016-07-21T02:56:18.663Z","type":"message"}
{"nick":"Dirkson","message":"Ralith_: Mostly I have a bunch of read threads, with the rare (and usually rather heavy) need to write. As a result, I've designed a system of rwlocks. Mostly it works fine - There are only two outstanding threading errors I'm aware of currently. And based on the section of code I'm fuddling around in, I'm starting to suspect that they're actually the same single error popping out at a couple separate seams.","date":"2016-07-21T02:56:19.607Z","type":"message"}
{"nick":"Ralith_","message":"that type of architecture is almost never the correct solution","date":"2016-07-21T02:56:48.897Z","type":"message"}
{"nick":"Ralith_","message":"why are you doing that?","date":"2016-07-21T02:57:11.774Z","type":"message"}
{"nick":"Dirkson","message":"Ralith_: Which bit did you mean to indicate with 'that'?","date":"2016-07-21T02:57:38.062Z","type":"message"}
{"nick":"Ralith_","message":"using large numbers of threads which simultaneously access a large amount of shared state","date":"2016-07-21T02:57:53.520Z","type":"message"}
{"nick":"Dirkson","message":"Because I have large data structures that numerous threads need to be able to access random sections of quite often. ","date":"2016-07-21T02:59:12.006Z","type":"message"}
{"nick":"Ralith_","message":"what makes you think you need that?","date":"2016-07-21T02:59:25.435Z","type":"message"}
{"nick":"Dirkson","message":"I am unusre what question you are asking. I ran of out of cpu to do the math I wanted to do?","date":"2016-07-21T03:00:09.663Z","type":"message"}
{"nick":"Ralith_","message":"distributing math across multiple cores requires neither large numbers of threads nor significant amounts of shared state","date":"2016-07-21T03:00:50.092Z","type":"message"}
{"nick":"Dirkson","message":"Yeah, then I don't understand the question you've asked.","date":"2016-07-21T03:01:06.451Z","type":"message"}
{"nick":"Ralith_","message":"I am trying to understand what lead you to this design so I can explain a better way in terms you can directly relate to","date":"2016-07-21T03:02:04.419Z","type":"message"}
{"nick":"Dirkson","message":"Ralith_: Yes! I kinda get that, and I'm trying to be helpful as a result :D ","date":"2016-07-21T03:02:33.511Z","type":"message"}
{"nick":"Ralith_","message":"would I be right to say that you started with a design that could have been single threaded, but it wasn't fast enough (at least in theory) so you divided up work into a bunch of different threads?","date":"2016-07-21T03:03:46.323Z","type":"message"}
{"nick":"Dirkson","message":"Ralith_: Ok, so I'm making a space game. Voxel space ships. Each ship is a -huge- amount of data. Even with compression techniques, a large ship could eat up a gigabyte of ram. I can't copy the entire ship. I have lots of operations that need to be done to ships - Preparing them for rendering is a huge one. There's also a good deal of simulation math. Taking user input, and seeing if that causes any changes to","date":"2016-07-21T03:06:13.237Z","type":"message"}
{"nick":"Dirkson","message":"the ships. Taking network information, and applying it to the ships. I could probably aggregate all changes into a thread that 'owns' the ships, but aggregating all the reads would rapidly turn into a nightmare, and destroy performance. ","date":"2016-07-21T03:06:15.008Z","type":"message"}
{"nick":"Ralith_","message":"have you thought about whta happens if a read operation (say, rendering) sees, in a single pass, part of a ship before a write operation has taken place, and another part after?","date":"2016-07-21T03:08:26.690Z","type":"message"}
{"nick":"Dirkson","message":"Ralith_: Generally speaking, some minor math inconsistencies, or 1/60th of a second of delayed rendering of the affected object. Assuming I don't bork up the mutexes :D","date":"2016-07-21T03:09:50.066Z","type":"message"}
{"nick":"Ralith_","message":"what if the write operation involves some material moving from an area controlled by one mutex to another? a read operation to e.g. compute total mass, or enable the user to pick up an object, might see the material doubled, or missing entirely","date":"2016-07-21T03:11:37.268Z","type":"message"}
{"nick":"Ralith_","message":"(to say nothing of inertia tensors and so forth)","date":"2016-07-21T03:12:07.347Z","type":"message"}
{"nick":"zhangyq_","date":"2016-07-21T03:14:13.447Z","type":"join"}
{"nick":"zhangyq","reason":"Ping timeout: 244 seconds","date":"2016-07-21T03:14:28.982Z","type":"quit"}
{"nick":"zhangyq_","new_nick":"zhangyq","date":"2016-07-21T03:14:38.023Z","type":"nick"}
{"nick":"Dirkson","message":"Ralith_: Heat springs to mind. That'd move from block to block, potentially over current chunk-mutex bounds. It's difficult to imagine how I'd lock the mutexes without introducing deadlocks, or allowing for occasional undefined behavior between unlocking one mutex and locking another. I probably haven't been seeing these effects because I have few things that travel from one chunk to another.","date":"2016-07-21T03:18:20.946Z","type":"message"}
{"nick":"Ralith_","message":"avoiding this type of error is indeed not really feasible with your current massively-shared-state design, and is one of several reasons why such a design should be avoided","date":"2016-07-21T03:19:52.911Z","type":"message"}
{"nick":"Ralith_","message":"instead, you can transpose the problem","date":"2016-07-21T03:20:32.140Z","type":"message"}
{"nick":"Ralith_","message":"allocate sufficient memory for two copies of the time-varying aspects of your world state","date":"2016-07-21T03:21:25.637Z","type":"message"}
{"nick":"Ralith_","message":"construct a single simulation thread, which will proceed in discrete timesteps","date":"2016-07-21T03:22:23.106Z","type":"message"}
{"nick":"Ralith_","message":"construct a worker thread pool, having a number of threads roughly equal to the number of CPUs available on the system","date":"2016-07-21T03:22:56.310Z","type":"message"}
{"nick":"Ralith_","message":"each time step, the simulation thread computes a new world state based on the old world state, by reading the old world state and writing the updated state elsewhere","date":"2016-07-21T03:24:44.965Z","type":"message"}
{"nick":"Ralith_","message":"note that rendering of the \"old\" world state can take place simultaneous to this, if desired, because you are not writing to it","date":"2016-07-21T03:25:22.218Z","type":"message"}
{"nick":"Dirkson","message":"This is roughly the approach I was taking with my heat simulation, writ-large across the sky. ","date":"2016-07-21T03:25:36.817Z","type":"message"}
{"nick":"Dirkson","message":"With some tweaks, I suspect it could work very well indeed.","date":"2016-07-21T03:25:58.534Z","type":"message"}
{"nick":"Ralith_","message":"to take full advantage of available CPU cores, you can divide up the \"compute a new world state\" operation into discrete units, which you submit to a job queue for execution by the thread pool","date":"2016-07-21T03:26:18.950Z","type":"message"}
{"nick":"Ralith_","message":"each discrete unit will be responsible for writing to a specific part of the new world state, such that no two distinct units will ever write to the same location","date":"2016-07-21T03:26:51.173Z","type":"message"}
{"nick":"Dirkson","message":"Yup.","date":"2016-07-21T03:27:03.695Z","type":"message"}
{"nick":"Ralith_","message":"because the previous world state is separate and not being modified, any work unit can read any location in it freely","date":"2016-07-21T03:27:16.298Z","type":"message"}
{"nick":"Ralith_","message":"when the new world state has been computed, and any rendering or other work accessing the old world state has finished, you swap new/old labels and start again","date":"2016-07-21T03:27:50.624Z","type":"message"}
{"nick":"Ralith_","message":"tihs allows you to take full advantage of SMP hardware without requiring any shared writable state or mutexes except the trivial amount necessary to manage the work queue for the thread pool","date":"2016-07-21T03:28:46.467Z","type":"message"}
{"nick":"Ralith_","message":"and it ensures that all portions of the system always see a single internally-consistent view of the world","date":"2016-07-21T03:29:37.164Z","type":"message"}
{"nick":"Ralith_","message":"not only is this drastically less fragile and better-behaved than continuously munging a single shared state, it's also got a lot less overhead, assuming the size of your work units isn't completely off (and you can tune that adaptively if you like)","date":"2016-07-21T03:30:05.742Z","type":"message"}
{"nick":"Dirkson","message":"Ralith_: I associate this style of parallelism with openmp. Should I?","date":"2016-07-21T03:31:33.747Z","type":"message"}
{"nick":"Ralith_","action":"shrug","date":"2016-07-21T03:31:48.426Z","type":"action"}
{"nick":"Ralith_","message":"I would never use openmp for gamedev, but it's not a totally ridiculous analogy","date":"2016-07-21T03:32:09.164Z","type":"message"}
{"nick":"Ralith_","message":"this approach will give you an especially visible performance improvement in cases where you have a lot of work to be done that requires accessing a single region, e.g. a ship, since units of simulation are never blocked by eachother","date":"2016-07-21T03:33:07.397Z","type":"message"}
{"nick":"Ralith_","message":"you'd manage user input by queueing it up as received and consuming the whole queue at the beginning of each timestep","date":"2016-07-21T03:33:41.723Z","type":"message"}
{"nick":"Dirkson","message":"Aye. It also forces me to parallelize other sections of the game that could probably use it, such as preparing ships for rendering.","date":"2016-07-21T03:33:56.758Z","type":"message"}
{"nick":"Ralith_","message":"it allows you to, it certainly doesn't force you to","date":"2016-07-21T03:34:07.427Z","type":"message"}
{"nick":"Ralith_","message":"anything that isn't factored into a work unit for execution by the thread pool can just be done directly in the simulation thread","date":"2016-07-21T03:34:32.880Z","type":"message"}
{"nick":"Dirkson","message":"Currently preparing ships for rendering eats up one core of whatever cpu it gets. And on most CPUs, could use a bit extra :D","date":"2016-07-21T03:34:40.672Z","type":"message"}
{"nick":"Dirkson","message":"Ralith_: Yeah, I like this idea. I'd never seriously considered this style of paralellization, but it has significant advantages. And whatever cycles it drops dealing with single threaded stuff is almost certainly made up for with more even load balance, and not having to deal with slow mutexes.","date":"2016-07-21T03:42:07.566Z","type":"message"}
{"nick":"Ralith_","message":"at the limit, the only \"single-threaded stuff\" that takes place in such a system is the issuing of work units, which is computationally trivial","date":"2016-07-21T03:43:26.629Z","type":"message"}
{"nick":"importantshock","date":"2016-07-21T03:45:11.766Z","type":"join"}
{"nick":"importantshock","reason":"Ping timeout: 272 seconds","date":"2016-07-21T03:50:14.184Z","type":"quit"}
{"nick":"Dirkson","message":"Ralith_: Right. Thanks for talking through this with me. You really forced me to think about the benefits offered by this style of parallelism. ","date":"2016-07-21T03:57:09.493Z","type":"message"}
{"nick":"Ralith_","message":"np","date":"2016-07-21T03:57:38.704Z","type":"message"}
{"nick":"Dirkson","message":"Opengl does complicate things a bit. It -needs- its own thread, because it will absolutely peg whatever I hand to it so that it can sit there and, as near as I can tell, do absolutely nothing. (opengl render calls are blocking, which seems odd, since all the work is done on the gpu, isn't it?) And there is a -little- shared data that the render thread currently utilizes mutexes to access. But almost all of","date":"2016-07-21T04:02:35.104Z","type":"message"}
{"nick":"Dirkson","message":"that is small enough that I can just send copies back and forth without issue.","date":"2016-07-21T04:02:37.040Z","type":"message"}
{"nick":"PerilousApricot","date":"2016-07-21T04:05:00.220Z","type":"join"}
{"nick":"Dirkson","message":"And this applies in reverse to input, as the window system I'm using has to run its input on the main thread. I think. I'll verify that, just to be sure.","date":"2016-07-21T04:06:42.694Z","type":"message"}
{"nick":"rgrinberg","reason":"Ping timeout: 260 seconds","date":"2016-07-21T04:09:22.720Z","type":"quit"}
{"nick":"importantshock","date":"2016-07-21T04:14:33.066Z","type":"join"}
{"nick":"importantshock","reason":"Remote host closed the connection","date":"2016-07-21T04:15:33.631Z","type":"quit"}
{"nick":"importantshock","date":"2016-07-21T04:28:32.592Z","type":"join"}
{"nick":"PerilousApricot","reason":"Remote host closed the connection","date":"2016-07-21T04:30:26.316Z","type":"quit"}
{"nick":"importantshock","reason":"Remote host closed the connection","date":"2016-07-21T04:35:50.176Z","type":"quit"}
{"nick":"rmg_","date":"2016-07-21T04:40:08.442Z","type":"join"}
{"nick":"rmg_","reason":"Ping timeout: 252 seconds","date":"2016-07-21T04:44:22.400Z","type":"quit"}
{"nick":"rmg_","date":"2016-07-21T06:41:42.058Z","type":"join"}
{"nick":"rmg_","reason":"Ping timeout: 240 seconds","date":"2016-07-21T06:46:03.960Z","type":"quit"}
{"nick":"rendar","date":"2016-07-21T07:26:10.054Z","type":"join"}
{"nick":"rmg_","date":"2016-07-21T07:42:27.162Z","type":"join"}
{"nick":"rmg_","reason":"Ping timeout: 240 seconds","date":"2016-07-21T07:46:31.162Z","type":"quit"}
