{"nick":"kenansulayman","date":"2013-09-05T00:03:01.361Z","type":"join"}
{"nick":"kenansulayman","reason":"Remote host closed the connection","date":"2013-09-05T00:03:03.262Z","type":"quit"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-09-05T00:04:41.799Z","type":"quit"}
{"nick":"ednapiranha","date":"2013-09-05T00:09:00.436Z","type":"join"}
{"nick":"werle","reason":"Ping timeout: 264 seconds","date":"2013-09-05T00:13:43.214Z","type":"quit"}
{"nick":"ryan_ramage","date":"2013-09-05T00:14:08.541Z","type":"join"}
{"nick":"ryan_ramage","reason":"Client Quit","date":"2013-09-05T00:15:45.778Z","type":"quit"}
{"nick":"tmcw","reason":"Remote host closed the connection","date":"2013-09-05T00:25:12.482Z","type":"quit"}
{"nick":"jxson","reason":"Remote host closed the connection","date":"2013-09-05T00:31:40.548Z","type":"quit"}
{"nick":"ednapiranha","reason":"Remote host closed the connection","date":"2013-09-05T00:36:33.708Z","type":"quit"}
{"nick":"ryan_ramage","date":"2013-09-05T00:41:37.820Z","type":"join"}
{"nick":"werle","date":"2013-09-05T00:43:42.354Z","type":"join"}
{"nick":"timoxley","date":"2013-09-05T00:48:32.035Z","type":"join"}
{"nick":"timoxley_","date":"2013-09-05T00:50:02.332Z","type":"join"}
{"nick":"timoxley","reason":"Ping timeout: 264 seconds","date":"2013-09-05T00:53:02.749Z","type":"quit"}
{"nick":"timoxley_","reason":"Ping timeout: 268 seconds","date":"2013-09-05T00:55:13.168Z","type":"quit"}
{"nick":"ryan_ramage","reason":"Quit: ryan_ramage","date":"2013-09-05T00:55:54.190Z","type":"quit"}
{"nick":"jxson","date":"2013-09-05T01:01:58.579Z","type":"join"}
{"nick":"jxson","reason":"Ping timeout: 240 seconds","date":"2013-09-05T01:10:14.532Z","type":"quit"}
{"nick":"thlorenz_zz","date":"2013-09-05T01:28:58.788Z","type":"join"}
{"nick":"mikeal","date":"2013-09-05T01:30:34.338Z","type":"join"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-09-05T01:36:38.793Z","type":"quit"}
{"nick":"ramitos","reason":"Quit: Textual IRC Client: www.textualapp.com","date":"2013-09-05T01:37:44.200Z","type":"quit"}
{"nick":"levelbot","message":"[npm] level-agile@0.2.0 <http://npm.im/level-agile>: multilevel client and server with various data transforms for inserting into leveldb (@jcrugzz)","date":"2013-09-05T01:44:17.171Z","type":"message"}
{"nick":"thlorenz_zz","message":"mbalho: I fixed the bug we talked about in the meantime (until we separate those out) https://github.com/maxogden/level-batcher/commit/df648ec176b0290a5783ac8a154d0a81a7538952","date":"2013-09-05T01:47:00.122Z","type":"message"}
{"nick":"thlorenz_zz","new_nick":"thlorenz","date":"2013-09-05T01:47:05.225Z","type":"nick"}
{"nick":"thlorenz","message":"do you wanna review this or should I just go ahead and publish - output from test is identical as before","date":"2013-09-05T01:47:32.259Z","type":"message"}
{"nick":"timoxley","date":"2013-09-05T01:49:30.540Z","type":"join"}
{"nick":"mikeal","date":"2013-09-05T02:00:54.976Z","type":"join"}
{"nick":"ryan_ramage","date":"2013-09-05T02:05:10.575Z","type":"join"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-09-05T02:10:34.157Z","type":"quit"}
{"nick":"werle","reason":"Quit: leaving","date":"2013-09-05T02:11:42.184Z","type":"quit"}
{"nick":"ryan_ramage","reason":"Quit: ryan_ramage","date":"2013-09-05T02:25:27.169Z","type":"quit"}
{"nick":"jcrugzz","reason":"Ping timeout: 250 seconds","date":"2013-09-05T02:40:53.606Z","type":"quit"}
{"nick":"jmartins","reason":"Quit: Konversation terminated!","date":"2013-09-05T02:41:28.198Z","type":"quit"}
{"nick":"mbalho","message":"thlorenz: i fixed it also, just havent published yet cause i was gonna put it in the new module which isnt done","date":"2013-09-05T02:44:38.014Z","type":"message"}
{"nick":"thlorenz","message":"ok - well I'm still seeing the problems though","date":"2013-09-05T02:44:55.770Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: what problems?","date":"2013-09-05T02:45:05.273Z","type":"message"}
{"nick":"thlorenz","message":"high memory and slow writes","date":"2013-09-05T02:45:09.928Z","type":"message"}
{"nick":"mbalho","message":"is your code online that uses it?","date":"2013-09-05T02:45:37.852Z","type":"message"}
{"nick":"mbalho","message":"i wrote 11 million items with level-batcher today","date":"2013-09-05T02:45:52.600Z","type":"message"}
{"nick":"thlorenz","message":"yes I'm now writing items one by one: https://github.com/thlorenz/valuepack-mine-github/blob/master/lib/store-github-repos.js","date":"2013-09-05T02:46:06.219Z","type":"message"}
{"nick":"thlorenz","message":"maybe I'm doing something wrong - not sure","date":"2013-09-05T02:47:05.210Z","type":"message"}
{"nick":"thlorenz","message":"I'm thinking of throttling my requests to gitub in order to avoid creating a backlog of data that needs to be stored","date":"2013-09-05T02:47:38.574Z","type":"message"}
{"nick":"thlorenz","message":"right now I'm doing 20 concurrent requests, maybe I'll dial that down a bit","date":"2013-09-05T02:48:01.861Z","type":"message"}
{"nick":"mbalho","message":"ah yes that would do it","date":"2013-09-05T02:48:11.639Z","type":"message"}
{"nick":"levelbot","message":"[npm] grunt-s3-sync@0.3.0 <http://npm.im/grunt-s3-sync>: A streaming interface for uploading multiple files to S3 (@calvein, @bockit, @hughsk)","date":"2013-09-05T02:48:17.573Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: I'm also not sure how the auto indexing works out: https://github.com/thlorenz/valuepack-mine-github/blob/master/lib/store-github-repos.js#L40","date":"2013-09-05T02:48:51.509Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: you arent doing anything with the return value of chunke.rwrite, it should be true if it is resumed and false if it is paused","date":"2013-09-05T02:49:20.005Z","type":"message"}
{"nick":"mbalho","message":"chunker.write*","date":"2013-09-05T02:49:22.569Z","type":"message"}
{"nick":"thlorenz","message":"since that will add a put into the batch automagically - should be fairly small though and have not much impact","date":"2013-09-05T02:49:24.293Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: e.g. if you do httprequest.pipe(chunker).pipe(db) then node will handle tcp backpressure","date":"2013-09-05T02:49:41.924Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: but in your use case there is no backpressure since you are doing a buncha http requests yourself but theres no throttling","date":"2013-09-05T02:50:06.242Z","type":"message"}
{"nick":"thlorenz","message":"ah - I need to handle backpressure myself? wasn't aware of that","date":"2013-09-05T02:50:27.061Z","type":"message"}
{"nick":"mbalho","message":"nowhere in your code does chunker hook up to the upstream http requests, so yes","date":"2013-09-05T02:51:03.360Z","type":"message"}
{"nick":"mbalho","message":"its complicated in your use case cause you are writing N http requests to 1 chunker","date":"2013-09-05T02:51:13.052Z","type":"message"}
{"nick":"mbalho","message":"in a situation where it is 1:1 and not N:1 its a lot more straightforward cause you can just pipe","date":"2013-09-05T02:51:26.951Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: thanks that makes more sense, so I guess when write returns false I'm gonna pause and sit around for a bit before trying to write again?","date":"2013-09-05T02:52:37.629Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: something like that, not sure exactly what the best solution is for that scenario","date":"2013-09-05T02:53:28.851Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: ideally it'd be streams all the way - maybe I could refactor this into that somehow","date":"2013-09-05T02:53:59.648Z","type":"message"}
{"nick":"thlorenz","message":"I mean the driver of all of it are logins that I can stream out of level - I could just map stream those into requests and continue that pipeline","date":"2013-09-05T02:54:39.190Z","type":"message"}
{"nick":"thlorenz","message":"it may be easier than manually handling backpressure","date":"2013-09-05T02:54:54.642Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: actually maybe not that easy at all since I'm doing so many writes per request","date":"2013-09-05T02:59:18.360Z","type":"message"}
{"nick":"thlorenz","message":"will have to come up with some mechanism to stop github requests while pausing due to backpressure","date":"2013-09-05T02:59:42.446Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: just pipe the http request to the batcher","date":"2013-09-05T03:01:25.606Z","type":"message"}
{"nick":"thlorenz","message":"you mean res.pipe(..) right?","date":"2013-09-05T03:02:04.693Z","type":"message"}
{"nick":"levelbot","message":"[npm] grunt-s3-sync@0.3.1 <http://npm.im/grunt-s3-sync>: A streaming interface for uploading multiple files to S3 (@calvein, @bockit, @hughsk)","date":"2013-09-05T03:02:18.000Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: yea","date":"2013-09-05T03:02:22.761Z","type":"message"}
{"nick":"thlorenz","message":"but I have multiple writes to different sublevels for each response","date":"2013-09-05T03:02:23.496Z","type":"message"}
{"nick":"mbalho","message":"ah","date":"2013-09-05T03:02:38.280Z","type":"message"}
{"nick":"thlorenz","message":"hey I could use dominictarr's from and batch up all the puts and then generate a stream from that which handles backpressure","date":"2013-09-05T03:05:18.985Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: I'd just need to find a way to communicate to the module pushing the requests that I'm backed up","date":"2013-09-05T03:06:02.046Z","type":"message"}
{"nick":"ryan_ramage","date":"2013-09-05T03:08:01.752Z","type":"join"}
{"nick":"ryan_ramage","reason":"Client Quit","date":"2013-09-05T03:08:09.548Z","type":"quit"}
{"nick":"thlorenz","message":"mbalho: got a solution in the works, thanks for your help - hopefully can finish it tomorrow morning - will post the code to get your feedback","date":"2013-09-05T03:24:35.503Z","type":"message"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-09-05T03:28:02.186Z","type":"quit"}
{"nick":"jcrugzz","date":"2013-09-05T03:49:08.539Z","type":"join"}
{"nick":"disordinary","reason":"Ping timeout: 264 seconds","date":"2013-09-05T03:56:30.456Z","type":"quit"}
{"nick":"julianduque","reason":"Quit: leaving","date":"2013-09-05T04:01:44.057Z","type":"quit"}
{"nick":"esundahl","date":"2013-09-05T04:07:08.472Z","type":"join"}
{"nick":"thlorenz","date":"2013-09-05T04:25:17.336Z","type":"join"}
{"nick":"dguttman","reason":"Quit: dguttman","date":"2013-09-05T04:55:41.540Z","type":"quit"}
{"nick":"esundahl","reason":"Remote host closed the connection","date":"2013-09-05T04:58:32.876Z","type":"quit"}
{"nick":"esundahl","date":"2013-09-05T04:59:06.514Z","type":"join"}
{"nick":"esundahl_","date":"2013-09-05T05:00:58.744Z","type":"join"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-09-05T05:02:46.010Z","type":"quit"}
{"nick":"esundahl","reason":"Ping timeout: 264 seconds","date":"2013-09-05T05:03:42.274Z","type":"quit"}
{"nick":"dguttman","date":"2013-09-05T05:06:08.348Z","type":"join"}
{"nick":"jxson","date":"2013-09-05T05:08:18.183Z","type":"join"}
{"nick":"jxson","reason":"Ping timeout: 245 seconds","date":"2013-09-05T05:12:30.119Z","type":"quit"}
{"nick":"wolfeidau","reason":"Ping timeout: 260 seconds","date":"2013-09-05T05:45:51.951Z","type":"quit"}
{"nick":"dguttman","reason":"Quit: dguttman","date":"2013-09-05T05:46:44.475Z","type":"quit"}
{"nick":"wolfeidau","date":"2013-09-05T05:46:45.488Z","type":"join"}
{"nick":"Acconut","date":"2013-09-05T06:40:00.149Z","type":"join"}
{"nick":"st_luke","date":"2013-09-05T06:41:19.149Z","type":"join"}
{"nick":"Acconut","reason":"Quit: Acconut","date":"2013-09-05T06:56:36.034Z","type":"quit"}
{"nick":"esundahl_","reason":"Remote host closed the connection","date":"2013-09-05T06:57:39.312Z","type":"quit"}
{"nick":"st_luke","reason":"Remote host closed the connection","date":"2013-09-05T07:15:31.001Z","type":"quit"}
{"nick":"esundahl","date":"2013-09-05T07:28:44.672Z","type":"join"}
{"nick":"dominictarr","date":"2013-09-05T07:31:42.501Z","type":"join"}
{"nick":"esundahl","reason":"Ping timeout: 260 seconds","date":"2013-09-05T07:37:15.186Z","type":"quit"}
