{"nick":"jcrugzz","date":"2013-10-13T00:05:56.492Z","type":"join"}
{"nick":"thlorenz","date":"2013-10-13T00:19:23.027Z","type":"join"}
{"nick":"esundahl","date":"2013-10-13T00:48:58.956Z","type":"join"}
{"nick":"insertcoffee","reason":"Ping timeout: 240 seconds","date":"2013-10-13T00:51:26.541Z","type":"quit"}
{"nick":"kenansul_","date":"2013-10-13T01:28:28.336Z","type":"join"}
{"nick":"kenansulayman","reason":"Ping timeout: 264 seconds","date":"2013-10-13T01:31:40.374Z","type":"quit"}
{"nick":"kenansul_","reason":"Remote host closed the connection","date":"2013-10-13T01:45:35.510Z","type":"quit"}
{"nick":"jmartins","date":"2013-10-13T02:06:36.728Z","type":"join"}
{"nick":"jmartins","reason":"Quit: Konversation terminated!","date":"2013-10-13T02:33:20.681Z","type":"quit"}
{"nick":"jmartins","date":"2013-10-13T02:44:05.186Z","type":"join"}
{"nick":"jmartins","reason":"Read error: Connection reset by peer","date":"2013-10-13T02:53:31.426Z","type":"quit"}
{"nick":"esundahl","reason":"Remote host closed the connection","date":"2013-10-13T03:03:36.687Z","type":"quit"}
{"nick":"tarruda","date":"2013-10-13T03:11:09.168Z","type":"join"}
{"nick":"gwenbell","date":"2013-10-13T03:11:48.264Z","type":"join"}
{"nick":"jcrugzz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T03:17:08.422Z","type":"quit"}
{"nick":"esundahl","date":"2013-10-13T03:30:22.961Z","type":"join"}
{"nick":"timoxley","date":"2013-10-13T03:37:08.098Z","type":"join"}
{"nick":"jcrugzz","date":"2013-10-13T03:45:29.356Z","type":"join"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-10-13T03:47:56.548Z","type":"quit"}
{"nick":"timoxley","date":"2013-10-13T04:14:17.887Z","type":"join"}
{"nick":"tarruda","reason":"Quit: leaving","date":"2013-10-13T04:24:33.294Z","type":"quit"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-10-13T04:26:51.667Z","type":"quit"}
{"nick":"eugeneware","new_nick":"zz_eugeneware","date":"2013-10-13T04:30:00.838Z","type":"nick"}
{"nick":"jcrugzz","reason":"Ping timeout: 253 seconds","date":"2013-10-13T04:42:00.357Z","type":"quit"}
{"nick":"gwenbell","reason":"Ping timeout: 245 seconds","date":"2013-10-13T04:44:21.263Z","type":"quit"}
{"nick":"jcrugzz","date":"2013-10-13T04:51:40.744Z","type":"join"}
{"nick":"jcrugzz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T04:56:14.572Z","type":"quit"}
{"nick":"jcrugzz","date":"2013-10-13T05:02:00.301Z","type":"join"}
{"nick":"jcrugzz","reason":"Ping timeout: 265 seconds","date":"2013-10-13T05:07:18.936Z","type":"quit"}
{"nick":"jcrugzz","date":"2013-10-13T05:07:54.409Z","type":"join"}
{"nick":"zz_eugeneware","new_nick":"eugeneware","date":"2013-10-13T05:09:06.514Z","type":"nick"}
{"nick":"DTrejo","reason":"Remote host closed the connection","date":"2013-10-13T05:11:28.789Z","type":"quit"}
{"nick":"jcrugzz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T05:17:08.275Z","type":"quit"}
{"nick":"eugeneware","new_nick":"zz_eugeneware","date":"2013-10-13T05:19:30.816Z","type":"nick"}
{"nick":"esundahl","reason":"Ping timeout: 272 seconds","date":"2013-10-13T05:37:57.742Z","type":"quit"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-10-13T05:43:33.491Z","type":"quit"}
{"nick":"mikeal","date":"2013-10-13T05:44:29.947Z","type":"join"}
{"nick":"zz_eugeneware","new_nick":"eugeneware","date":"2013-10-13T06:14:56.157Z","type":"nick"}
{"nick":"jcrugzz","date":"2013-10-13T06:25:28.082Z","type":"join"}
{"nick":"thlorenz","date":"2013-10-13T06:54:56.753Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T07:03:02.542Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T07:29:09.181Z","type":"join"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-10-13T07:35:01.916Z","type":"quit"}
{"nick":"thlorenz","reason":"Ping timeout: 246 seconds","date":"2013-10-13T07:37:10.879Z","type":"quit"}
{"nick":"dominictarr","date":"2013-10-13T07:57:44.963Z","type":"join"}
{"nick":"thlorenz","date":"2013-10-13T08:24:07.074Z","type":"join"}
{"nick":"dominictarr","reason":"Ping timeout: 272 seconds","date":"2013-10-13T08:25:47.782Z","type":"quit"}
{"nick":"thlorenz","reason":"Ping timeout: 272 seconds","date":"2013-10-13T08:29:13.820Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T08:34:03.001Z","type":"join"}
{"nick":"timoxley","date":"2013-10-13T08:35:31.210Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 272 seconds","date":"2013-10-13T08:39:05.730Z","type":"quit"}
{"nick":"fb55","date":"2013-10-13T09:10:31.458Z","type":"join"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-10-13T09:15:00.908Z","type":"quit"}
{"nick":"fb55","reason":"Ping timeout: 272 seconds","date":"2013-10-13T09:15:27.858Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T09:31:20.069Z","type":"join"}
{"nick":"thlorenz","date":"2013-10-13T09:34:44.758Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T09:39:02.542Z","type":"quit"}
{"nick":"nathan7","date":"2013-10-13T10:06:26.946Z","type":"part"}
{"nick":"thlorenz","date":"2013-10-13T10:35:22.159Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T10:39:27.680Z","type":"quit"}
{"nick":"DTrejo","reason":"Remote host closed the connection","date":"2013-10-13T11:04:34.791Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T11:05:01.409Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T11:05:52.321Z","type":"quit"}
{"nick":"dscape","date":"2013-10-13T11:09:53.585Z","type":"part"}
{"nick":"DTrejo","date":"2013-10-13T11:33:28.760Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T11:33:33.276Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T11:35:59.518Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 256 seconds","date":"2013-10-13T11:40:38.244Z","type":"quit"}
{"nick":"tarruda","date":"2013-10-13T11:49:36.814Z","type":"join"}
{"nick":"kenansulayman","date":"2013-10-13T12:11:49.358Z","type":"join"}
{"nick":"DTrejo","date":"2013-10-13T12:15:04.687Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T12:15:42.522Z","type":"quit"}
{"nick":"jcrugzz","reason":"Ping timeout: 246 seconds","date":"2013-10-13T12:27:40.959Z","type":"quit"}
{"nick":"jcrugzz","date":"2013-10-13T12:54:58.802Z","type":"join"}
{"nick":"jcrugzz","reason":"Ping timeout: 265 seconds","date":"2013-10-13T13:03:53.515Z","type":"quit"}
{"nick":"levelbot","message":"[npm] contextdb@0.2.0 <http://npm.im/contextdb>: Use json-context with leveldb. Contexts are automatically generated from matchers, and provides ability to watch matchers for realtime notifications. (@mmckegg)","date":"2013-10-13T13:35:08.170Z","type":"message"}
{"nick":"thlorenz","date":"2013-10-13T13:37:14.786Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 246 seconds","date":"2013-10-13T13:41:31.976Z","type":"quit"}
{"nick":"levelbot","message":"[npm] level-stay@0.0.1 <http://npm.im/level-stay>: Persistence for scuttlebutts based on LevelDB (@juliangruber)","date":"2013-10-13T13:50:38.572Z","type":"message"}
{"nick":"jcrugzz","date":"2013-10-13T13:52:58.050Z","type":"join"}
{"nick":"DTrejo","date":"2013-10-13T13:55:59.249Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T13:56:17.407Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T14:05:33.105Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T14:05:51.979Z","type":"quit"}
{"nick":"tarruda","reason":"Ping timeout: 272 seconds","date":"2013-10-13T14:18:33.728Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T14:27:44.938Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T14:27:50.683Z","type":"quit"}
{"nick":"jcrugzz","reason":"Ping timeout: 272 seconds","date":"2013-10-13T14:32:07.779Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T14:54:18.762Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T14:54:31.938Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T14:56:06.941Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T14:56:15.922Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T14:56:36.272Z","type":"join"}
{"nick":"DTrejo","reason":"Read error: Connection reset by peer","date":"2013-10-13T14:56:41.612Z","type":"quit"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-10-13T14:59:45.264Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T15:08:49.556Z","type":"join"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-10-13T15:08:50.571Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T15:11:54.475Z","type":"join"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-10-13T15:11:54.475Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T15:12:25.795Z","type":"join"}
{"nick":"levelbot","message":"[npm] meatspace-leveldb@0.0.8 <http://npm.im/meatspace-leveldb>: Decentralized micro[b]logging with leveldb (@ednapiranha)","date":"2013-10-13T15:25:08.294Z","type":"message"}
{"nick":"mikeal","date":"2013-10-13T15:25:11.051Z","type":"join"}
{"nick":"kenansulayman","message":"yoyo guys","date":"2013-10-13T15:49:45.157Z","type":"message"}
{"nick":"kenansulayman","message":"rvagg rescrv ogd Had a test run with mmalecki at Sophia","date":"2013-10-13T15:50:10.706Z","type":"message"}
{"nick":"kenansulayman","message":"Check this out","date":"2013-10-13T15:50:14.030Z","type":"message"}
{"nick":"kenansulayman","message":"https://github.com/mmalecki/node-sophia/issues/3","date":"2013-10-13T15:50:14.358Z","type":"message"}
{"nick":"kenansulayman","message":"Batches etc is to come","date":"2013-10-13T15:53:08.506Z","type":"message"}
{"nick":"levelbot","message":"[npm] lev@1.1.3 <http://npm.im/lev>: commandline and REPL access for leveldb (@hij1nx, @juliangruber)","date":"2013-10-13T15:53:38.377Z","type":"message"}
{"nick":"kenansulayman","message":"Yet we got put get del running (and fixed Sophia not compiling on OSX)","date":"2013-10-13T15:53:39.291Z","type":"message"}
{"nick":"fb55","date":"2013-10-13T15:58:53.890Z","type":"join"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-10-13T16:05:12.624Z","type":"quit"}
{"nick":"mikeal","date":"2013-10-13T16:06:23.088Z","type":"join"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-10-13T16:18:07.760Z","type":"quit"}
{"nick":"jcrugzz","date":"2013-10-13T16:31:37.409Z","type":"join"}
{"nick":"fb55","reason":"Read error: Connection reset by peer","date":"2013-10-13T16:31:42.914Z","type":"quit"}
{"nick":"mikeal","date":"2013-10-13T16:48:53.501Z","type":"join"}
{"nick":"jcrugzz","reason":"Ping timeout: 252 seconds","date":"2013-10-13T17:09:27.185Z","type":"quit"}
{"nick":"levelbot","message":"[npm] lev@1.2.0 <http://npm.im/lev>: commandline and REPL access for leveldb (@hij1nx, @juliangruber)","date":"2013-10-13T17:12:38.377Z","type":"message"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-10-13T17:25:48.422Z","type":"quit"}
{"nick":"rescrv","message":"kenansulayman: how about a summary?  What's the workload?  What's the setup?  Is there already data in the database?  What insight should I take away?","date":"2013-10-13T17:40:04.228Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv this is not finetuned, no special adjustments or flags. Just an empty database. I write a million entries from a csv which I previously read into memory so that there is no high fluctuation because of disk-usage issues (I'm on a HDD)","date":"2013-10-13T17:42:32.569Z","type":"message"}
{"nick":"kenansulayman","message":"They keys are Strings generated by ~~(Math.random()*1E17) and split in the read process","date":"2013-10-13T17:43:52.305Z","type":"message"}
{"nick":"kenansulayman","message":"Since batch etc isn't implemented in the driver yet the quite low level put to write is used","date":"2013-10-13T17:44:57.567Z","type":"message"}
{"nick":"rescrv","message":"kenansulayman:  I'm still not sure what I should be learning from the issue you linked.","date":"2013-10-13T17:45:37.908Z","type":"message"}
{"nick":"rescrv","message":"I see that sophia takes 6s, HyperLevelDB takes 15s, and that you love makefiles","date":"2013-10-13T17:45:57.766Z","type":"message"}
{"nick":"rescrv","message":"but I don't see any description of what it is you're doing, why it is a valid comparison, or what I should learn from the results","date":"2013-10-13T17:46:35.508Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv I don't know what you'd \"learn\" it's just a comparision in speed for cases that apply for us in production (we wouldn't actually batch since we write per request of a user)","date":"2013-10-13T17:46:42.385Z","type":"message"}
{"nick":"rescrv","message":"To do that, I have to download the CSV, which still doesn't tell me much as it's just random strings.","date":"2013-10-13T17:47:05.516Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv Why are you so negative? I did not want to offend if I did.","date":"2013-10-13T17:47:41.663Z","type":"message"}
{"nick":"rescrv","message":"kenansulayman: then say that.  That tells me something I didn't know before.  Something like, \"In production, we often have to do X.  X is characterized by Y.  This benchmark mimics Y by measuring Z.  As you can see, ...\"","date":"2013-10-13T17:48:53.847Z","type":"message"}
{"nick":"rescrv","message":"I'm not trying to be negative.  Maybe I've just had too much coffee/too little sleep.  I also tend to get frustrated by benchmarks that show raw numbers that reflect on (part of) my work, without explaining anything.","date":"2013-10-13T17:49:46.074Z","type":"message"}
{"nick":"thlorenz","date":"2013-10-13T17:50:03.414Z","type":"join"}
{"nick":"kenansulayman","message":"rescrv It","date":"2013-10-13T17:50:06.251Z","type":"message"}
{"nick":"rescrv","message":"one little benchmark saying something is bad/broken, and it's many hours of headaches down the line defending it.","date":"2013-10-13T17:50:15.956Z","type":"message"}
{"nick":"rescrv","message":"I appreciate that you took time to benchmark hyperleveldb, and perhaps am being a little rude in trying to understand what's going on.","date":"2013-10-13T17:50:47.902Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv It is actually that it took quite some time to get right, so it's rather to show off the work since rvagg talked about it being a possible new level backend","date":"2013-10-13T17:51:15.525Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv and I didn't chose hyperlevel because I would like to say \"this\" or \"that\" is better; actually hyperlevel and lmdb is what we use / used in production; that is I just tried to (I admit it's quite blurry) get a grasp on Sophia's functions and performance","date":"2013-10-13T17:52:47.124Z","type":"message"}
{"nick":"rescrv","message":"kenansulayman: understood","date":"2013-10-13T17:53:03.420Z","type":"message"}
{"nick":"rescrv","message":"I overreacted.","date":"2013-10-13T17:53:25.168Z","type":"message"}
{"nick":"rescrv","message":"out of curiosity, what'd you use HyperLevelDB in production for/where'd you use it?","date":"2013-10-13T17:53:41.439Z","type":"message"}
{"nick":"kenansulayman","message":"At several places: primarily user avatar cache andsession storage","date":"2013-10-13T17:55:33.406Z","type":"message"}
{"nick":"kenansulayman","message":"We used it previously as backend for the main-database of for all the users (with replication) but switched to lmdb because it allows clustering of the process","date":"2013-10-13T17:56:23.920Z","type":"message"}
{"nick":"kenansulayman","message":"used it until recently*","date":"2013-10-13T17:56:40.149Z","type":"message"}
{"nick":"kenansulayman","message":"(clustering = just a new instance of the server using the same database; we usually spawn as many instances of the server as there are cores)","date":"2013-10-13T17:58:00.377Z","type":"message"}
{"nick":"rescrv","message":"my main concern with lmdb is that it manages its own heap.  Which is fine standalone, but the changes coming down the pipe in HyperDex leverage LevelDB's structure in huge ways, to allow quick backup and server-server repair.  Sophia/LMDB cannot match that","date":"2013-10-13T17:58:19.548Z","type":"message"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-10-13T18:00:04.658Z","type":"quit"}
{"nick":"kenansulayman","message":"Could you elaborate on why it's a concern of yours?","date":"2013-10-13T18:00:12.822Z","type":"message"}
{"nick":"rescrv","message":"well, it's mainly a concern in clustered/replicated production environments.  Say I want to take a backup of a server.  Stop the server, backup, start the server.  Except if you have a lot of data, that backup is expensive and translates into downtime.","date":"2013-10-13T18:02:08.993Z","type":"message"}
{"nick":"rescrv","message":"We modified LevelDB to do it nearly instantly, leveraging FS-level hardlinks.","date":"2013-10-13T18:02:24.828Z","type":"message"}
{"nick":"rescrv","message":"AFAIK, you cannot do that with things that manage their own heap as one file.  You need invasive modification to enable that.","date":"2013-10-13T18:02:58.683Z","type":"message"}
{"nick":"rescrv","message":"So, in HyperLevelDB, I call \"LiveBackup\" and it's done.  It takes literally milliseconds, even if you have terabytes of data.  You can then do differential backup, rsync'ing from the host, only that data that changed since your last backup.","date":"2013-10-13T18:03:48.694Z","type":"message"}
{"nick":"kenansulayman","message":"hm. What stops us from just copying the directory while load-balancing the data to another server?","date":"2013-10-13T18:04:15.592Z","type":"message"}
{"nick":"rescrv","message":"Similar tricks enable us to transfer a small subset of data to re-sync replicas, rather than having to scan everything.","date":"2013-10-13T18:04:25.497Z","type":"message"}
{"nick":"kenansulayman","message":"Ah! Is that possible over the node hyperleveldb binding?","date":"2013-10-13T18:04:40.035Z","type":"message"}
{"nick":"rescrv","message":"I don't know your arch, but for us that means that you now have to take something offline and keep it offline while copying.","date":"2013-10-13T18:04:57.037Z","type":"message"}
{"nick":"rescrv","message":"For HyperDex, we can pause the entire cluster, backup every node at a consistent snapshot, and unpause the writes, to give a fully 100% consistent backup with just a few seconds of downtime.","date":"2013-10-13T18:05:28.364Z","type":"message"}
{"nick":"rescrv","message":"the live backup?  I don't know if it's exposed over Node.  I haven't looked.","date":"2013-10-13T18:05:52.949Z","type":"message"}
{"nick":"brycebaril","message":"I think LiveBackup was exposed (via ogd) but I haven't tried it yet.","date":"2013-10-13T18:06:11.906Z","type":"message"}
{"nick":"kenansulayman","message":"That's to HyperDex. I kept nagging you about a node binding to that ;)","date":"2013-10-13T18:06:12.118Z","type":"message"}
{"nick":"kenansulayman","message":"brycebaril Have you got a link to that? :)","date":"2013-10-13T18:07:22.645Z","type":"message"}
{"nick":"brycebaril","message":"rescrv: do you pause the cluster for the consistency? That way no need to track missed operation deltas between LiveBackup init & completion?","date":"2013-10-13T18:07:23.702Z","type":"message"}
{"nick":"rescrv","message":"brycebaril: exactly.  It's (theoretically) also possible to take a sloppy backup that you'd then have to replay against an old version, but I've got enough to do already.","date":"2013-10-13T18:08:21.555Z","type":"message"}
{"nick":"brycebaril","message":"kenansulayman: : look at the commit log for the node-LevelDOWN HyperLevelDB branch","date":"2013-10-13T18:08:28.099Z","type":"message"}
{"nick":"rescrv","message":"kenansulayman: I'm illustrating how it's useful.  If you shard across different instances in your app, you'd be able to take the same approach to backup your dat.","date":"2013-10-13T18:08:53.627Z","type":"message"}
{"nick":"brycebaril","message":"rescrv: Yeah, in a clustered environment I agree that's the easiest way to get consistency. It'll be interesting to see what Redis does for it's cluster. Right now it's equivalent of LiveBackup that it uses for replication (SYNC) tracks the delta and sends that. But that's a single-master environment.","date":"2013-10-13T18:10:07.318Z","type":"message"}
{"nick":"rescrv","message":"brycebaril: I think they are poking around in the dark, looking for a solution.  In an ideal world, you'd use this: http://ece842.com/S13/readings/chandy85.pdf to take the snapshot.","date":"2013-10-13T18:12:36.881Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv I'd love to get started with HyperDex since it's hard to maintain our own replication / master-slave code while doing the other work, too.","date":"2013-10-13T18:12:43.725Z","type":"message"}
{"nick":"rescrv","message":"the complexity of getting that right in the implementation","date":"2013-10-13T18:12:49.445Z","type":"message"}
{"nick":"rescrv","message":"it would keep me up at night, and I'd much rather give people the ability to either pause the cluster, or have to restore after failure.","date":"2013-10-13T18:13:32.170Z","type":"message"}
{"nick":"rescrv","message":"kenansulayman: I cannot wait to get htis release done so I can get bindings ready, but there's a few other surprises we're getting ready first.","date":"2013-10-13T18:14:09.441Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv I am really curious. After that Google employee told me about the pricing of their K/V and that every transaction costs actually 3 writes I cringed","date":"2013-10-13T18:15:05.716Z","type":"message"}
{"nick":"rescrv","message":"can you fill me in on a little more?  What system?  Spanner?","date":"2013-10-13T18:15:59.747Z","type":"message"}
{"nick":"kenansulayman","message":"Spanner?","date":"2013-10-13T18:16:33.635Z","type":"message"}
{"nick":"rescrv","message":"so spanner's got a neat design, and they don't care about cost because they can run more servers to overcome it","date":"2013-10-13T18:17:42.824Z","type":"message"}
{"nick":"brycebaril","message":"rescrv: this paper looks cool. Long shot, but do you know of similar work for write-only (i.e. no updates/deletes) databases? my assumption is that vastly simplifies the problemset","date":"2013-10-13T18:18:17.402Z","type":"message"}
{"nick":"rescrv","message":"the three writes aren't the problem as much as having to do a wide-area round trip through paxos to commit","date":"2013-10-13T18:18:33.277Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv We recently been to Google HQ berlin to discuss our adwords campaign and we met a group of cloud engineers there. We talked about our database to keep up with realtime collaboration etc. They gave us $2k to test app engine when we discussed how to use the platform","date":"2013-10-13T18:19:44.802Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv The issue is mainly that we run a high-concurrency realtime system and need to write up to 20 - 30 things per user per second (for instance a realtime editor)","date":"2013-10-13T18:20:46.766Z","type":"message"}
{"nick":"rescrv","message":"brycebaril: not really.  the algorithm doesn't really care about whether a write is an update/delete/first-insert.  In fact, if you had such an algorithm that did care, you could turn everything into an append, and a delete would be an append that says a previous value is invalid.","date":"2013-10-13T18:20:59.452Z","type":"message"}
{"nick":"rescrv","message":"kenansulayman: yeah, app engine doesn't offer that high of a perf, but the arch of spanner/big table can support many thousands of writes/s-tablet","date":"2013-10-13T18:22:04.150Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv Could you link to that? Google shows stuff that's most probably not \"spanner\"","date":"2013-10-13T18:22:37.117Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv ow it's from google","date":"2013-10-13T18:23:15.235Z","type":"message"}
{"nick":"rescrv","message":"here's what I'd recommend watching/reading: https://www.usenix.org/conference/osdi12/elmo-building-globally-distributed-highly-available-database","date":"2013-10-13T18:23:34.828Z","type":"message"}
{"nick":"rescrv","message":"there's plenty of spanner references floating around (back to '07 or earlier), but that's the definitive publication on how it works/what it does","date":"2013-10-13T18:24:37.627Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv it's neither open source nor a service, right?","date":"2013-10-13T18:24:49.740Z","type":"message"}
{"nick":"rescrv","message":"kenansulayman: correct.  I suspect it'll be closed for quite some time.  I also suspect that any clones claiming to be open source will be nowhere near close to what Spanner really is.","date":"2013-10-13T18:25:34.756Z","type":"message"}
{"nick":"kenansulayman","message":"That's pity. In order to get the stores right which enables our realtime collaboration, we should have been a storage architecture company on the first place ... :x","date":"2013-10-13T18:26:37.025Z","type":"message"}
{"nick":"rescrv","message":"I'm not sure what exactly you're doing, but it sounds like you might benefit from doing in-memory replication, and persisting a log of what was done that can be replayed to restore state.","date":"2013-10-13T18:27:57.728Z","type":"message"}
{"nick":"rescrv","message":"rather than trying to write the outputs of the computation (20-30 per user per second), write the inputs from the user, and make it deterministic.","date":"2013-10-13T18:28:42.514Z","type":"message"}
{"nick":"brycebaril","message":"kenansulayman: or look into some of what dominictarr has done with https://github.com/dominictarr/scuttlebutt","date":"2013-10-13T18:28:52.620Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv That's why the in-memory concept of lmdb was a delight to us","date":"2013-10-13T18:30:25.502Z","type":"message"}
{"nick":"kenansulayman","message":"brycebaril We already use that code for log rotation :) But master/slave for the rest","date":"2013-10-13T18:31:23.565Z","type":"message"}
{"nick":"rescrv","message":"kenansulayman: I'm not talking storing a database in memory.  I'm thinking more along the lines of a replicated state machine: http://www.cs.cornell.edu/fbs/publications/smsurvey.pdf","date":"2013-10-13T18:32:45.662Z","type":"message"}
{"nick":"rescrv","message":"you have an in-memory application that's very fast.  Not a stateless app writing to a db, but an actual application.","date":"2013-10-13T18:33:10.519Z","type":"message"}
{"nick":"rescrv","message":"log the inputs to the state machine, and you'll always end up in the same state given the same inputs","date":"2013-10-13T18:33:29.128Z","type":"message"}
{"nick":"rescrv","message":"so you're not bound by the speed of a data store, you're bound by the speed of whatever app you're writing","date":"2013-10-13T18:33:48.000Z","type":"message"}
{"nick":"rescrv","message":"which I almost guarantee can be faster than storing/retrieving items from a data store","date":"2013-10-13T18:34:01.516Z","type":"message"}
{"nick":"rescrv","message":"so there's an actual thread (and replicas of that thread) doing the work corresponding to the user","date":"2013-10-13T18:34:21.468Z","type":"message"}
{"nick":"thlorenz","date":"2013-10-13T18:35:16.418Z","type":"join"}
{"nick":"kenansulayman","message":"rescrv Are there existing solutions for that, so that we do not lose a high amount of HR on building that? :)","date":"2013-10-13T18:38:30.932Z","type":"message"}
{"nick":"rescrv","message":"I don't know about existing solutions that let you maintain a cluster of state machines.  I do know of at least one that lets you maintain several state machines in one cluster.  You'd have to maintain several clusters independently, but it's easy scaling.","date":"2013-10-13T18:39:40.069Z","type":"message"}
{"nick":"kenansulayman","message":"Which one?","date":"2013-10-13T18:41:18.726Z","type":"message"}
{"nick":"rescrv","message":"https://github.com/rescrv/Replicant","date":"2013-10-13T18:41:32.269Z","type":"message"}
{"nick":"rescrv","message":"Here's an example: https://github.com/rescrv/Replicant/blob/master/examples/log.c","date":"2013-10-13T18:41:37.821Z","type":"message"}
{"nick":"rescrv","message":"that's the server-side code","date":"2013-10-13T18:41:43.334Z","type":"message"}
{"nick":"rescrv","message":"here's the client header: https://github.com/rescrv/Replicant/blob/master/client/replicant.h","date":"2013-10-13T18:42:11.727Z","type":"message"}
{"nick":"rescrv","message":"let's say you call send(\"myobj\", \"log\", \"abc\", 4, ...)","date":"2013-10-13T18:42:40.264Z","type":"message"}
{"nick":"rescrv","message":"it'll translate that into the call to log_log and it'll print \"log was asked to log\\\"abc\\\" and it is 3 bytes long\"","date":"2013-10-13T18:43:29.898Z","type":"message"}
{"nick":"thlorenz","reason":"Ping timeout: 245 seconds","date":"2013-10-13T18:43:31.223Z","type":"quit"}
{"nick":"kenansulayman","message":"rescrvs Wouldn't that stateless log be a deadlock when the system goes down","date":"2013-10-13T18:44:07.371Z","type":"message"}
{"nick":"rescrv","message":"underneath the hood, it takes care of maintaining the correct number of replicas of the \"log\" object, and will send that to it","date":"2013-10-13T18:44:14.626Z","type":"message"}
{"nick":"rescrv","message":"what do you mean?","date":"2013-10-13T18:44:23.284Z","type":"message"}
{"nick":"kenansulayman","message":"Because it has to be replayed?","date":"2013-10-13T18:44:28.895Z","type":"message"}
{"nick":"rescrv","message":"there's two aspects to that.  First, it maintains multiple replicas.  You deploy 5, you can have two crash, and then another one crash later.  So those replicas are kept live.  It's only after you kill/restart everything that you have to replay the log","date":"2013-10-13T18:45:39.033Z","type":"message"}
{"nick":"rescrv","message":"if it's that big of a deal, you can always take a snapshot that represents a prefix of the log, and replay from that point","date":"2013-10-13T18:48:26.600Z","type":"message"}
{"nick":"kenansulayman","message":"That makes sense, yes.","date":"2013-10-13T18:51:26.503Z","type":"message"}
{"nick":"kenansulayman","message":"But lets assume we have a hash-ring whereas each node is a memory-mapped database","date":"2013-10-13T18:52:45.250Z","type":"message"}
{"nick":"kenansulayman","message":"I thing that makes more sense at this point, because on high-load writes the logs could grow infinitely, not?","date":"2013-10-13T18:53:18.747Z","type":"message"}
{"nick":"rescrv","message":"not if you keep snapshotting/truncating it.","date":"2013-10-13T18:53:42.114Z","type":"message"}
{"nick":"rescrv","message":"it's two differnet ways of building things.","date":"2013-10-13T18:53:49.341Z","type":"message"}
{"nick":"rescrv","message":"you could put the state machines around a hash ring too","date":"2013-10-13T18:53:56.898Z","type":"message"}
{"nick":"rescrv","message":"the state machine approach only makes sense if the app itself has a lot of amplification when persisted to a database","date":"2013-10-13T18:54:16.268Z","type":"message"}
{"nick":"rescrv","message":"if the overhead of persisting to the database isn't that much, then the DB makes sense too","date":"2013-10-13T18:55:39.686Z","type":"message"}
{"nick":"DTrejo","date":"2013-10-13T18:55:40.754Z","type":"join"}
{"nick":"kenansulayman","message":"rescrv We'll see where we go. We're not too much on users right now and will grow (around 1000 recurring users; we just started) That is, our peak is at max some thousand queries per second.. so I think we should focus on the general development for now.","date":"2013-10-13T18:57:36.851Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv However, I'd be extremely delighted to see HyperDex land in the \"userspace\" :)","date":"2013-10-13T18:57:59.270Z","type":"message"}
{"nick":"rescrv","message":"I'm working hard to make that possible","date":"2013-10-13T18:58:59.098Z","type":"message"}
{"nick":"rescrv","message":"I have to run for a bit though","date":"2013-10-13T18:59:03.825Z","type":"message"}
{"nick":"rescrv","message":"it was good talking to you","date":"2013-10-13T18:59:07.290Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv Yes, thanks. I learned quite somethings and will read into the pdfs :)","date":"2013-10-13T19:00:13.462Z","type":"message"}
{"nick":"kenansulayman","message":"rescrv btw https://developers.google.com/datastore/","date":"2013-10-13T19:01:22.177Z","type":"message"}
{"nick":"DTrejo","reason":"Remote host closed the connection","date":"2013-10-13T19:04:39.607Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T19:05:06.541Z","type":"join"}
{"nick":"Acconut","date":"2013-10-13T19:07:34.735Z","type":"join"}
{"nick":"DTrejo","reason":"Ping timeout: 240 seconds","date":"2013-10-13T19:09:28.196Z","type":"quit"}
{"nick":"Acconut","reason":"Ping timeout: 272 seconds","date":"2013-10-13T19:12:25.733Z","type":"quit"}
{"nick":"wolfeida_","date":"2013-10-13T19:12:28.478Z","type":"join"}
{"nick":"wolfeidau","reason":"Ping timeout: 272 seconds","date":"2013-10-13T19:15:51.881Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T19:20:23.962Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 272 seconds","date":"2013-10-13T19:25:05.767Z","type":"quit"}
{"nick":"eugeneware","reason":"Ping timeout: 264 seconds","date":"2013-10-13T19:37:40.383Z","type":"quit"}
{"nick":"wolfeidau","date":"2013-10-13T19:39:49.615Z","type":"join"}
{"nick":"rud","reason":"Quit: rud","date":"2013-10-13T19:40:36.750Z","type":"quit"}
{"nick":"wolfeida_","reason":"Ping timeout: 265 seconds","date":"2013-10-13T19:43:07.939Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T20:20:50.792Z","type":"join"}
{"nick":"rud","date":"2013-10-13T20:21:35.918Z","type":"join"}
{"nick":"rud","reason":"Changing host","date":"2013-10-13T20:21:36.132Z","type":"quit"}
{"nick":"rud","date":"2013-10-13T20:21:36.132Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T20:25:02.534Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T20:40:46.836Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 265 seconds","date":"2013-10-13T20:45:28.992Z","type":"quit"}
{"nick":"thlorenz","date":"2013-10-13T21:21:12.789Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T21:25:26.532Z","type":"quit"}
{"nick":"trevnorris","message":"rvagg: what's going on?","date":"2013-10-13T21:38:10.300Z","type":"message"}
{"nick":"thlorenz","date":"2013-10-13T21:49:18.749Z","type":"join"}
{"nick":"disordinary","date":"2013-10-13T21:53:03.057Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 240 seconds","date":"2013-10-13T21:53:26.535Z","type":"quit"}
{"nick":"jcrugzz","date":"2013-10-13T22:04:35.534Z","type":"join"}
{"nick":"thlorenz","date":"2013-10-13T22:08:40.752Z","type":"join"}
{"nick":"DTrejo","date":"2013-10-13T22:13:36.752Z","type":"join"}
{"nick":"dguttman","date":"2013-10-13T22:15:22.364Z","type":"join"}
{"nick":"DTrejo","reason":"Remote host closed the connection","date":"2013-10-13T22:16:28.816Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T22:17:02.381Z","type":"join"}
{"nick":"DTrejo","reason":"Ping timeout: 245 seconds","date":"2013-10-13T22:21:26.172Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T22:35:41.004Z","type":"join"}
{"nick":"DTrejo","reason":"Ping timeout: 272 seconds","date":"2013-10-13T22:40:25.919Z","type":"quit"}
{"nick":"DTrejo","date":"2013-10-13T22:47:51.160Z","type":"join"}
{"nick":"DTrejo","reason":"Ping timeout: 246 seconds","date":"2013-10-13T22:52:04.871Z","type":"quit"}
{"nick":"levelbot","message":"[npm] nitrogen@0.1.44 <http://npm.im/nitrogen>: Nitrogen is a platform for building connected devices.  Nitrogen provides the authentication, authorization, and real time message passing framework so that you can focus on your device and application.  All with a consistent development platform that leverages the ubiquity of Javascript. (@tpark)","date":"2013-10-13T23:13:10.421Z","type":"message"}
{"nick":"thlorenz_","date":"2013-10-13T23:22:03.498Z","type":"join"}
{"nick":"thlorenz_","reason":"Ping timeout: 256 seconds","date":"2013-10-13T23:26:42.299Z","type":"quit"}
