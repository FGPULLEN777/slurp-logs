{"nick":"brianloveswords","date":"2014-05-18T00:01:42.589Z","type":"join"}
{"nick":"brianloveswords","reason":"Quit: Computer has gone to sleep.","date":"2014-05-18T00:17:13.116Z","type":"quit"}
{"nick":"brianloveswords","date":"2014-05-18T00:19:46.050Z","type":"join"}
{"nick":"brianloveswords","reason":"Quit: Computer has gone to sleep.","date":"2014-05-18T00:30:55.472Z","type":"quit"}
{"nick":"pbw__","date":"2014-05-18T00:34:38.384Z","type":"join"}
{"nick":"thlorenz","date":"2014-05-18T00:44:22.611Z","type":"join"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2014-05-18T00:47:36.476Z","type":"quit"}
{"nick":"ramitos","reason":"Remote host closed the connection","date":"2014-05-18T01:16:05.796Z","type":"quit"}
{"nick":"ramitos","date":"2014-05-18T01:17:03.670Z","type":"join"}
{"nick":"jerrysv","reason":"Remote host closed the connection","date":"2014-05-18T01:29:09.230Z","type":"quit"}
{"nick":"ednapiranha","reason":"Quit: Leaving...","date":"2014-05-18T02:06:33.690Z","type":"quit"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2014-05-18T02:06:59.383Z","type":"quit"}
{"nick":"thlorenz","date":"2014-05-18T02:07:31.710Z","type":"join"}
{"nick":"thlorenz","reason":"Read error: Connection reset by peer","date":"2014-05-18T02:08:54.927Z","type":"quit"}
{"nick":"thlorenz","date":"2014-05-18T02:09:15.419Z","type":"join"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2014-05-18T02:13:47.286Z","type":"quit"}
{"nick":"ednapiranha","date":"2014-05-18T02:14:21.345Z","type":"join"}
{"nick":"thlorenz","date":"2014-05-18T02:15:21.564Z","type":"join"}
{"nick":"binocarlos","reason":"Read error: Connection reset by peer","date":"2014-05-18T02:18:50.663Z","type":"quit"}
{"nick":"thlorenz","reason":"Ping timeout: 240 seconds","date":"2014-05-18T02:28:43.061Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T02:37:42.508Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 252 seconds","date":"2014-05-18T02:42:03.396Z","type":"quit"}
{"nick":"pbw__","reason":"Quit: Connection closed for inactivity","date":"2014-05-18T02:42:33.280Z","type":"quit"}
{"nick":"contrahax","date":"2014-05-18T02:53:08.995Z","type":"join"}
{"nick":"contrahax","new_nick":"_contrahax","date":"2014-05-18T02:53:22.902Z","type":"nick"}
{"nick":"tec27","new_nick":"_tec27","date":"2014-05-18T02:53:40.836Z","type":"nick"}
{"nick":"saibotvisad","reason":"Quit: Leaving.","date":"2014-05-18T03:15:28.201Z","type":"quit"}
{"nick":"saibotvisad","date":"2014-05-18T03:19:21.989Z","type":"join"}
{"nick":"saibotvisad","reason":"Client Quit","date":"2014-05-18T03:20:43.337Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T03:31:56.368Z","type":"join"}
{"nick":"_contrahax","new_nick":"contrahax","date":"2014-05-18T03:32:15.000Z","type":"nick"}
{"nick":"_tec27","new_nick":"tec27","date":"2014-05-18T03:32:23.864Z","type":"nick"}
{"nick":"daviddias","reason":"Ping timeout: 258 seconds","date":"2014-05-18T03:36:17.244Z","type":"quit"}
{"nick":"ednapiranha","reason":"Quit: Leaving...","date":"2014-05-18T03:51:27.527Z","type":"quit"}
{"nick":"contrahax","reason":"Quit: Sleeping","date":"2014-05-18T04:01:52.552Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T04:26:04.149Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 245 seconds","date":"2014-05-18T04:30:29.062Z","type":"quit"}
{"nick":"Sorella","reason":"Quit: It is tiem!","date":"2014-05-18T04:34:34.216Z","type":"quit"}
{"nick":"ramitos","reason":"Ping timeout: 240 seconds","date":"2014-05-18T04:37:14.744Z","type":"quit"}
{"nick":"ramitos","date":"2014-05-18T04:38:01.172Z","type":"join"}
{"nick":"ramitos","reason":"Ping timeout: 258 seconds","date":"2014-05-18T04:42:36.257Z","type":"quit"}
{"nick":"ramitos","date":"2014-05-18T04:44:57.896Z","type":"join"}
{"nick":"JasonSmith","message":"How do I stream a value? e.g. I want the value for \"video.mp4\" and I know it's 50TB but I just want to stream it over http?","date":"2014-05-18T04:46:13.872Z","type":"message"}
{"nick":"JasonSmith","message":" /cc jcrugzz Any thoughts? I don't want to stream the db keyvals, just the binary blob of one specific value","date":"2014-05-18T04:46:41.662Z","type":"message"}
{"nick":"jcrugzz","message":"JasonSmith: if you are saying the video.mp4 is in the leveldb database there is no way to stream from a db.get persay","date":"2014-05-18T04:49:41.664Z","type":"message"}
{"nick":"jcrugzz","message":"id use something like https://github.com/dominictarr/content-addressable-store","date":"2014-05-18T04:49:45.222Z","type":"message"}
{"nick":"jcrugzz","message":"and just store the hash and metadata in the leveldb","date":"2014-05-18T04:49:52.379Z","type":"message"}
{"nick":"jcrugzz","message":"and just use the filesystem for the video or binary content","date":"2014-05-18T04:50:09.058Z","type":"message"}
{"nick":"jcrugzz","message":"so you can stream it straight over http","date":"2014-05-18T04:50:24.064Z","type":"message"}
{"nick":"JasonSmith","message":"Oh bummer","date":"2014-05-18T04:53:05.212Z","type":"message"}
{"nick":"JasonSmith","message":"I guess I'm spoiled by couch attachments.","date":"2014-05-18T04:53:31.769Z","type":"message"}
{"nick":"JasonSmith","message":"Thanks jcrugzz","date":"2014-05-18T04:53:46.364Z","type":"message"}
{"nick":"jcrugzz","message":"JasonSmith: yea totally. i think using that type of store you could build a nice wrapper using leveldb and the filesystem","date":"2014-05-18T04:54:10.063Z","type":"message"}
{"nick":"jcrugzz","message":"JasonSmith: to make the same type of experience","date":"2014-05-18T04:54:27.828Z","type":"message"}
{"nick":"jcrugzz","message":"and np","date":"2014-05-18T04:54:33.388Z","type":"message"}
{"nick":"JasonSmith","message":"Surely it's not a limitation of level down","date":"2014-05-18T04:57:14.668Z","type":"message"}
{"nick":"JasonSmith","message":"I should think the c++ API let's you store values which are larger than system memory","date":"2014-05-18T04:57:45.666Z","type":"message"}
{"nick":"JasonSmith","message":"Maybe I could build a stream that just stores 1MB chunks in a sublevel","date":"2014-05-18T04:58:48.284Z","type":"message"}
{"nick":"saibotvisad","date":"2014-05-18T05:13:57.009Z","type":"join"}
{"nick":"blessYahu","reason":"Ping timeout: 252 seconds","date":"2014-05-18T05:17:31.404Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T05:20:14.151Z","type":"join"}
{"nick":"jcrugzz","message":"JasonSmith: yea i think theres just no API for it. But the filesystem is still better than leveldb is at storing files iirc cc juliangruber. Due to how the GC works","date":"2014-05-18T05:22:07.362Z","type":"message"}
{"nick":"daviddias","reason":"Ping timeout: 245 seconds","date":"2014-05-18T05:24:14.045Z","type":"quit"}
{"nick":"fritzy","date":"2014-05-18T05:52:25.020Z","type":"join"}
{"nick":"saibotvisad1","date":"2014-05-18T06:04:20.967Z","type":"join"}
{"nick":"saibotvisad","reason":"Ping timeout: 276 seconds","date":"2014-05-18T06:07:55.975Z","type":"quit"}
{"nick":"mikeal","date":"2014-05-18T06:16:45.529Z","type":"join"}
{"nick":"daviddias","date":"2014-05-18T06:19:36.517Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 252 seconds","date":"2014-05-18T06:23:53.409Z","type":"quit"}
{"nick":"saibotvisad1","reason":"Quit: Leaving.","date":"2014-05-18T06:51:03.088Z","type":"quit"}
{"nick":"fritzy","reason":"Remote host closed the connection","date":"2014-05-18T07:36:10.607Z","type":"quit"}
{"nick":"fritzy","date":"2014-05-18T07:39:11.960Z","type":"join"}
{"nick":"fritzy","reason":"Remote host closed the connection","date":"2014-05-18T08:07:44.855Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T08:14:23.164Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 245 seconds","date":"2014-05-18T08:18:49.046Z","type":"quit"}
{"nick":"sygi","date":"2014-05-18T08:37:19.020Z","type":"join"}
{"nick":"contrahax","date":"2014-05-18T08:38:31.185Z","type":"join"}
{"nick":"contrahax","reason":"Quit: Sleeping","date":"2014-05-18T09:06:03.071Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T10:02:21.415Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 252 seconds","date":"2014-05-18T10:06:49.298Z","type":"quit"}
{"nick":"rescrv","message":"JasonSmith: you'd be best to put anything over 2MB onto the filesystem and store a pointer to it.","date":"2014-05-18T12:36:58.909Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: why?","date":"2014-05-18T12:37:19.976Z","type":"message"}
{"nick":"rescrv","message":"JasonSmith jcrugzz: my recommendation is to not store anything over 4K in LevelDB values","date":"2014-05-18T12:37:28.234Z","type":"message"}
{"nick":"JasonSmith","message":"why not?","date":"2014-05-18T12:37:35.727Z","type":"message"}
{"nick":"rescrv","message":"internally, the SSTs are 2MB","date":"2014-05-18T12:37:39.085Z","type":"message"}
{"nick":"rescrv","message":"if you have a file that is bigger than 2MB, you're going to end up with one kv pair per SST","date":"2014-05-18T12:37:56.570Z","type":"message"}
{"nick":"rescrv","message":"with all the overhead of the SST, and none of the flexibility of a file (like streaming)","date":"2014-05-18T12:38:08.370Z","type":"message"}
{"nick":"JasonSmith","message":"ok, I thought leveldb was some kind of spiritual replacement for bigtable, and so I thought it would be comfortable with very large values","date":"2014-05-18T12:38:08.765Z","type":"message"}
{"nick":"rescrv","message":"the reason I recommend 4K is because you get a couple hundred values into each SST","date":"2014-05-18T12:38:41.066Z","type":"message"}
{"nick":"JasonSmith","message":"yeah","date":"2014-05-18T12:39:06.396Z","type":"message"}
{"nick":"JasonSmith","message":"4k is really tiny, that's just one disk block","date":"2014-05-18T12:39:11.636Z","type":"message"}
{"nick":"JasonSmith","message":"Not disagreeing with you","date":"2014-05-18T12:39:18.540Z","type":"message"}
{"nick":"rescrv","message":"JasonSmith: it's built using many ideas from BigTable.  One idea they have in BigTable is to put large files in GFS (now Colossus), and refer to them from the BigTable value.","date":"2014-05-18T12:39:26.509Z","type":"message"}
{"nick":"JasonSmith","message":"I would imagine people are storing small binary files like images in level though, no problem, right?","date":"2014-05-18T12:39:36.899Z","type":"message"}
{"nick":"rescrv","message":"yes","date":"2014-05-18T12:39:41.274Z","type":"message"}
{"nick":"JasonSmith","message":"Oh, this channel is general leveldb? I thought it was all Node.js people :)","date":"2014-05-18T12:40:29.618Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: Ok, thanks for the perspective","date":"2014-05-18T12:40:38.112Z","type":"message"}
{"nick":"rescrv","message":"there's a lot of confusion on that.  I'm one of the non-Node.js devs (I've used it; not my primary platform), and the message in the main channel blindly directs people here, so I'm here to help out when they end up here.","date":"2014-05-18T12:41:13.091Z","type":"message"}
{"nick":"JasonSmith","message":"That's good. I don't know the code, but based on how abstract LevelUp is (it now has LDAP and MySQL back-ends) I imagine you lose some lowe-level features with that abstraction","date":"2014-05-18T12:42:06.111Z","type":"message"}
{"nick":"rescrv","message":"I don't think you lose many.  rvagg did a pretty good job, and levelup came from the leveldb API if I'm not mistaken.  The only thing I know is missing is explicit snapshot support.","date":"2014-05-18T12:42:58.673Z","type":"message"}
{"nick":"thlorenz","date":"2014-05-18T12:43:19.117Z","type":"join"}
{"nick":"rescrv","message":"you also lose the ability to manage your own memory, but if you're using JS, you want to lose that anyway","date":"2014-05-18T12:43:23.349Z","type":"message"}
{"nick":"JasonSmith","message":"I think for the Node.js world, leveldb was \"good enough\" (don't get me wrong, it's quite good) but basically it was right time right place when the community needed to build momentum around a very simple get/put/del API","date":"2014-05-18T12:43:25.422Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: Do you know of any replication projects out there that have gained much community momentum?","date":"2014-05-18T12:44:15.146Z","type":"message"}
{"nick":"rescrv","message":"JasonSmith: others can say that better.  I'm partial to HyperDex, but I wrote it, and it's not embeddable.  It does have Node bindings though.","date":"2014-05-18T12:44:42.989Z","type":"message"}
{"nick":"JasonSmith","message":"That is what I'm working on. I'm using Node because of the convenient interception points of updates","date":"2014-05-18T12:44:50.894Z","type":"message"}
{"nick":"JasonSmith","message":"My current theory is a \"sublevel\" (sorry to get levelup-specific) with a change log","date":"2014-05-18T12:45:29.928Z","type":"message"}
{"nick":"JasonSmith","message":"I intercept writes and add a change entry to the batch","date":"2014-05-18T12:45:42.214Z","type":"message"}
{"nick":"JasonSmith","message":"I am basing it on the CouchDB model (I'm a CouchDB developer) so I may have a hammer-nail problem. But I think it will be simple; however I haven't figured out how to replicate batch updates yet (transactions)","date":"2014-05-18T12:46:41.622Z","type":"message"}
{"nick":"rescrv","message":"You'll have difficulty there because LevelDB doesn't keep it as a batch internally","date":"2014-05-18T12:47:39.982Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: Oh really, I thought I could get an all-or-nothing semantic from a batch call?","date":"2014-05-18T12:47:59.721Z","type":"message"}
{"nick":"JasonSmith","message":"I'm looking at https://github.com/rvagg/node-levelup#batch","date":"2014-05-18T12:48:39.632Z","type":"message"}
{"nick":"JasonSmith","message":"In Node, there is a way to intercept and amend batch operations before they are sent to leveldown","date":"2014-05-18T12:48:53.663Z","type":"message"}
{"nick":"rescrv","message":"You do.  But unless you take a snapshot immediately after the batch completes, with no intervening writes, you won't be able to read those items as they were in the batch, because someone else could have overwrote them.","date":"2014-05-18T12:49:37.158Z","type":"message"}
{"nick":"JasonSmith","message":"I would be comfortable with no batch guarantees though. That's how Couch does it because it's \"impossible\" to guarantee correct replication in general","date":"2014-05-18T12:49:40.909Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: Right, I see","date":"2014-05-18T12:49:52.210Z","type":"message"}
{"nick":"rescrv","message":"I would disagree on that \"impossible\", unless we're going to the absurdity that it's impossible to write applications in general.","date":"2014-05-18T12:50:18.407Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: Yes that is quite similar to my batch replication problem","date":"2014-05-18T12:50:24.404Z","type":"message"}
{"nick":"JasonSmith","message":"Yes I quoted \"impossible,\" it is doable but in the Couch model it's considered not worth it. It sacrifices too much architectural simplicity","date":"2014-05-18T12:50:44.606Z","type":"message"}
{"nick":"rescrv","message":"hyperleveldb provides a replay iterator that will give you most of what you want","date":"2014-05-18T12:51:03.522Z","type":"message"}
{"nick":"JasonSmith","message":"ok, thanks","date":"2014-05-18T12:51:12.873Z","type":"message"}
{"nick":"rescrv","message":"whatever consumes the replay iterator will move forward in time","date":"2014-05-18T12:51:16.409Z","type":"message"}
{"nick":"rescrv","message":"this is not wrapped by levelup, so you'll have to add that","date":"2014-05-18T12:51:30.535Z","type":"message"}
{"nick":"JasonSmith","message":"right","date":"2014-05-18T12:51:36.126Z","type":"message"}
{"nick":"JasonSmith","message":"in couch, the change log is very simple. You have an index of changes (first change gets id 1, second gets id 2, etc.)","date":"2014-05-18T12:52:06.658Z","type":"message"}
{"nick":"JasonSmith","message":"The trick is, every key appears in the change log exactly once, guaranteed","date":"2014-05-18T12:52:32.376Z","type":"message"}
{"nick":"JasonSmith","message":"so to get the latest state, you just pull down changes from your last checkpoint (e.g. \"show me changes since id 1234)","date":"2014-05-18T12:52:48.641Z","type":"message"}
{"nick":"rescrv","message":"how do you guarantee a key won't appear twice, e.g., if you update its value?","date":"2014-05-18T12:53:19.837Z","type":"message"}
{"nick":"rescrv","message":"the replay iterator works much the same way","date":"2014-05-18T12:54:15.260Z","type":"message"}
{"nick":"JasonSmith","message":"Your index is (conceptually) change ids (integers) to keys, so suppose you do put(\"foo\",\"I am foo\"); put(\"bar\", \"I am bar\");put(\"foo\", \"I am the 2nd foo\")","date":"2014-05-18T12:54:27.451Z","type":"message"}
{"nick":"JasonSmith","message":"the index would be [1=\"foo\"] then [1=\"foo\", 2=\"bar\"] then finally [2=\"bar\", 3=\"foo\"]","date":"2014-05-18T12:55:03.522Z","type":"message"}
{"nick":"rescrv","message":"got it","date":"2014-05-18T12:55:23.582Z","type":"message"}
{"nick":"JasonSmith","message":"So if I scan that index starting from 0, I will get \"bar\", \"foo\"","date":"2014-05-18T12:55:26.940Z","type":"message"}
{"nick":"rescrv","message":"so you have a secondary structure looking into the log","date":"2014-05-18T12:55:31.942Z","type":"message"}
{"nick":"JasonSmith","message":"I will look up those values and I've got my replica","date":"2014-05-18T12:55:32.508Z","type":"message"}
{"nick":"JasonSmith","message":"yes","date":"2014-05-18T12:55:37.007Z","type":"message"}
{"nick":"rescrv","message":"leveldb uses sequence numbers internally to order writes","date":"2014-05-18T12:55:57.207Z","type":"message"}
{"nick":"JasonSmith","message":"Obviously though, integrity between values is not supported though. That's the price. Hey, it's NoSQL. Big whoop.","date":"2014-05-18T12:55:58.744Z","type":"message"}
{"nick":"rescrv","message":"the replay iterator works on those sequence numbers","date":"2014-05-18T12:56:08.435Z","type":"message"}
{"nick":"JasonSmith","message":"Right, it is actually called the by_sequence tree internally","date":"2014-05-18T12:56:13.548Z","type":"message"}
{"nick":"rescrv","message":"there's no way to control them on the other end though","date":"2014-05-18T12:56:15.151Z","type":"message"}
{"nick":"JasonSmith","message":"in CouchDB there is a REST API to read it. In fact, confidentially, this is the only nice thing about couch. Ha, ha I kid!","date":"2014-05-18T12:56:44.376Z","type":"message"}
{"nick":"thlorenz","reason":"Ping timeout: 255 seconds","date":"2014-05-18T12:56:47.632Z","type":"quit"}
{"nick":"rescrv","message":"we use the replay iterator to give us integrity between values in HyperDex (with a lot of logic on top)","date":"2014-05-18T12:56:56.787Z","type":"message"}
{"nick":"JasonSmith","message":"There is actually no replication \"protocol.\" You just query that index and then fetch your values by key (or fetch \"documents\" by \"id\" in couch terminology)","date":"2014-05-18T12:57:17.518Z","type":"message"}
{"nick":"rescrv","message":"yeah.  That's probably a lot simpler to implement.","date":"2014-05-18T12:57:31.837Z","type":"message"}
{"nick":"JasonSmith","message":"Hyperdex is an alternative to leveldb?","date":"2014-05-18T12:57:40.367Z","type":"message"}
{"nick":"JasonSmith","message":"or a fork?","date":"2014-05-18T12:57:42.929Z","type":"message"}
{"nick":"JasonSmith","message":"Forgive me, I'm expanding my database horizons :)","date":"2014-05-18T12:57:57.791Z","type":"message"}
{"nick":"JasonSmith","message":"Been steeped in CouchDB since forever","date":"2014-05-18T12:58:03.495Z","type":"message"}
{"nick":"rescrv","message":"HyperDex is a distributed key-value store that uses LevelDB internally as its backend (really, HyperLevelDB).","date":"2014-05-18T12:58:05.983Z","type":"message"}
{"nick":"JasonSmith","message":"HyperLevelDB is a fork then?","date":"2014-05-18T12:58:23.441Z","type":"message"}
{"nick":"rescrv","message":"yes","date":"2014-05-18T12:58:26.304Z","type":"message"}
{"nick":"JasonSmith","message":"I've heard that there are a few forks out there","date":"2014-05-18T12:58:27.597Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: And you're a HyperDex developer?","date":"2014-05-18T12:58:35.985Z","type":"message"}
{"nick":"rescrv","message":"it's the fork of LevelDB that we use in HyperDex","date":"2014-05-18T12:58:37.441Z","type":"message"}
{"nick":"rescrv","message":"yes","date":"2014-05-18T12:58:38.923Z","type":"message"}
{"nick":"JasonSmith","message":"Cool! nice to meet you.","date":"2014-05-18T12:58:56.045Z","type":"message"}
{"nick":"JasonSmith","message":"Does HyperDex sort of compete with Redis then?","date":"2014-05-18T12:59:08.029Z","type":"message"}
{"nick":"ogd","message":"JasonSmith: check out dat, it's my NIH version of couchdb :P","date":"2014-05-18T12:59:16.773Z","type":"message"}
{"nick":"ogd","message":"JasonSmith: im actually working on putting npm into dat now","date":"2014-05-18T12:59:29.374Z","type":"message"}
{"nick":"JasonSmith","message":"ogd: Hey Max. Long time.","date":"2014-05-18T12:59:40.380Z","type":"message"}
{"nick":"JasonSmith","message":"ogd: Yeah I am trying to make a SLEEPY feature in Levelup","date":"2014-05-18T13:00:07.483Z","type":"message"}
{"nick":"ogd","message":"JasonSmith: nice, theres https://www.npmjs.org/package/sleep-ref which is okay","date":"2014-05-18T13:00:27.315Z","type":"message"}
{"nick":"ogd","message":"just handles the http/tcp part","date":"2014-05-18T13:00:42.642Z","type":"message"}
{"nick":"thlorenz","date":"2014-05-18T13:01:06.334Z","type":"join"}
{"nick":"rescrv","message":"Nice to meet you as well!  We're not directly competing with any one system.  We have many of Redis's data structures, but we persist to disk, while they are in main memory.  We offer document support like MongoDB, and with better fault tolerance too.  It's a mix of things we think people would need.","date":"2014-05-18T13:01:52.219Z","type":"message"}
{"nick":"JasonSmith","message":"ogd: \"Syncasble\" - is that a typo or some term of art?","date":"2014-05-18T13:01:57.362Z","type":"message"}
{"nick":"ogd","message":"lol","date":"2014-05-18T13:02:04.373Z","type":"message"}
{"nick":"ogd","message":"typo!","date":"2014-05-18T13:02:07.513Z","type":"message"}
{"nick":"ogd","message":"or danish for 'it synchronizes'","date":"2014-05-18T13:02:19.822Z","type":"message"}
{"nick":"JasonSmith","message":"Right it sounds like some jargon invented in Oakland","date":"2014-05-18T13:02:39.394Z","type":"message"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2014-05-18T13:02:41.640Z","type":"quit"}
{"nick":"JasonSmith","message":":p","date":"2014-05-18T13:02:42.912Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: Cool! Yes I have heard of HyperDex. I've used Redis but I fear it has some scaling issues","date":"2014-05-18T13:03:16.758Z","type":"message"}
{"nick":"rescrv","message":"most people I know that use Redis for more than a local cache (like you'd use memcached) have written their own network stack for it.","date":"2014-05-18T13:05:03.100Z","type":"message"}
{"nick":"rescrv","message":"I should clarify that it's most people I've had long serious discussions with.  Not causal blog posts or the like","date":"2014-05-18T13:07:14.421Z","type":"message"}
{"nick":"JasonSmith","message":"Yes. It's true that every site that scales up needs to address its own specific problems in its own specific ways","date":"2014-05-18T13:09:05.474Z","type":"message"}
{"nick":"JasonSmith","message":"but making a custom network stack for Redis sounds like a major major yak shave to me","date":"2014-05-18T13:09:20.279Z","type":"message"}
{"nick":"JasonSmith","message":"The tragedy of scaling is that it's not even \"one size doesn't fit all.\" It's that \"every size fits at most one\"","date":"2014-05-18T13:10:28.533Z","type":"message"}
{"nick":"rescrv","message":"that's not quite true.  People very much prefer to do things the way they are comfortable, and would much rather incrementally improve the design they have than wholesale switch to a better one.  It's why you see people layering memcache on top of cassandra on top of postgres, rather than adopting a backend with better throughput and latency.","date":"2014-05-18T13:12:21.714Z","type":"message"}
{"nick":"JasonSmith","message":"Yes that's true.","date":"2014-05-18T13:12:51.205Z","type":"message"}
{"nick":"JasonSmith","message":"I used to work at Grindr.","date":"2014-05-18T13:12:55.556Z","type":"message"}
{"nick":"JasonSmith","message":"which is a pretty big social network","date":"2014-05-18T13:13:02.922Z","type":"message"}
{"nick":"JasonSmith","message":"They were on Google App Engine and they were paying easily 10x \"too much\" for hosting","date":"2014-05-18T13:13:18.548Z","type":"message"}
{"nick":"JasonSmith","message":"it was topping over $50k per month by the time I left","date":"2014-05-18T13:13:27.624Z","type":"message"}
{"nick":"rescrv","message":"could they have done better with 4 in house engineers to write a replacement?","date":"2014-05-18T13:14:33.386Z","type":"message"}
{"nick":"JasonSmith","message":"App Engine is a very tempting platform to prototype but wow it gets expensive if you grow, and you are very much locked-in. I must admit though, as far as the technology, it did scale as promised","date":"2014-05-18T13:14:50.298Z","type":"message"}
{"nick":"JasonSmith","message":"rescrv: In my opinion yes. I was just a consultant and I wanted to get more involved but the owner did not agree","date":"2014-05-18T13:15:15.615Z","type":"message"}
{"nick":"rescrv","message":"and that 50K per month would have gotten you at most 4 additional engineers and no hosting.","date":"2014-05-18T13:15:28.584Z","type":"message"}
{"nick":"JasonSmith","message":"They were only seeing about 20-50 hits a second","date":"2014-05-18T13:15:52.614Z","type":"message"}
{"nick":"rescrv","message":"Average?  Median?  Peak?","date":"2014-05-18T13:16:11.886Z","type":"message"}
{"nick":"JasonSmith","message":"that's not very expensive hosting, even if you multiply for high-availibility","date":"2014-05-18T13:16:15.434Z","type":"message"}
{"nick":"JasonSmith","message":"peak","date":"2014-05-18T13:16:17.844Z","type":"message"}
{"nick":"JasonSmith","message":"median was about 20","date":"2014-05-18T13:16:21.420Z","type":"message"}
{"nick":"JasonSmith","message":"this was 3 years ago though","date":"2014-05-18T13:16:32.494Z","type":"message"}
{"nick":"rescrv","message":"then that is a little pricey.","date":"2014-05-18T13:16:39.302Z","type":"message"}
{"nick":"JasonSmith","message":"Yes, I ran npm after that","date":"2014-05-18T13:16:47.622Z","type":"message"}
{"nick":"JasonSmith","message":"the node.js npm registry","date":"2014-05-18T13:16:50.652Z","type":"message"}
{"nick":"JasonSmith","message":"It's apples-oranges perhaps. But we were processing 900 HTTP hits per second","date":"2014-05-18T13:17:06.787Z","type":"message"}
{"nick":"JasonSmith","message":"peak","date":"2014-05-18T13:17:09.697Z","type":"message"}
{"nick":"JasonSmith","message":"on just a mid-low-end SoftLayer box","date":"2014-05-18T13:17:24.265Z","type":"message"}
{"nick":"JasonSmith","message":"The only reason we had replication and load balancing is we needed high-availability anyway, so we may as well balance the load while we were at it","date":"2014-05-18T13:17:45.122Z","type":"message"}
{"nick":"rescrv","message":"it's all about what's being done.  I don't know Grindr's internals, but Facebook can issue 1000s of hits against memcache per wall view.  Imagine if that were not cached, or were charged per request.","date":"2014-05-18T13:18:38.879Z","type":"message"}
{"nick":"JasonSmith","message":"Right, I think they added a memcache API after I left. When I was there they basically only had the basic compute service and data store API","date":"2014-05-18T13:19:18.513Z","type":"message"}
{"nick":"JasonSmith","message":"Grindr was very simple, there were login/logout and stuff. But 99% of the cost was processing proximity searches","date":"2014-05-18T13:20:02.322Z","type":"message"}
{"nick":"JasonSmith","message":"(the idea is finding other users nearby)","date":"2014-05-18T13:20:06.312Z","type":"message"}
{"nick":"JasonSmith","message":"On a NoSQL database that can get tricky","date":"2014-05-18T13:20:14.508Z","type":"message"}
{"nick":"JasonSmith","message":"There's a really brilliant video and I think writeup out there about \"geoboxes.\" It's IMO a very clever way to do proximity search in a key-value database","date":"2014-05-18T13:21:01.295Z","type":"message"}
{"nick":"JasonSmith","message":"Basically you break the earth into lat-long squares, and every square has a unique ID","date":"2014-05-18T13:21:35.315Z","type":"message"}
{"nick":"JasonSmith","message":"The trick though, is you do that at several different resolutions","date":"2014-05-18T13:21:46.899Z","type":"message"}
{"nick":"JasonSmith","message":"A small square might be a few meters, a large one might be tens of degress of latitude","date":"2014-05-18T13:21:59.005Z","type":"message"}
{"nick":"JasonSmith","message":"To find people nearby, you find the ID of your squares (\"geo boxes\") at every resolution. Say there are 5 resolution levels","date":"2014-05-18T13:22:36.445Z","type":"message"}
{"nick":"JasonSmith","message":"Then you do 5 queries, basically \"show all users registered for geobox with ID <foo>\"","date":"2014-05-18T13:22:59.766Z","type":"message"}
{"nick":"JasonSmith","message":"Then you sort in memory","date":"2014-05-18T13:23:02.275Z","type":"message"}
{"nick":"JasonSmith","message":"quite clever","date":"2014-05-18T13:23:05.031Z","type":"message"}
{"nick":"JasonSmith","message":"using pythagorean or whatever","date":"2014-05-18T13:23:17.160Z","type":"message"}
{"nick":"JasonSmith","message":"If you prefer fewer lookups but possibly longer latency, you can query for the smallest boxes first, and only follow up with larger ones if you haven't found enough people","date":"2014-05-18T13:24:12.578Z","type":"message"}
{"nick":"Sorella","date":"2014-05-18T13:40:03.345Z","type":"join"}
{"nick":"thlorenz","date":"2014-05-18T13:43:22.413Z","type":"join"}
{"nick":"rescrv","message":"JasonSmith: sounds like an rtree.  And HyperDex's hyperspace hashing could be applied to that problem quite well to make it efficient.","date":"2014-05-18T14:08:17.733Z","type":"message"}
{"nick":"JasonSmith","message":"Oh, nice. I used to work with a guy who built an r-tree index for CouchDB","date":"2014-05-18T14:08:52.834Z","type":"message"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2014-05-18T14:23:49.092Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T14:33:59.297Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 240 seconds","date":"2014-05-18T14:38:18.452Z","type":"quit"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2014-05-18T15:19:30.538Z","type":"quit"}
{"nick":"mikeal","date":"2014-05-18T15:22:08.142Z","type":"join"}
{"nick":"daviddias","date":"2014-05-18T15:28:00.157Z","type":"join"}
{"nick":"brianloveswords","date":"2014-05-18T15:30:13.464Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 245 seconds","date":"2014-05-18T15:32:09.052Z","type":"quit"}
{"nick":"brianloveswords","reason":"Quit: Computer has gone to sleep.","date":"2014-05-18T16:11:43.984Z","type":"quit"}
{"nick":"brianloveswords","date":"2014-05-18T16:16:55.609Z","type":"join"}
{"nick":"ednapiranha","date":"2014-05-18T16:19:30.209Z","type":"join"}
{"nick":"ednapiranha","reason":"Client Quit","date":"2014-05-18T16:20:22.132Z","type":"quit"}
{"nick":"thefoxis","date":"2014-05-18T16:56:36.612Z","type":"join"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2014-05-18T17:08:49.666Z","type":"quit"}
{"nick":"ednapiranha","date":"2014-05-18T17:15:00.239Z","type":"join"}
{"nick":"daviddias","date":"2014-05-18T17:16:15.508Z","type":"join"}
{"nick":"mikeal","date":"2014-05-18T17:18:55.766Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 252 seconds","date":"2014-05-18T17:20:35.475Z","type":"quit"}
{"nick":"ednapiranha","reason":"Quit: Leaving...","date":"2014-05-18T17:21:41.734Z","type":"quit"}
{"nick":"blessYahu","date":"2014-05-18T17:37:27.363Z","type":"join"}
{"nick":"mafintosh","reason":"Ping timeout: 258 seconds","date":"2014-05-18T17:59:56.263Z","type":"quit"}
{"nick":"daleharvey","reason":"Ping timeout: 276 seconds","date":"2014-05-18T18:00:19.797Z","type":"quit"}
{"nick":"mafintosh","date":"2014-05-18T18:02:10.275Z","type":"join"}
{"nick":"ralphtheninja","message":"level-* should only be for pure level modules right?","date":"2014-05-18T18:02:36.373Z","type":"message"}
{"nick":"ralphtheninja","message":"and not for applications that is ..","date":"2014-05-18T18:02:57.196Z","type":"message"}
{"nick":"daleharvey","date":"2014-05-18T18:05:18.462Z","type":"join"}
{"nick":"brianloveswords","reason":"Quit: Computer has gone to sleep.","date":"2014-05-18T18:20:08.729Z","type":"quit"}
{"nick":"fritzy","date":"2014-05-18T18:51:40.945Z","type":"join"}
{"nick":"fritzy_","date":"2014-05-18T18:52:55.922Z","type":"join"}
{"nick":"fritzy","reason":"Remote host closed the connection","date":"2014-05-18T18:53:08.225Z","type":"quit"}
{"nick":"thlorenz","date":"2014-05-18T18:53:22.247Z","type":"join"}
{"nick":"thefoxis","reason":"Quit: Connection closed for inactivity","date":"2014-05-18T19:03:17.295Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T19:04:32.409Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 258 seconds","date":"2014-05-18T19:08:56.356Z","type":"quit"}
{"nick":"fritzy_","reason":"Remote host closed the connection","date":"2014-05-18T19:30:48.565Z","type":"quit"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2014-05-18T19:38:07.262Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T19:58:36.719Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 255 seconds","date":"2014-05-18T20:02:56.532Z","type":"quit"}
{"nick":"daviddias","date":"2014-05-18T20:04:03.135Z","type":"join"}
{"nick":"daviddias","reason":"Ping timeout: 245 seconds","date":"2014-05-18T20:11:19.021Z","type":"quit"}
{"nick":"fritzy","date":"2014-05-18T20:27:24.363Z","type":"join"}
{"nick":"fritzy","reason":"Remote host closed the connection","date":"2014-05-18T20:55:05.631Z","type":"quit"}
{"nick":"fritzy","date":"2014-05-18T21:44:57.667Z","type":"join"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2014-05-18T21:51:07.088Z","type":"quit"}
{"nick":"mafintosh","reason":"*.net *.split","date":"2014-05-18T22:13:32.271Z","type":"quit"}
{"nick":"wolfeidau","reason":"*.net *.split","date":"2014-05-18T22:13:32.385Z","type":"quit"}
{"nick":"dstokes","reason":"*.net *.split","date":"2014-05-18T22:13:32.861Z","type":"quit"}
{"nick":"ggreer","reason":"*.net *.split","date":"2014-05-18T22:13:32.980Z","type":"quit"}
{"nick":"jez0990","reason":"*.net *.split","date":"2014-05-18T22:13:32.981Z","type":"quit"}
{"nick":"gyaresu","reason":"*.net *.split","date":"2014-05-18T22:13:32.981Z","type":"quit"}
{"nick":"chilts","reason":"*.net *.split","date":"2014-05-18T22:13:32.982Z","type":"quit"}
{"nick":"ramitos","reason":"*.net *.split","date":"2014-05-18T22:13:33.222Z","type":"quit"}
{"nick":"chapel","reason":"*.net *.split","date":"2014-05-18T22:13:33.582Z","type":"quit"}
{"nick":"book`","reason":"*.net *.split","date":"2014-05-18T22:13:33.821Z","type":"quit"}
{"nick":"jayne","reason":"*.net *.split","date":"2014-05-18T22:13:35.101Z","type":"quit"}
{"nick":"mafintosh","date":"2014-05-18T22:19:36.795Z","type":"join"}
{"nick":"ramitos","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"wolfeidau","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"chilts","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"gyaresu","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"jez0990","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"ggreer","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"dstokes","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"book`","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"chapel","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"jayne","date":"2014-05-18T22:19:36.796Z","type":"join"}
{"nick":"sygi","reason":"Quit: Connection closed for inactivity","date":"2014-05-18T22:52:28.971Z","type":"quit"}
{"nick":"fritzy","reason":"Remote host closed the connection","date":"2014-05-18T23:05:07.513Z","type":"quit"}
{"nick":"dguttman","date":"2014-05-18T23:24:20.119Z","type":"join"}
