{"nick":"eugeneware","reason":"Ping timeout: 260 seconds","date":"2013-08-29T00:00:51.190Z","type":"quit"}
{"nick":"esundahl","reason":"Ping timeout: 264 seconds","date":"2013-08-29T00:03:42.242Z","type":"quit"}
{"nick":"fallsemo","reason":"Quit: Leaving.","date":"2013-08-29T00:06:28.179Z","type":"quit"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-08-29T00:10:15.610Z","type":"quit"}
{"nick":"timoxley_","date":"2013-08-29T00:12:45.487Z","type":"join"}
{"nick":"i_m_ca","reason":"Ping timeout: 256 seconds","date":"2013-08-29T00:14:40.648Z","type":"quit"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-08-29T00:16:47.422Z","type":"quit"}
{"nick":"mbalho","message":"recommendations on how to get batch sizes? im trying to figure out if i'm exceeding my writeBufferSize","date":"2013-08-29T00:22:44.972Z","type":"message"}
{"nick":"timoxley_","reason":"Remote host closed the connection","date":"2013-08-29T00:23:20.836Z","type":"quit"}
{"nick":"kenansulayman","reason":"Quit: ∞♡∞","date":"2013-08-29T00:27:09.518Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T00:34:25.309Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 264 seconds","date":"2013-08-29T00:38:55.014Z","type":"quit"}
{"nick":"soldair","reason":"Quit: Page closed","date":"2013-08-29T00:42:31.262Z","type":"quit"}
{"nick":"dguttman","reason":"Quit: dguttman","date":"2013-08-29T00:44:33.414Z","type":"quit"}
{"nick":"jxson","reason":"Remote host closed the connection","date":"2013-08-29T00:46:54.719Z","type":"quit"}
{"nick":"Raynos","message":"rvagg: levelup does not fsync by default. If fsync fails. i.e. the value being put at a key was not actually committed to the database how do I get an error of that failure ?","date":"2013-08-29T00:52:36.660Z","type":"message"}
{"nick":"Raynos","message":"I genuinely don't understand how one handles errors if you dont wait for fsync","date":"2013-08-29T00:52:51.306Z","type":"message"}
{"nick":"timoxley","date":"2013-08-29T00:54:58.282Z","type":"join"}
{"nick":"thlorenz","date":"2013-08-29T00:58:32.980Z","type":"join"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T01:04:43.117Z","type":"quit"}
{"nick":"eugeneware","date":"2013-08-29T01:11:28.295Z","type":"join"}
{"nick":"jcrugzz_","date":"2013-08-29T01:16:55.216Z","type":"join"}
{"nick":"jcrugzz","reason":"Ping timeout: 264 seconds","date":"2013-08-29T01:19:40.365Z","type":"quit"}
{"nick":"eugeneware","reason":"Remote host closed the connection","date":"2013-08-29T01:27:33.399Z","type":"quit"}
{"nick":"rvagg","message":"Raynos: all you have available to you is the 'sync' flag for writes (batch and put and writestream)","date":"2013-08-29T01:28:37.992Z","type":"message"}
{"nick":"rvagg","message":"if you don't use fsync then you're probably going to have to wait for an error from the next i/o operation that has a problem","date":"2013-08-29T01:28:55.029Z","type":"message"}
{"nick":"rvagg","message":"apart from that... I'm not sure if errors bubble up from the OS when you don't do an fsync","date":"2013-08-29T01:29:29.940Z","type":"message"}
{"nick":"brycebaril","message":"I bet you could test it pretty easily","date":"2013-08-29T01:29:54.209Z","type":"message"}
{"nick":"brycebaril","message":"just start writing every few ms and in the middle of that change the write perms on the db","date":"2013-08-29T01:30:22.301Z","type":"message"}
{"nick":"rvagg","message":"mbalho: Riak chooses a random writeBuffer between 30M and 60M for each of its nodes","date":"2013-08-29T01:30:22.554Z","type":"message"}
{"nick":"rvagg","message":"mbalho: so feel free to go into that order; seems a little high for me but they've obviously found that it works","date":"2013-08-29T01:30:37.807Z","type":"message"}
{"nick":"esundahl","date":"2013-08-29T01:30:42.379Z","type":"join"}
{"nick":"rvagg","message":"I probably wouldn't push beyond 128M unless you can prove perf improvement","date":"2013-08-29T01:30:51.518Z","type":"message"}
{"nick":"rvagg","message":"mbalho: one minor thing about write buffer size is that it increases the time it takes to *open* a database the next time because it does a flush on open","date":"2013-08-29T01:31:13.438Z","type":"message"}
{"nick":"rvagg","message":"s/128/64","date":"2013-08-29T01:31:35.358Z","type":"message"}
{"nick":"rvagg","message":"not sure why I said 128...","date":"2013-08-29T01:31:42.740Z","type":"message"}
{"nick":"esundahl","reason":"Remote host closed the connection","date":"2013-08-29T01:32:02.663Z","type":"quit"}
{"nick":"eugeneware","date":"2013-08-29T01:32:59.317Z","type":"join"}
{"nick":"rescrv","message":"brycebaril, rvagg: OS errors bubble up and are returned on subsequent writes","date":"2013-08-29T01:34:07.028Z","type":"message"}
{"nick":"rescrv","message":"brycebaril: I found 16M to be a sweet-spot for HyperLevelDB and LevelDB.  Basho's design needs a little higher write buffer.","date":"2013-08-29T01:34:43.861Z","type":"message"}
{"nick":"thlorenz_","date":"2013-08-29T01:34:48.918Z","type":"join"}
{"nick":"rescrv","message":"Anything abouve 64M is useless as the implementation will internally cap it at ~64MB","date":"2013-08-29T01:35:08.790Z","type":"message"}
{"nick":"jmartins","reason":"Ping timeout: 264 seconds","date":"2013-08-29T01:35:30.284Z","type":"quit"}
{"nick":"brycebaril","message":"rescrv: so in hyperdex does it always sync (or default to fsync)?","date":"2013-08-29T01:35:37.525Z","type":"message"}
{"nick":"rescrv","message":"Raynos: by default, LevelDB will have passed the operation off to the OS for flushing.  It's not a guarantee (even with fsync, you can have gaps if the fs is bad), but it's \"good enough\" for some apps.","date":"2013-08-29T01:36:53.760Z","type":"message"}
{"nick":"rescrv","message":"brycebaril: HyperDex does not employ local fsync.  Fsync provides durability on one node, which is a useful property, but HyperDex provides f-fault-tolerance, which is a stronger guarantee.","date":"2013-08-29T01:37:43.502Z","type":"message"}
{"nick":"rescrv","message":"Of course, it'd be trivial to enable it.","date":"2013-08-29T01:38:07.512Z","type":"message"}
{"nick":"brycebaril","message":"interesting, is there a paper about f-fault-tolerance I can check out?","date":"2013-08-29T01:38:27.110Z","type":"message"}
{"nick":"rescrv","message":"We're in talks with some people researching nvram (and who have contacts with big vendors, who are willing to provide early prototypes) and will be leveraging those contacts to push the bleeding edge further.","date":"2013-08-29T01:39:14.438Z","type":"message"}
{"nick":"thlorenz_","reason":"Ping timeout: 248 seconds","date":"2013-08-29T01:39:17.681Z","type":"quit"}
{"nick":"rescrv","message":"brycebaril: there's no one paper about \"f-fault-tolerance,\" primarily because it's a convention used in almost all fault tolerance papers.  You can think of it as the answer to the question, \"When can I remain available?\"","date":"2013-08-29T01:41:03.913Z","type":"message"}
{"nick":"alanhoff","date":"2013-08-29T01:41:21.254Z","type":"quit"}
{"nick":"rescrv","message":"If you want to remain available despite $f$ faults, you'll need to have N nodes where N is some function of $f$.","date":"2013-08-29T01:41:38.130Z","type":"message"}
{"nick":"alanhoff","date":"2013-08-29T01:41:53.757Z","type":"join"}
{"nick":"brycebaril","message":"Ahh, ok.","date":"2013-08-29T01:41:58.281Z","type":"message"}
{"nick":"rescrv","message":"For example, Paxos, Raft, ZooKeeper, all are able to remain available in the presence of $f$ failures if you have $2f + 1$ servers.","date":"2013-08-29T01:42:12.705Z","type":"message"}
{"nick":"rescrv","message":"HyperDex's coordinator is the same way","date":"2013-08-29T01:42:18.350Z","type":"message"}
{"nick":"brycebaril","message":"So I guess my next question still applies -- does it acknowledge writes prior to ensuring the adequate number of copies?","date":"2013-08-29T01:43:05.758Z","type":"message"}
{"nick":"rescrv","message":"For HyperDex itself, we replicate within partitions.  To safeguard data, you need $f + 1$ nodes.  Any $f$ can fail, and there'll still be one node remaining with your data, and that node is available.","date":"2013-08-29T01:43:09.063Z","type":"message"}
{"nick":"rescrv","message":"brycebaril: It guarantees that the data has made it to all $f + 1$ nodes (except in the active failure case, where up to $f$ of the nodes are failed).","date":"2013-08-29T01:43:46.308Z","type":"message"}
{"nick":"rescrv","message":"so no, it does not acknowledge before it is safe to do so","date":"2013-08-29T01:43:58.582Z","type":"message"}
{"nick":"rescrv","message":"that's one of the reasons why we're able to guarantee strong consistency","date":"2013-08-29T01:44:38.390Z","type":"message"}
{"nick":"brycebaril","message":"Gotcha, so for the case of this leveldb library that's more or less analagous to fsync before reply","date":"2013-08-29T01:44:42.192Z","type":"message"}
{"nick":"rescrv","message":"it depends upon your failure assumptions.","date":"2013-08-29T01:45:02.476Z","type":"message"}
{"nick":"rescrv","message":"If you assume that spontaneous reboots are the norm, and not process crashes, then you'll want to fsync before reply.","date":"2013-08-29T01:45:24.642Z","type":"message"}
{"nick":"rescrv","message":"If, instead, it's likely that your app is what will crash, and your machines are stable, you'll can assume that the machine will act safely.","date":"2013-08-29T01:45:57.950Z","type":"message"}
{"nick":"rescrv","message":"Personally, I'd take neither option","date":"2013-08-29T01:46:02.741Z","type":"message"}
{"nick":"brycebaril","message":"Of course, bullet to the drive won't matter if fsync or not :P","date":"2013-08-29T01:46:05.397Z","type":"message"}
{"nick":"Raynos","message":"rvagg: I dont really understand what happens when fsync fails and you dont have sync set to true","date":"2013-08-29T01:46:30.090Z","type":"message"}
{"nick":"rescrv","message":"You either have to rely on the entire stack to get fsync correct (LevelDB has been getting better at this), or it all falls apart.","date":"2013-08-29T01:46:50.443Z","type":"message"}
{"nick":"brycebaril","message":"Raynos: I think the state of your app will be incorrect -- it will assume writes to have succeeded that won't, and then subsequent operations will error.","date":"2013-08-29T01:47:07.477Z","type":"message"}
{"nick":"jxson","date":"2013-08-29T01:47:13.593Z","type":"join"}
{"nick":"rescrv","message":"I don't have the citation handy, but there's a paper where automatic analysis of several Linux filesystems found serious bugs that could lose data on common fsync paths.","date":"2013-08-29T01:47:22.226Z","type":"message"}
{"nick":"Raynos","message":"rescrv: what is f-fault-tolerance ?","date":"2013-08-29T01:47:29.677Z","type":"message"}
{"nick":"rescrv","message":"Raynos: if sync is not set to true, then the only way fsync will be called is from the background threads","date":"2013-08-29T01:47:55.590Z","type":"message"}
{"nick":"rescrv","message":"ls","date":"2013-08-29T01:47:58.502Z","type":"message"}
{"nick":"Raynos","message":"brycebaril: by subsequent operations will error you mean that level db instance will error on all future mutation and reading operations because its in a corrupted state ?","date":"2013-08-29T01:48:05.459Z","type":"message"}
{"nick":"rescrv","message":"Raynos: the error should be returned exactly once.  Where the error is returned, you should be able to retry the op that returns the failed error.","date":"2013-08-29T01:48:49.084Z","type":"message"}
{"nick":"brycebaril","message":"That instance, no idea about reads, I have about 5 minutes, let me see if I can write a quick test","date":"2013-08-29T01:48:49.303Z","type":"message"}
{"nick":"rescrv","message":"Raynos: I can look in the code and tell you if you gimme 5","date":"2013-08-29T01:49:08.769Z","type":"message"}
{"nick":"chapel","date":"2013-08-29T01:50:26.551Z","type":"join"}
{"nick":"Raynos","message":"rescrv: so if write to key A with value A has sync: false and it has an fsync failure then some future operation to key B with value B sync: false will fail with an error ?","date":"2013-08-29T01:50:39.471Z","type":"message"}
{"nick":"Raynos","message":"I genuinely have no idea how this works or how disk tolerance and fault tolerance works in general","date":"2013-08-29T01:51:16.961Z","type":"message"}
{"nick":"jxson","reason":"Ping timeout: 245 seconds","date":"2013-08-29T01:51:41.587Z","type":"quit"}
{"nick":"rescrv","message":"Raynos: speaking strictly from a LevelDB standpoint (I don't touch the higher level sugar), the write to key(A) will either succeed, or fail.  If it succeeds, then the fsync was successful.  If anything, including the fsync fails, then the write to A is considered failed as well.","date":"2013-08-29T01:52:18.066Z","type":"message"}
{"nick":"rescrv","message":"the write to B will proceed independently from the stand point of fsyncs.  A's fsync status will not affect B's status unless the system has a bigger issue that affects all writes.","date":"2013-08-29T01:53:04.870Z","type":"message"}
{"nick":"rescrv","message":"it's worth noting that LevelDB is going to perform poorly when fsync'ing every write.  Even if you fsync only every thousandth write, that write will hold up every non-synchronous write that comes after until the fsync completes.","date":"2013-08-29T01:54:04.642Z","type":"message"}
{"nick":"esundahl_","date":"2013-08-29T01:54:34.391Z","type":"join"}
{"nick":"rescrv","message":"We've got a partial patch to HyperLevelDB that significantly improves fsync/non-fsync workloads, but I never finished it because we have no use case for it","date":"2013-08-29T01:54:35.878Z","type":"message"}
{"nick":"Raynos","message":"rescrv: so levelDB itself always fsyncs ?","date":"2013-08-29T01:55:25.202Z","type":"message"}
{"nick":"rescrv","message":"Raynos: there are times where LevelDB will fsync for internal structures, but it won't fsync after each write unless you request it to do so.","date":"2013-08-29T01:56:31.845Z","type":"message"}
{"nick":"Raynos","message":"rescrv: so if if I write to A without asking it to fsync how do I know whether its in disk ?","date":"2013-08-29T01:57:08.866Z","type":"message"}
{"nick":"rescrv","message":"I maybe jumped the gun, but you were proposing a scenario with mixed sync/non-sync writes, and I was simply pointing out that although such a scenario will have an increased performance boost, it will still be almost as slow as a sync workload.","date":"2013-08-29T01:57:43.482Z","type":"message"}
{"nick":"rescrv","message":"Raynos: you won't","date":"2013-08-29T01:57:46.574Z","type":"message"}
{"nick":"Raynos","message":"So, in my head I think of fsync on means \"delay the callback until we have an fsync result\" and fsync off means \"call the callback immediately, we get the fsync result later and either discard or we give it to some global db error handler\"","date":"2013-08-29T01:58:00.925Z","type":"message"}
{"nick":"rescrv","message":"rvagg or others would need to clarify what the JS wrapper is doing, but I imagine that the sync option is passed through to LevelDB and the callback is called only once the LevelDB call completes.","date":"2013-08-29T01:59:35.317Z","type":"message"}
{"nick":"rescrv","message":"If that is how the JS wrapper works, then \"sync=true\" -> \"call the callback only after LevelDB calls 'write'/'fsync'\" and \"sync=false\" -> \"call the callback only after LevelDB calls 'write'\"","date":"2013-08-29T02:00:36.945Z","type":"message"}
{"nick":"rescrv","message":"note that in both cases, LevelDB has passed the data all the way to the OS.","date":"2013-08-29T02:01:33.295Z","type":"message"}
{"nick":"Raynos","message":"rescrv: so if sync is false I have no way of knowing whether a thing is in disk and `db.put(\"a\", \"a\", function (err) { if (err) throw err; db.get(\"a\", function (err, v) { if (err) throw err; assert(v === null) }) })` can happen","date":"2013-08-29T02:01:46.249Z","type":"message"}
{"nick":"eugeneware","reason":"Remote host closed the connection","date":"2013-08-29T02:02:30.462Z","type":"quit"}
{"nick":"brycebaril","message":"Raynos: it may be worse than that... my test is... interesting","date":"2013-08-29T02:02:57.281Z","type":"message"}
{"nick":"rescrv","message":"Raynos: this is all above the level where I work.  I'm strictly a LevelDB and below kind of guy.  You'll have to talk to the others to see if they are wrapping the API in a non-standard way.","date":"2013-08-29T02:03:41.761Z","type":"message"}
{"nick":"Raynos","message":"what im asking is can you PUT a key without sync=true and GET it and level returns whatever its not found or key does not exist error is ?","date":"2013-08-29T02:04:29.568Z","type":"message"}
{"nick":"brycebaril","message":"brycebaril: my test has left me with only questions... I'll make a gist later and see what people say","date":"2013-08-29T02:04:44.935Z","type":"message"}
{"nick":"brycebaril","message":"Essentially it is just writing/reading every few ms and I'm deleting the db out from under it/etc. And it just keeps trucking no matter what (sync or not)","date":"2013-08-29T02:05:11.523Z","type":"message"}
{"nick":"brycebaril","message":"but for now, I have to go :(","date":"2013-08-29T02:05:51.706Z","type":"message"}
{"nick":"rescrv","message":"Raynos: LevelDB will return the stored object even if sync=false.  Doing (pseudo-C) 'put(\"key-a\", \"val-a\", sync=false); get(\"key-a\");' should always return \"val-a\".","date":"2013-08-29T02:06:07.175Z","type":"message"}
{"nick":"rescrv","message":"brycebaril: that's not surprising","date":"2013-08-29T02:06:21.824Z","type":"message"}
{"nick":"Raynos","message":"rescrv: leveldb does this because it reads it from cache ?","date":"2013-08-29T02:06:23.745Z","type":"message"}
{"nick":"rescrv","message":"it opens the log file","date":"2013-08-29T02:06:24.393Z","type":"message"}
{"nick":"jmartins","date":"2013-08-29T02:06:37.074Z","type":"join"}
{"nick":"rescrv","message":"you can unlink this file, but the file still exists until it is closed","date":"2013-08-29T02:06:40.137Z","type":"message"}
{"nick":"rescrv","message":"so no matter what you can sync to the file","date":"2013-08-29T02:06:55.075Z","type":"message"}
{"nick":"Raynos","message":"if you were to close and restart the process do a put before the process restart and a get afterwards will it still be always consistent ?","date":"2013-08-29T02:06:56.493Z","type":"message"}
{"nick":"rescrv","message":"the FS still has a reference","date":"2013-08-29T02:07:00.333Z","type":"message"}
{"nick":"rescrv","message":"Raynos: leveldb does this because the written value is in the log (so it'll be there on a non-failure-case restart), and it's in the memtable (so the current process sees it).","date":"2013-08-29T02:07:43.500Z","type":"message"}
{"nick":"rescrv","message":"Raynos: yes, LevelDB will allow you to close/open like that and it'll still be there (assuming no failures)","date":"2013-08-29T02:08:06.471Z","type":"message"}
{"nick":"Raynos","message":"what happens if the disk failed ? it will just not be there","date":"2013-08-29T02:08:29.167Z","type":"message"}
{"nick":"rescrv","message":"if the disk failed, nothing will be there.  It's dead","date":"2013-08-29T02:08:41.295Z","type":"message"}
{"nick":"Raynos","message":"I'm asking about the case where sync=false but if sync=true the operation would have failed","date":"2013-08-29T02:08:45.802Z","type":"message"}
{"nick":"rescrv","message":"in that case, the close will fail","date":"2013-08-29T02:09:00.118Z","type":"message"}
{"nick":"Raynos","message":"by nothing will be there you mean that key wont be or the entire disk / db is corrupted ?","date":"2013-08-29T02:09:05.567Z","type":"message"}
{"nick":"rescrv","message":"s/will/should/","date":"2013-08-29T02:09:08.641Z","type":"message"}
{"nick":"Raynos","message":"so if a leveldb is an inconsistent state and you were not notified because you didnt wait for fsync you will be notified on close() ?","date":"2013-08-29T02:09:36.130Z","type":"message"}
{"nick":"rescrv","message":"if your disk has failed, then it has failed and doesn't work anymore","date":"2013-08-29T02:09:36.346Z","type":"message"}
{"nick":"rescrv","message":"Raynos: I can confirm that in the code, but the design they've used and the care they've taken elsewhere lead me to believe that they'll do that, yes.","date":"2013-08-29T02:10:12.114Z","type":"message"}
{"nick":"rescrv","message":"I can verify in the code.","date":"2013-08-29T02:10:20.229Z","type":"message"}
{"nick":"Raynos","message":"Ok cool","date":"2013-08-29T02:10:30.325Z","type":"message"}
{"nick":"rescrv","message":"Raynos: For what it's worth, we, the HyperDex team, offer commercial support for LevelDB and HyperLevelDB, even when not used in HyperDex.  If you're just a hobbiest that's cool.  If you need someone to call in SHTF scenarios, we offer support and consulting for that.","date":"2013-08-29T02:11:32.233Z","type":"message"}
{"nick":"Raynos","message":"rescrv: cool :)","date":"2013-08-29T02:12:35.344Z","type":"message"}
{"nick":"julianduque","reason":"Ping timeout: 260 seconds","date":"2013-08-29T02:12:55.278Z","type":"quit"}
{"nick":"rvagg","message":"sorry, had to disappear for a while-- Raynos, rescrv, you get the callback from a .put() or .batch() after it gets handed back control from leveldb, which in turn returns when it gets handed back control from the OS, that last bit is what 'sync'=true/false determines so it's all in the hands of the OS - but that doesn't mean you will never get an I/O error if you sync=false (default), there are other kinds of things that can go","date":"2013-08-29T02:13:10.327Z","type":"message"}
{"nick":"rvagg","message":"wrong","date":"2013-08-29T02:13:10.650Z","type":"message"}
{"nick":"rescrv","message":"Raynos: \"Close\" will always succeed.  You'll need to do a final write to return any residual errors.","date":"2013-08-29T02:13:51.360Z","type":"message"}
{"nick":"rvagg","message":"rescrv: we're not quite set up properly yet but I'm going to try and make sure that hyperleveldb is always available as an alternative to the default google leveldb in Node and I'll probably be promoting it as the ideal choice for perf in Node; there's a chance we may even switch to it as the default but that discussion is a bit of a way off yet","date":"2013-08-29T02:14:13.562Z","type":"message"}
{"nick":"rescrv","message":"rvagg: sounds great!  We're trying to keep as close to upstream while adding the features we need (see LiveBackup).","date":"2013-08-29T02:14:50.278Z","type":"message"}
{"nick":"rescrv","message":"We've even pushed bug fixes upstream.","date":"2013-08-29T02:14:57.105Z","type":"message"}
{"nick":"rvagg","message":"yerp and you've been much more responsive in dealing with reported bugs","date":"2013-08-29T02:15:08.524Z","type":"message"}
{"nick":"rvagg","message":"the google guys are a little blinkered, focused on chromium as the primary target and they obviously have a whole lot of other things on their place","date":"2013-08-29T02:15:27.870Z","type":"message"}
{"nick":"rvagg","message":"s/place/plate","date":"2013-08-29T02:15:30.449Z","type":"message"}
{"nick":"rescrv","message":"If there's anything I can do to make it easy for you guys to keep HyperLevelDB as an option, please let me know before we become enough of a burden that you drop us ;-)","date":"2013-08-29T02:15:36.985Z","type":"message"}
{"nick":"rvagg","message":"heh, no, it's actually pretty easy, much easier than the basho port which is a bit of a mess tbh","date":"2013-08-29T02:16:08.853Z","type":"message"}
{"nick":"rvagg","message":"the only thing I need to do is stay on top of your versioning","date":"2013-08-29T02:16:17.228Z","type":"message"}
{"nick":"rescrv","message":"rvagg: I honestly don't think Sanjay and Jeff care about Chromium much.  If they were solely focused on that, LevelDB would be much worse off than it is.","date":"2013-08-29T02:16:26.364Z","type":"message"}
{"nick":"rescrv","message":"rvagg: We've not cut a release.  We'll do that with the next HyperDex release.  I don't know how the numbers will go.","date":"2013-08-29T02:16:47.650Z","type":"message"}
{"nick":"rvagg","message":"ok, well I'd better sync with you some time about how to obtain correct releases and make sure I'm not just grabbing an unstable branch","date":"2013-08-29T02:17:13.073Z","type":"message"}
{"nick":"rescrv","message":"Sanjay and Jeff are cool guys though.  I was at lunch with Jeff and someone asked him about Google's self-driving Prius, and why Google would invest in it.  His answer was, \"Why not?\" with a big-ass grin.","date":"2013-08-29T02:17:27.449Z","type":"message"}
{"nick":"rvagg","message":"it's easy with leveldb cause I can subscribe to the google code wiki feed for it","date":"2013-08-29T02:17:31.545Z","type":"message"}
{"nick":"rescrv","message":"rvagg: anything unstable/experimental doesn't get pushed until it's better than what it replaces or is under a \"dev/\" prefix.","date":"2013-08-29T02:18:34.250Z","type":"message"}
{"nick":"rvagg","message":"k","date":"2013-08-29T02:18:45.314Z","type":"message"}
{"nick":"rvagg","message":"I'm still on leveldb-1.11.0, need to get around to pushing out a 1.13.0 version","date":"2013-08-29T02:19:01.056Z","type":"message"}
{"nick":"mbalho","message":"rescrv: how did you implement livebackup?","date":"2013-08-29T02:19:43.482Z","type":"message"}
{"nick":"rescrv","message":"Raynos: your final \"write\" need not be sync=true.  And making it sync=true will not allow you to assume that other, previous operations completed.","date":"2013-08-29T02:19:52.261Z","type":"message"}
{"nick":"rescrv","message":"mbalho: you mean how does it work?","date":"2013-08-29T02:20:01.075Z","type":"message"}
{"nick":"mbalho","message":"rescrv: yes. i was working on a similar problem this week","date":"2013-08-29T02:20:30.124Z","type":"message"}
{"nick":"rescrv","message":"There's a LiveBackup(const Slice& name) call.  It'll create the directory \"backup-<name>\" under the DB directory.  Then it'll copy/link every LevelDB-related file in a manner that's consistent.  What you end up with is a directory that is itself a LevelDB instance.","date":"2013-08-29T02:22:09.941Z","type":"message"}
{"nick":"rescrv","message":"It's great for making incremental backups, because it copies at most 2*sizeof(write_buffer) + sizeof(metadata), and the rest is hard-links","date":"2013-08-29T02:22:42.471Z","type":"message"}
{"nick":"rvagg","message":"you build on Env for that don't you?","date":"2013-08-29T02:23:05.062Z","type":"message"}
{"nick":"rescrv","message":"combine it with rsync, and you have an efficient way to backup solely that data that's changed since the last backup","date":"2013-08-29T02:23:20.093Z","type":"message"}
{"nick":"rescrv","message":"rvagg: lemme check, but I think I was as general as I could be","date":"2013-08-29T02:23:29.505Z","type":"message"}
{"nick":"rvagg","message":"we have a custom Env for Windows, does hyperleveldb support windows at all?","date":"2013-08-29T02:23:50.389Z","type":"message"}
{"nick":"rescrv","message":"rvagg:  it uses env https://github.com/rescrv/HyperLevelDB/commit/486ca7f6e81c00796a5c24396039fd1a108b582f","date":"2013-08-29T02:24:30.581Z","type":"message"}
{"nick":"rescrv","message":"I see now that I used some GCC intrinsics. I should fix those sometime","date":"2013-08-29T02:24:55.069Z","type":"message"}
{"nick":"rescrv","message":"rvagg: we don't actively avoid Windows, but it's not a supported target, and will likely not become an active focus for us unless there's external financial backing.","date":"2013-08-29T02:25:49.222Z","type":"message"}
{"nick":"rvagg","message":"ahhh, you've built the locking into db_impl.cc, that's nicer than the suggestions flying around on the leveldb mailing list, so it should be relatively easy for us to expose that too","date":"2013-08-29T02:26:23.889Z","type":"message"}
{"nick":"rescrv","message":"enough to hire someone else to deal with it, because I don't touch Windows","date":"2013-08-29T02:26:24.793Z","type":"message"}
{"nick":"rvagg","message":"tho it looks like we'll have to make sure our Env has CopyFile() and LinkFile()","date":"2013-08-29T02:26:42.319Z","type":"message"}
{"nick":"mbalho","message":"rescrv: my backup solution was actually for initial bulk cloning (im working on a sort of distributed system). implementation is here https://github.com/maxogden/dat/blob/master/lib/commands.js#L65 and https://github.com/maxogden/dat/blob/master/lib/commands.js#L114-L120. haven't tackled consistency yet, was gonna start with a global write lock during cloning","date":"2013-08-29T02:26:50.627Z","type":"message"}
{"nick":"rvagg","message":"yeah, we had to add Windows because a surprising number of people are using Windows + Node","date":"2013-08-29T02:26:59.265Z","type":"message"}
{"nick":"rescrv","message":"rvagg: Yep.  We are restructuring all of the locking to make sense from a performance perspective","date":"2013-08-29T02:27:08.083Z","type":"message"}
{"nick":"rvagg","message":"https://github.com/rvagg/node-leveldown/tree/master/deps/leveldb/port-libuv actually wasn't too difficult in the end to do windows, uses libuv (native with Node) to do all the thread and locking stuff in port.h, then uses a borrowed env.cc from a guy that did a windows port himself","date":"2013-08-29T02:28:15.837Z","type":"message"}
{"nick":"rescrv","message":"mbalho: you'll need to completely block both writers and compaction.  It'll be expensive, but should be correct.","date":"2013-08-29T02:28:20.728Z","type":"message"}
{"nick":"rvagg","message":"apart from that there's only a single line in the main port.h that we have to change to load our windows stuff when compiling on windows, otherwise leveldb is left alone","date":"2013-08-29T02:28:38.266Z","type":"message"}
{"nick":"mbalho","message":"rvagg: is there any way to temporarily disable compaction from JS-land?","date":"2013-08-29T02:29:13.782Z","type":"message"}
{"nick":"rescrv","message":"rvagg: the only thing we've done that could hurt Windows compilation then is the additional copy/link (does Windows even know what a hardlink is?) calls in the env, and reliance upon mmap.","date":"2013-08-29T02:29:20.525Z","type":"message"}
{"nick":"rvagg","message":"mbalho: nope","date":"2013-08-29T02:29:22.346Z","type":"message"}
{"nick":"rvagg","message":"mbalho: the best you can do is close()/backup/open()","date":"2013-08-29T02:29:34.880Z","type":"message"}
{"nick":"rescrv","message":"mbalho: there's no way to temporarily disable from C++ land either","date":"2013-08-29T02:29:42.095Z","type":"message"}
{"nick":"mbalho","message":"ah too bad","date":"2013-08-29T02:29:52.121Z","type":"message"}
{"nick":"rvagg","message":"but if you wanted to then you could do a custom leveldown layer that does that for you and just buffers incoming reads & writes, it wouldn't be hard but the buffering could get out of control","date":"2013-08-29T02:29:58.902Z","type":"message"}
{"nick":"mbalho","message":"is there any way to monitor compaction state?","date":"2013-08-29T02:30:18.051Z","type":"message"}
{"nick":"rescrv","message":"mbalho: override the log and parse log messages?","date":"2013-08-29T02:30:30.493Z","type":"message"}
{"nick":"rvagg","message":"rescrv: well, tbh, anyone that's seriously using it in node is deploying on linux or smartos/solaris, mostly developing in osx I think","date":"2013-08-29T02:30:48.286Z","type":"message"}
{"nick":"rvagg","message":"there's a few people playing with node+leveldb+azure, but even then you can just use linux on azure","date":"2013-08-29T02:31:04.569Z","type":"message"}
{"nick":"rescrv","message":"the HyperLevelDB code is BSD-licensed.  You may be able to use our live-backup code to deal with that","date":"2013-08-29T02:31:20.201Z","type":"message"}
{"nick":"mbalho","message":"rescrv: oh great to hear, i'll look into it","date":"2013-08-29T02:31:37.847Z","type":"message"}
{"nick":"rvagg","message":"mbalho: hang on, dominic and I played with exposing the logging, let me see where that's at","date":"2013-08-29T02:31:40.889Z","type":"message"}
{"nick":"mbalho","message":"rvagg: also you mentioned somewhere sometime about making hyperleveldb + basho leveldb easier to swap out in leveldown, did that ever happen?","date":"2013-08-29T02:32:36.130Z","type":"message"}
{"nick":"rvagg","message":"mbalho: https://github.com/rvagg/node-leveldown/compare/logger-play","date":"2013-08-29T02:32:48.191Z","type":"message"}
{"nick":"niftylettuce","reason":"Quit: Updating details, brb","date":"2013-08-29T02:33:26.197Z","type":"quit"}
{"nick":"rvagg","message":"mbalho: it's available but a bit behind latest, it's just a matter of me dealing with the leveldown branches I have, syncing with their upstream repos for leveldb, compiling & testing & publishing","date":"2013-08-29T02:33:32.975Z","type":"message"}
{"nick":"mbalho","message":"rvagg: ah nice. do you think it would impact performance to expose the log as a stream in JS?","date":"2013-08-29T02:33:34.195Z","type":"message"}
{"nick":"niftylettuce","date":"2013-08-29T02:33:41.566Z","type":"join"}
{"nick":"rvagg","message":"mbalho: I honestly don't know, probably a little cause of lots of string copying that needs to go on, it'd probably have to be something you can turn off and on while you're using the db","date":"2013-08-29T02:34:34.421Z","type":"message"}
{"nick":"mbalho","message":"ahh yea makes sense","date":"2013-08-29T02:34:49.650Z","type":"message"}
{"nick":"thlorenz_","date":"2013-08-29T02:35:16.534Z","type":"join"}
{"nick":"rescrv","message":"mbalho: I'd recommend strongly against the logging-based approach.  You'll have a TTCTTU race condition that'll make your backups prone to random corruption upon restore","date":"2013-08-29T02:35:41.699Z","type":"message"}
{"nick":"rvagg","message":"mbalho: there's also this, don't know if you've looked at it at all: https://github.com/rvagg/node-leveldown#leveldown_getProperty","date":"2013-08-29T02:36:32.173Z","type":"message"}
{"nick":"mbalho","message":"oh nice","date":"2013-08-29T02:36:52.940Z","type":"message"}
{"nick":"alanhoff","reason":"Ping timeout: 264 seconds","date":"2013-08-29T02:37:04.450Z","type":"quit"}
{"nick":"rvagg","message":"it's off leveldown so, db.db.getProperty('foo')","date":"2013-08-29T02:37:10.554Z","type":"message"}
{"nick":"mbalho","message":"i have some crazy ideas like open a second leveldb and direct all writes to that during the file copy and then after the file copy is done then network replicate the buffered writes into the main db again.","date":"2013-08-29T02:38:26.465Z","type":"message"}
{"nick":"rescrv","message":"mbalho: is there a reason for copy vs hardlink?","date":"2013-08-29T02:39:02.307Z","type":"message"}
{"nick":"mbalho","message":"rescrv: its a client server relationship, e.g. git clone","date":"2013-08-29T02:39:30.332Z","type":"message"}
{"nick":"mbalho","message":"or multi-master i guess","date":"2013-08-29T02:39:49.258Z","type":"message"}
{"nick":"thlorenz_","reason":"Ping timeout: 264 seconds","date":"2013-08-29T02:40:07.004Z","type":"quit"}
{"nick":"rescrv","message":"mbalho: I'm just trying to understand why you'd have two LevelDB instances if you're just going to use the second to load the first.","date":"2013-08-29T02:40:46.678Z","type":"message"}
{"nick":"mbalho","message":"the secondary one is just for availability during the write lock while the client clones the files of the first one","date":"2013-08-29T02:41:11.560Z","type":"message"}
{"nick":"mbalho","message":"because copying the leveldb files is a lot faster than iterating over them and serializing them","date":"2013-08-29T02:41:56.255Z","type":"message"}
{"nick":"rescrv","message":"mbalho: so your overall goal is to create a snapshot of the files on the filesystem which may then be sent directly to the client.  And, you want to do it without blocking future writes.","date":"2013-08-29T02:42:28.723Z","type":"message"}
{"nick":"rvagg","message":"yar and you could transparently replace the storage engine at runtime without impacting on the exposed api... but it would get pretty messy","date":"2013-08-29T02:42:33.287Z","type":"message"}
{"nick":"rvagg","message":"rescrv: do you have a page that talks about your hyperdex/leveldb support options? this might be good to mention to people wanting to deploy serious stuff with node","date":"2013-08-29T02:43:20.658Z","type":"message"}
{"nick":"mbalho","message":"rescrv: right. the project im working on is for scientists to publish large datasets in a way that is syncable","date":"2013-08-29T02:43:24.149Z","type":"message"}
{"nick":"rvagg","message":"https://github.com/maxogden/dat","date":"2013-08-29T02:44:02.537Z","type":"message"}
{"nick":"mbalho","message":"rescrv: so if someone is downloading the data for the first time it would be nice if i could optimize that, but also would be nice to allow the source data to get updated while the clones are happening","date":"2013-08-29T02:44:04.931Z","type":"message"}
{"nick":"rescrv","message":"mbalho: it sounds like HyperLevelDB's live-backup is exactly what you need.  I can backup a 10GB DB with a constant stream of new writes on the order of milliseconds.","date":"2013-08-29T02:44:53.655Z","type":"message"}
{"nick":"rescrv","message":"downloading the data would then just be rsync","date":"2013-08-29T02:45:09.108Z","type":"message"}
{"nick":"rescrv","message":"it'll automatically optimize","date":"2013-08-29T02:45:13.810Z","type":"message"}
{"nick":"mbalho","message":"i'm more keen on node gzip streams as they are more portable than rsync","date":"2013-08-29T02:45:32.824Z","type":"message"}
{"nick":"mbalho","message":"but yea, taht sounds great","date":"2013-08-29T02:45:36.055Z","type":"message"}
{"nick":"rvagg","message":"you should include No9 in your discussions about this mbalho, if you make an issue on github anywhere CC him","date":"2013-08-29T02:46:47.478Z","type":"message"}
{"nick":"rvagg","message":"he ended up just using zfs snapshots for backups","date":"2013-08-29T02:46:55.622Z","type":"message"}
{"nick":"rvagg","message":"which isn't exactly a portable solution!","date":"2013-08-29T02:47:03.704Z","type":"message"}
{"nick":"rescrv","message":"rvagg: it's also not likely correct unless you stop compaction and grab the internal lock.","date":"2013-08-29T02:47:26.886Z","type":"message"}
{"nick":"rescrv","message":"or do something equivalent","date":"2013-08-29T02:47:32.230Z","type":"message"}
{"nick":"rvagg","message":"yeah, but it's close-enough to serve as a just-in-case","date":"2013-08-29T02:47:47.463Z","type":"message"}
{"nick":"rescrv","message":"yeah.  especially if you're willing to fix it up manuallly","date":"2013-08-29T02:49:07.095Z","type":"message"}
{"nick":"mbalho","message":"does the on disk format change between leveldb implementations?","date":"2013-08-29T02:49:16.741Z","type":"message"}
{"nick":"rvagg","message":"no","date":"2013-08-29T02:49:33.563Z","type":"message"}
{"nick":"mbalho","message":"nice","date":"2013-08-29T02:49:41.812Z","type":"message"}
{"nick":"rescrv","message":"mbalho: HyperLevelDB is binary compatible and we will keep it that way well into the future","date":"2013-08-29T02:50:00.550Z","type":"message"}
{"nick":"rescrv","message":"err... the files are binary compatible.  It's not ABI compatible","date":"2013-08-29T02:50:24.671Z","type":"message"}
{"nick":"mbalho","message":"for windows users (i expect lots of scientists + governments etc to be on windows using dat) as long as streaming the leveldb files from unix users works when they open them with google leveldb then i'm all good","date":"2013-08-29T02:54:07.700Z","type":"message"}
{"nick":"mbalho","message":"and by unix users i mean hyperleveldb users","date":"2013-08-29T02:54:18.417Z","type":"message"}
{"nick":"mbalho","message":"i can just disable live backup if youre a windows server","date":"2013-08-29T02:54:31.508Z","type":"message"}
{"nick":"rvagg","message":"if I had time I'd tinker with getting hyperleveldb working for us in windows but I don't and I just don't care enough about windows!","date":"2013-08-29T02:55:03.714Z","type":"message"}
{"nick":"mbalho","message":"yea i dont think many people will care","date":"2013-08-29T02:55:24.111Z","type":"message"}
{"nick":"mbalho","message":"i just got a new big empty external hard drive, and github has a 1GB internet connection","date":"2013-08-29T02:56:27.381Z","type":"message"}
{"nick":"mbalho","message":"gonna go there tomorrow and download some huge datasets to play with for dat :D","date":"2013-08-29T02:56:37.967Z","type":"message"}
{"nick":"rescrv","message":"mbalho: you work for GitHub?","date":"2013-08-29T02:56:48.815Z","type":"message"}
{"nick":"mbalho","message":"rescrv: my girlfriend does","date":"2013-08-29T02:56:58.872Z","type":"message"}
{"nick":"mbalho","message":"rescrv: im grant funded, working on dat full time now","date":"2013-08-29T02:57:09.672Z","type":"message"}
{"nick":"rescrv","message":"neat!  I've chatted with someone who had an \"in\" there.","date":"2013-08-29T02:57:44.762Z","type":"message"}
{"nick":"mbalho","message":"if you need anything let me know. mostly i can offer api rate limit increases :D","date":"2013-08-29T02:59:10.885Z","type":"message"}
{"nick":"mbalho","message":"my secret plan is to get them to use dat down the road","date":"2013-08-29T02:59:35.155Z","type":"message"}
{"nick":"rescrv","message":"rvagg: we've not yet put together a page with our support options.  I'm just one guy though, and code comes first.","date":"2013-08-29T02:59:40.207Z","type":"message"}
{"nick":"rescrv","message":"mbalho: I keep a list in the back of my mind of who has connections where, mainly because I'm a grad student/hacker and one of the reqs of the job is going around promoting my research","date":"2013-08-29T03:00:30.170Z","type":"message"}
{"nick":"rescrv","message":"my research just happens to be much more implementation heavy than most","date":"2013-08-29T03:00:43.553Z","type":"message"}
{"nick":"rescrv","message":"which is why we're moving toward the commercial support for both HyperDex and LevelDB","date":"2013-08-29T03:01:12.014Z","type":"message"}
{"nick":"rescrv","message":"it'd be cool to talk with people at GitHub about HyperDex.  I suspect they may be interested in some of the upcoming projects we're building around it.","date":"2013-08-29T03:02:42.567Z","type":"message"}
{"nick":"mbalho","message":"they have some smart folks working on git and their data center ops + storage stuff","date":"2013-08-29T03:03:43.585Z","type":"message"}
{"nick":"mbalho","message":"alot of the programmers there do rails full time. but there is a contingent of low level hackers too","date":"2013-08-29T03:04:37.987Z","type":"message"}
{"nick":"rescrv","message":"I had a great time chatting with the Dropbox folks who do much of Dropbox's work on HBase/HDFS and it inspired some of our current work.  I'd love to know more about the backend structure, and where their pain points are.","date":"2013-08-29T03:05:01.032Z","type":"message"}
{"nick":"rvagg","message":"DDoS #1 pain point I suspect","date":"2013-08-29T03:05:38.455Z","type":"message"}
{"nick":"rescrv","message":"rvagg: I suspect that's a problem for the ruby guys, not so much the backend","date":"2013-08-29T03:05:58.010Z","type":"message"}
{"nick":"mbalho","message":"rackspace is the #1 pain point, which prevents them from preventing the ddoses","date":"2013-08-29T03:06:13.023Z","type":"message"}
{"nick":"rvagg","message":"although I've always suspected that \"DDoS\" is code for \"out stuff is freaking out and can't handle what's going on!\"","date":"2013-08-29T03:06:14.492Z","type":"message"}
{"nick":"mbalho","message":"so theyre moving to their own hardware","date":"2013-08-29T03:06:21.285Z","type":"message"}
{"nick":"rvagg","message":"or DDoS = Rails can't cope!","date":"2013-08-29T03:06:28.558Z","type":"message"}
{"nick":"rescrv","message":"by the time you've got one person connected to the ruby process, it's ground to enough of a halt that it cannot hurt the backend (can you tell I dislike Ruby)","date":"2013-08-29T03:06:40.287Z","type":"message"}
{"nick":"mbalho","message":"rvagg: nah the ddoses are definitely legimiate","date":"2013-08-29T03:06:43.004Z","type":"message"}
{"nick":"mbalho","message":"legitimate*","date":"2013-08-29T03:06:56.244Z","type":"message"}
{"nick":"mbalho","message":"le*git* hehehe","date":"2013-08-29T03:07:04.228Z","type":"message"}
{"nick":"rvagg","message":"yeah, I'm sure they are legit, it's just such an odd thing to happen to github of all sites","date":"2013-08-29T03:07:10.240Z","type":"message"}
{"nick":"rescrv","message":"can you give insight into why people are ddos'ing GitHub?","date":"2013-08-29T03:07:15.066Z","type":"message"}
{"nick":"mbalho","message":"i dunno the answer to that sadly","date":"2013-08-29T03:07:35.769Z","type":"message"}
{"nick":"rescrv","message":"is it just because it's a convenient target?  are they masking intrusions?  or is it just that people like taking down big sites?","date":"2013-08-29T03:07:37.374Z","type":"message"}
{"nick":"mbalho","message":"i think the last one. ive seen some data about volume + geo ip distribution and its definitely botnets","date":"2013-08-29T03:07:59.126Z","type":"message"}
{"nick":"julianduque","date":"2013-08-29T03:08:35.303Z","type":"join"}
{"nick":"rescrv","message":"those who can, do; those who can't DDOS everyone else's infrastructure?","date":"2013-08-29T03:09:42.758Z","type":"message"}
{"nick":"rvagg","message":"it must be Syria, we should bomb them","date":"2013-08-29T03:10:24.478Z","type":"message"}
{"nick":"mbalho","message":"rvagg: since when are you an american!??","date":"2013-08-29T03:11:08.857Z","type":"message"}
{"nick":"rvagg","message":"oh, sorry, forgot for a moment","date":"2013-08-29T03:11:19.438Z","type":"message"}
{"nick":"rescrv","message":"I'd just like to take a moment to say hello to the NSA agent who's silently joined ##leveldb","date":"2013-08-29T03:12:01.931Z","type":"message"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-08-29T03:18:40.073Z","type":"quit"}
{"nick":"jmartins","reason":"Remote host closed the connection","date":"2013-08-29T03:22:41.563Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T03:35:40.283Z","type":"join"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T03:37:07.016Z","type":"quit"}
{"nick":"ryan_ramage","date":"2013-08-29T03:40:03.650Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 264 seconds","date":"2013-08-29T03:40:07.018Z","type":"quit"}
{"nick":"timoxley","reason":"Ping timeout: 264 seconds","date":"2013-08-29T03:48:31.107Z","type":"quit"}
{"nick":"mbalho","message":"inserting 20 million 10kb rows in 1600 row batches, gonna see how long it takes","date":"2013-08-29T03:50:43.931Z","type":"message"}
{"nick":"chapel","date":"2013-08-29T03:51:52.858Z","type":"join"}
{"nick":"mbalho","message":"wow, after 20 or so batches of 1600 the time per batch went from around 1 second to around 20 seconds","date":"2013-08-29T03:54:51.768Z","type":"message"}
{"nick":"mbalho","message":"oooh seems i had a memory leak","date":"2013-08-29T03:55:57.698Z","type":"message"}
{"nick":"rvagg","message":"ugh, could be that memory leak that we're discussing on levelup/leveldown that we haven't managed to find yet","date":"2013-08-29T03:56:31.098Z","type":"message"}
{"nick":"eugeneware","date":"2013-08-29T03:56:56.742Z","type":"join"}
{"nick":"mbalho","message":"good to know, i have to rule otu my own code first","date":"2013-08-29T03:57:16.322Z","type":"message"}
{"nick":"mbalho","message":"rvagg: in case you wanna reproduce https://github.com/maxogden/dat/blob/master/test/insert-junk-data.js","date":"2013-08-29T03:59:48.894Z","type":"message"}
{"nick":"mbalho","message":"rvagg: (ignore the ugly dat programmatic api)","date":"2013-08-29T04:00:21.871Z","type":"message"}
{"nick":"mbalho","message":"it uses level-mutex which automatically batches","date":"2013-08-29T04:00:51.647Z","type":"message"}
{"nick":"mbalho","message":"this is the output i got https://gist.github.com/maxogden/0ddccdd28263391a2251","date":"2013-08-29T04:02:08.325Z","type":"message"}
{"nick":"rvagg","message":"ouch","date":"2013-08-29T04:02:52.666Z","type":"message"}
{"nick":"rvagg","message":"similar to what others have reported","date":"2013-08-29T04:03:57.465Z","type":"message"}
{"nick":"rvagg","message":"darn it","date":"2013-08-29T04:04:03.169Z","type":"message"}
{"nick":"jxson","date":"2013-08-29T04:04:46.051Z","type":"join"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T04:05:19.027Z","type":"quit"}
{"nick":"jxson","reason":"Remote host closed the connection","date":"2013-08-29T04:15:31.736Z","type":"quit"}
{"nick":"wolfeidau","reason":"Remote host closed the connection","date":"2013-08-29T04:20:30.658Z","type":"quit"}
{"nick":"wolfeidau","date":"2013-08-29T04:20:51.737Z","type":"join"}
{"nick":"timoxley","date":"2013-08-29T04:25:40.646Z","type":"join"}
{"nick":"eugeneware","reason":"Remote host closed the connection","date":"2013-08-29T04:30:14.626Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T04:36:04.602Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 264 seconds","date":"2013-08-29T04:40:40.342Z","type":"quit"}
{"nick":"dguttman","date":"2013-08-29T04:42:29.240Z","type":"join"}
{"nick":"SomeoneWeird","date":"2013-08-29T04:44:11.417Z","type":"join"}
{"nick":"dguttman","reason":"Quit: dguttman","date":"2013-08-29T04:50:41.010Z","type":"quit"}
{"nick":"esundahl_","reason":"Remote host closed the connection","date":"2013-08-29T05:06:23.243Z","type":"quit"}
{"nick":"esundahl","date":"2013-08-29T05:06:56.582Z","type":"join"}
{"nick":"esundahl","reason":"Ping timeout: 264 seconds","date":"2013-08-29T05:11:54.989Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T05:36:28.832Z","type":"join"}
{"nick":"esundahl","date":"2013-08-29T05:37:31.572Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 245 seconds","date":"2013-08-29T05:40:51.619Z","type":"quit"}
{"nick":"tomerd","date":"2013-08-29T05:42:19.520Z","type":"join"}
{"nick":"esundahl","reason":"Ping timeout: 264 seconds","date":"2013-08-29T05:46:04.367Z","type":"quit"}
{"nick":"timoxley","reason":"Ping timeout: 260 seconds","date":"2013-08-29T05:46:11.435Z","type":"quit"}
{"nick":"timoxley","date":"2013-08-29T05:49:48.928Z","type":"join"}
{"nick":"chapel","date":"2013-08-29T05:51:51.128Z","type":"join"}
{"nick":"timoxley","reason":"Read error: Connection reset by peer","date":"2013-08-29T05:53:50.328Z","type":"quit"}
{"nick":"timoxley","date":"2013-08-29T05:54:16.666Z","type":"join"}
{"nick":"tomerd","reason":"Remote host closed the connection","date":"2013-08-29T06:05:55.725Z","type":"quit"}
{"nick":"tomerd","date":"2013-08-29T06:06:30.591Z","type":"join"}
{"nick":"ryan_ramage","reason":"Quit: ryan_ramage","date":"2013-08-29T06:08:49.034Z","type":"quit"}
{"nick":"jcrugzz_","reason":"Ping timeout: 268 seconds","date":"2013-08-29T06:09:04.124Z","type":"quit"}
{"nick":"tomerd","reason":"Ping timeout: 264 seconds","date":"2013-08-29T06:11:16.420Z","type":"quit"}
{"nick":"ryan_ramage","date":"2013-08-29T06:21:49.697Z","type":"join"}
{"nick":"timoxley","reason":"Read error: No route to host","date":"2013-08-29T06:22:00.442Z","type":"quit"}
{"nick":"ryan_ramage","reason":"Client Quit","date":"2013-08-29T06:22:22.014Z","type":"quit"}
{"nick":"timoxley","date":"2013-08-29T06:22:23.376Z","type":"join"}
{"nick":"timoxley","reason":"Read error: Connection reset by peer","date":"2013-08-29T06:23:40.271Z","type":"quit"}
{"nick":"timoxley","date":"2013-08-29T06:24:01.936Z","type":"join"}
{"nick":"thlorenz","date":"2013-08-29T06:36:55.101Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 240 seconds","date":"2013-08-29T06:41:12.473Z","type":"quit"}
{"nick":"esundahl","date":"2013-08-29T06:42:18.479Z","type":"join"}
{"nick":"levelbot","message":"[npm] abstract-leveldown@0.10.1 <http://npm.im/abstract-leveldown>: An abstract prototype matching the LevelDOWN API (@rvagg)","date":"2013-08-29T06:46:19.316Z","type":"message"}
{"nick":"esundahl","reason":"Ping timeout: 264 seconds","date":"2013-08-29T06:46:54.222Z","type":"quit"}
{"nick":"kenansulayman","date":"2013-08-29T06:49:44.339Z","type":"join"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T06:59:54.990Z","type":"quit"}
{"nick":"chapel","date":"2013-08-29T07:02:28.645Z","type":"join"}
{"nick":"dominictarr","date":"2013-08-29T07:04:21.852Z","type":"join"}
{"nick":"jcrugzz","date":"2013-08-29T07:12:56.279Z","type":"join"}
{"nick":"tomerd","date":"2013-08-29T07:21:27.477Z","type":"join"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-08-29T07:25:21.275Z","type":"quit"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T07:33:30.978Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T07:37:20.339Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 268 seconds","date":"2013-08-29T07:42:11.122Z","type":"quit"}
{"nick":"esundahl","date":"2013-08-29T07:42:50.337Z","type":"join"}
{"nick":"tomerd","reason":"Remote host closed the connection","date":"2013-08-29T07:43:03.267Z","type":"quit"}
{"nick":"tomerd","date":"2013-08-29T07:43:38.257Z","type":"join"}
{"nick":"esundahl","reason":"Ping timeout: 268 seconds","date":"2013-08-29T07:47:44.137Z","type":"quit"}
{"nick":"tomerd","reason":"Ping timeout: 264 seconds","date":"2013-08-29T07:48:30.989Z","type":"quit"}
{"nick":"chapel","date":"2013-08-29T07:51:34.015Z","type":"join"}
{"nick":"tomerd","date":"2013-08-29T07:59:25.262Z","type":"join"}
{"nick":"tomerd","reason":"Remote host closed the connection","date":"2013-08-29T07:59:56.026Z","type":"quit"}
{"nick":"julianduque","reason":"Quit: leaving","date":"2013-08-29T08:15:00.058Z","type":"quit"}
{"nick":"dominictarr","reason":"Quit: dominictarr","date":"2013-08-29T08:20:49.196Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T08:37:46.848Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 245 seconds","date":"2013-08-29T08:42:06.746Z","type":"quit"}
{"nick":"dominictarr","date":"2013-08-29T08:47:18.565Z","type":"join"}
{"nick":"jcrugzz","reason":"Ping timeout: 268 seconds","date":"2013-08-29T08:52:29.123Z","type":"quit"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T08:53:55.063Z","type":"quit"}
{"nick":"kenansulayman","message":"substack gj @ archy","date":"2013-08-29T08:56:36.226Z","type":"message"}
{"nick":"kenansulayman","message":"I'd be amazed if it'd be feasible to create flowcharts like that with node","date":"2013-08-29T08:57:11.491Z","type":"message"}
{"nick":"dominictarr","message":"kenansulayman: sure! it's just a graphlayout problem","date":"2013-08-29T09:01:20.190Z","type":"message"}
{"nick":"dominictarr","message":"there are already graph layout modules like dagre","date":"2013-08-29T09:01:38.327Z","type":"message"}
{"nick":"dominictarr","message":"doing so in the terminal would also be possible, just more constrained.","date":"2013-08-29T09:02:09.134Z","type":"message"}
{"nick":"kenansulayman","message":"I'd focus on terminal / CL apps","date":"2013-08-29T09:02:34.234Z","type":"message"}
{"nick":"kenansulayman","message":"We're currently outputting Wolfram Mathematica code for debugging relations","date":"2013-08-29T09:02:51.101Z","type":"message"}
{"nick":"kenansulayman","message":"That'd really be awesome, maybe I find some spartime for it :)","date":"2013-08-29T09:03:26.246Z","type":"message"}
{"nick":"substack","message":"kenansulayman: see also https://github.com/substack/undirender","date":"2013-08-29T09:03:58.374Z","type":"message"}
{"nick":"substack","message":"archy is not actually very interesting, graph-wise","date":"2013-08-29T09:04:08.403Z","type":"message"}
{"nick":"kenansulayman","message":"I know","date":"2013-08-29T09:04:26.955Z","type":"message"}
{"nick":"kenansulayman","message":"But it's inspiring ;)","date":"2013-08-29T09:04:35.780Z","type":"message"}
{"nick":"kenansulayman","message":"undirender is cool","date":"2013-08-29T09:05:12.881Z","type":"message"}
{"nick":"kenansulayman","message":"substack Does it support words?","date":"2013-08-29T09:06:26.893Z","type":"message"}
{"nick":"kenansulayman","message":"That's roughly what we're currently generating for debugging: http://data.sly.mn/R5Yc","date":"2013-08-29T09:07:42.765Z","type":"message"}
{"nick":"kenansulayman","message":"woah dirender is pure sex","date":"2013-08-29T09:10:03.610Z","type":"message"}
{"nick":"kenansulayman","message":"gf","date":"2013-08-29T09:10:09.325Z","type":"message"}
{"nick":"kenansulayman","message":"gj* :D","date":"2013-08-29T09:10:13.039Z","type":"message"}
{"nick":"kenansulayman","message":"though some bugs","date":"2013-08-29T09:10:25.262Z","type":"message"}
{"nick":"kenansulayman","message":"substack why is it that the lines are shifted?","date":"2013-08-29T09:21:02.228Z","type":"message"}
{"nick":"kenansulayman","message":"                        \\__                                            Lucas¯¯¯¯¯¯¯¯","date":"2013-08-29T09:21:03.494Z","type":"message"}
{"nick":"kenansulayman","message":"                           \\__                    /¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯","date":"2013-08-29T09:21:03.703Z","type":"message"}
{"nick":"kenansulayman","message":"                              Laura","date":"2013-08-29T09:21:03.703Z","type":"message"}
{"nick":"jcrugzz","date":"2013-08-29T09:21:46.232Z","type":"join"}
{"nick":"kenansulayman","message":"oh I see","date":"2013-08-29T09:22:05.249Z","type":"message"}
{"nick":"kenansulayman","message":"it's ok if I change the viewport size","date":"2013-08-29T09:22:12.929Z","type":"message"}
{"nick":"jcrugzz","reason":"Ping timeout: 264 seconds","date":"2013-08-29T09:27:31.051Z","type":"quit"}
{"nick":"kenansulayman","message":"awesome it works!","date":"2013-08-29T09:34:03.238Z","type":"message"}
{"nick":"kenansulayman","message":"0ebe2799b23c32ab5bf579895fb8227fe103ac8c290cafa653d8393b0b66d65316c5c91807014a4a7f20530ebf359f2cd194d8e150289a56cf6aa8edb75edf1e","date":"2013-08-29T09:34:12.836Z","type":"message"}
{"nick":"kenansulayman","message":"|","date":"2013-08-29T09:34:13.048Z","type":"message"}
{"nick":"kenansulayman","message":"|                                        ffc45912c3796d7bad69390ce184d68be2e1776904be7237dddaad630442435b745f1925cc204dc29a50955f251623cb9634d9b4bb6e11a6fffa360825e5a6e5¯¯¯¯¯¯¯¯","date":"2013-08-29T09:34:13.048Z","type":"message"}
{"nick":"kenansulayman","message":"|                   /¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯","date":"2013-08-29T09:34:13.048Z","type":"message"}
{"nick":"kenansulayman","message":"606fe34513ee3816f17b8734b735e360ca806eb37b32f19cd64d4594ac4104f34d51d79cab5d15846150adc4edbd8de07a1b3b5dc1530dbfc31e8c5979e30c39","date":"2013-08-29T09:34:13.048Z","type":"message"}
{"nick":"thlorenz","date":"2013-08-29T09:38:28.207Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 264 seconds","date":"2013-08-29T09:43:06.983Z","type":"quit"}
{"nick":"chapel","date":"2013-08-29T09:50:21.083Z","type":"join"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T10:04:42.958Z","type":"quit"}
{"nick":"timoxley","date":"2013-08-29T10:09:38.936Z","type":"join"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-08-29T10:11:29.287Z","type":"quit"}
{"nick":"mcollina","date":"2013-08-29T10:17:48.494Z","type":"join"}
{"nick":"dominictarr","message":"kenansulayman: substack one thing that would be useful (graphlayoutwise) is archy + back links","date":"2013-08-29T10:25:47.987Z","type":"message"}
{"nick":"dominictarr","message":"you could show the npm dep tree, but also show visually which modules resolved to what.","date":"2013-08-29T10:26:10.363Z","type":"message"}
{"nick":"timoxley","date":"2013-08-29T10:26:24.551Z","type":"join"}
{"nick":"kenansulayman","message":"which modules resolved to what?","date":"2013-08-29T10:28:29.023Z","type":"message"}
{"nick":"kenansulayman","message":"k gotta go. cheers","date":"2013-08-29T10:30:54.275Z","type":"message"}
{"nick":"kenansulayman","reason":"Quit: ∞♡∞","date":"2013-08-29T10:31:44.667Z","type":"quit"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-08-29T10:33:34.463Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T10:38:51.355Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 264 seconds","date":"2013-08-29T10:43:40.428Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T11:39:13.825Z","type":"join"}
{"nick":"thlorenz","reason":"Ping timeout: 245 seconds","date":"2013-08-29T11:43:21.661Z","type":"quit"}
{"nick":"chapel","date":"2013-08-29T11:50:01.062Z","type":"join"}
{"nick":"thlorenz","date":"2013-08-29T11:56:13.919Z","type":"join"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T12:09:30.992Z","type":"quit"}
{"nick":"thlorenz_","date":"2013-08-29T12:39:39.723Z","type":"join"}
{"nick":"thlorenz_","reason":"Ping timeout: 240 seconds","date":"2013-08-29T12:43:43.372Z","type":"quit"}
{"nick":"mcollina","reason":"Read error: Connection reset by peer","date":"2013-08-29T12:58:44.557Z","type":"quit"}
{"nick":"mcollina","date":"2013-08-29T13:04:36.984Z","type":"join"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-08-29T13:13:00.711Z","type":"quit"}
{"nick":"tmcw","date":"2013-08-29T13:14:43.331Z","type":"join"}
{"nick":"timoxley","date":"2013-08-29T13:17:06.413Z","type":"join"}
{"nick":"thlorenz","date":"2013-08-29T13:40:03.333Z","type":"join"}
{"nick":"mcollina","reason":"Read error: Connection reset by peer","date":"2013-08-29T13:44:05.748Z","type":"quit"}
{"nick":"thlorenz","reason":"Ping timeout: 268 seconds","date":"2013-08-29T13:44:47.069Z","type":"quit"}
{"nick":"thlorenz","date":"2013-08-29T13:50:13.329Z","type":"join"}
{"nick":"werle","date":"2013-08-29T13:51:47.482Z","type":"join"}
{"nick":"werle","action":"","date":"2013-08-29T13:51:57.211Z","type":"action"}
{"nick":"werle","message":"juliangruber: when are we going to do sealevel haha","date":"2013-08-29T13:52:12.558Z","type":"message"}
{"nick":"juliangruber","message":"werle: that was the c rewrite of multilevel, right?","date":"2013-08-29T13:52:31.938Z","type":"message"}
{"nick":"chapel","date":"2013-08-29T13:52:42.603Z","type":"join"}
{"nick":"werle","message":"juliangruber: yeah","date":"2013-08-29T13:52:52.191Z","type":"message"}
{"nick":"juliangruber","message":"werle: that's a _big_ project","date":"2013-08-29T14:12:04.754Z","type":"message"}
{"nick":"chapel","reason":"Ping timeout: 264 seconds","date":"2013-08-29T14:23:55.079Z","type":"quit"}
{"nick":"esundahl","date":"2013-08-29T14:25:12.247Z","type":"join"}
{"nick":"esundahl","reason":"Remote host closed the connection","date":"2013-08-29T14:27:04.693Z","type":"quit"}
{"nick":"esundahl","date":"2013-08-29T14:27:30.456Z","type":"join"}
{"nick":"esundahl_","date":"2013-08-29T14:28:14.505Z","type":"join"}
{"nick":"esundahl","reason":"Ping timeout: 264 seconds","date":"2013-08-29T14:32:16.446Z","type":"quit"}
{"nick":"fallsemo","date":"2013-08-29T14:41:11.521Z","type":"join"}
{"nick":"jerrysv","date":"2013-08-29T14:44:07.549Z","type":"join"}
{"nick":"dominictarr","message":"juliangruber: werle we need a way to make node_modules pattern work in c","date":"2013-08-29T14:48:43.418Z","type":"message"}
{"nick":"dominictarr","message":"I think I could stand writing C if it was modular and collaborative","date":"2013-08-29T14:49:17.226Z","type":"message"}
{"nick":"juliangruber","message":"dominictarr: yes","date":"2013-08-29T14:50:15.451Z","type":"message"}
{"nick":"juliangruber","message":"dominictarr: i have this dream about npm being my operating system's package manager","date":"2013-08-29T14:50:25.768Z","type":"message"}
{"nick":"dominictarr","message":"juliangruber: me too","date":"2013-08-29T14:51:11.633Z","type":"message"}
{"nick":"juliangruber","message":"dominictarr: kickstarter?","date":"2013-08-29T14:51:35.655Z","type":"message"}
{"nick":"dominictarr","message":"we are calling that idea \"anarchy os\"","date":"2013-08-29T14:51:38.627Z","type":"message"}
{"nick":"rickbergfalk","date":"2013-08-29T14:51:43.625Z","type":"join"}
{"nick":"juliangruber","message":"sweet","date":"2013-08-29T14:52:07.059Z","type":"message"}
{"nick":"dominictarr","message":"juliangruber: it already has the best logo :)","date":"2013-08-29T14:52:21.501Z","type":"message"}
{"nick":"juliangruber","message":"dominictarr: substacks' drawing?","date":"2013-08-29T14:52:31.584Z","type":"message"}
{"nick":"dominictarr","message":"just the (A) thing in general","date":"2013-08-29T14:52:44.361Z","type":"message"}
{"nick":"dominictarr","message":"you can draw it loads of ways","date":"2013-08-29T14:52:51.873Z","type":"message"}
{"nick":"dominictarr","message":"juliangruber: I have other priorites (before c node_modules, but this needs to happen eventually)","date":"2013-08-29T14:53:48.390Z","type":"message"}
{"nick":"juliangruber","message":"yeah, same situation","date":"2013-08-29T14:54:16.515Z","type":"message"}
{"nick":"werle","reason":"Ping timeout: 264 seconds","date":"2013-08-29T14:55:07.036Z","type":"quit"}
{"nick":"julianduque","date":"2013-08-29T14:55:11.279Z","type":"join"}
{"nick":"werle","date":"2013-08-29T14:57:31.478Z","type":"join"}
{"nick":"jerrysv","message":"but if dependency hell went away, what would we have to bitch about?","date":"2013-08-29T15:01:45.572Z","type":"message"}
{"nick":"werle","reason":"Ping timeout: 264 seconds","date":"2013-08-29T15:01:54.285Z","type":"quit"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-08-29T15:10:00.152Z","type":"quit"}
{"nick":"kenansulayman","date":"2013-08-29T15:10:09.328Z","type":"join"}
{"nick":"dominictarr","message":"jerrysv: there would still be callbacks!","date":"2013-08-29T15:14:23.252Z","type":"message"}
{"nick":"dominictarr","message":"jerrysv: also, pointer overruns","date":"2013-08-29T15:14:37.110Z","type":"message"}
{"nick":"dominictarr","message":"and that all the good module names are already taken.","date":"2013-08-29T15:14:59.900Z","type":"message"}
{"nick":"jerrysv","message":"hmph.","date":"2013-08-29T15:18:45.072Z","type":"message"}
{"nick":"dominictarr","message":"also, stupid kickstarter projects that should never be funded","date":"2013-08-29T15:19:03.118Z","type":"message"}
{"nick":"dominictarr","message":"and how twitter trends never reflects important current events","date":"2013-08-29T15:19:29.436Z","type":"message"}
{"nick":"dominictarr","message":"or people using whitespace the wrong way","date":"2013-08-29T15:20:10.520Z","type":"message"}
{"nick":"dominictarr","message":"spelling mistakes","date":"2013-08-29T15:20:35.435Z","type":"message"}
{"nick":"dominictarr","message":"jerrysv: ! I know !","date":"2013-08-29T15:21:03.869Z","type":"message"}
{"nick":"jerrysv","message":"c is my native language","date":"2013-08-29T15:21:04.084Z","type":"message"}
{"nick":"dominictarr","message":"what we'll also give C optional semicolons!","date":"2013-08-29T15:21:24.197Z","type":"message"}
{"nick":"dominictarr","message":"that will add a new thing, to replace dependency hell","date":"2013-08-29T15:22:00.810Z","type":"message"}
{"nick":"mbalho","message":"good idea","date":"2013-08-29T15:23:09.686Z","type":"message"}
{"nick":"kenansulayman","message":"I got some spare time, what about cloning logs.nodejs.org for level","date":"2013-08-29T15:31:41.866Z","type":"message"}
{"nick":"jxson","date":"2013-08-29T15:39:50.569Z","type":"join"}
{"nick":"chapel","date":"2013-08-29T15:50:26.867Z","type":"join"}
