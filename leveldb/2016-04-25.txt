{"nick":"ralphtheninja","reason":"Ping timeout: 276 seconds","date":"2016-04-25T00:19:03.631Z","type":"quit"}
{"nick":"ralphtheninja","date":"2016-04-25T00:19:10.935Z","type":"join"}
{"nick":"jerrysv","reason":"Remote host closed the connection","date":"2016-04-25T02:15:25.805Z","type":"quit"}
{"nick":"jerrysv","date":"2016-04-25T02:16:04.944Z","type":"join"}
{"nick":"jerrysv_","date":"2016-04-25T02:16:48.912Z","type":"join"}
{"nick":"jerrysv","reason":"Ping timeout: 244 seconds","date":"2016-04-25T02:20:26.930Z","type":"quit"}
{"nick":"jerrysv_","reason":"Remote host closed the connection","date":"2016-04-25T02:23:14.753Z","type":"quit"}
{"nick":"jerrysv","date":"2016-04-25T02:23:51.681Z","type":"join"}
{"nick":"jerrysv_","date":"2016-04-25T03:10:40.982Z","type":"join"}
{"nick":"jerrysv","reason":"Ping timeout: 276 seconds","date":"2016-04-25T03:13:54.629Z","type":"quit"}
{"nick":"jerrysv_","new_nick":"jerrysv","date":"2016-04-25T03:15:02.501Z","type":"nick"}
{"nick":"jerrysv","reason":"Remote host closed the connection","date":"2016-04-25T03:26:22.394Z","type":"quit"}
{"nick":"topi`","date":"2016-04-25T13:22:03.787Z","type":"join"}
{"nick":"topi`","message":"how does Snappy work inside LevelDB? If I store a lot of tiny bits of JSON data in LevelDB, is Snappy able to \"cross the boundaries\" so to speak and hence compress better?","date":"2016-04-25T13:22:43.732Z","type":"message"}
{"nick":"topi`","message":"let's say I'm storing temperature data, like {\"temperature\":20.0}","date":"2016-04-25T13:22:59.867Z","type":"message"}
{"nick":"topi`","message":"that's a lot of bytes when you only need the \"20.0\" bit","date":"2016-04-25T13:23:16.667Z","type":"message"}
{"nick":"lennon","message":"topi`: afaik snappy compresses the files (so a lots of records at once, so i guess it can compress those JSON objects very well)","date":"2016-04-25T15:08:17.653Z","type":"message"}
{"nick":"lennon","message":"also, snappy aims for speed, not compression ratio","date":"2016-04-25T15:09:23.243Z","type":"message"}
{"nick":"serapath","date":"2016-04-25T18:01:31.757Z","type":"join"}
{"nick":"dguttman","date":"2016-04-25T18:37:18.667Z","type":"join"}
{"nick":"jameskyburz","message":"mafintosh: I am creating a new module using a rpc solutiom along the lines of multileveldown using protocol-buffers. Do you have time for some questions?","date":"2016-04-25T18:41:46.105Z","type":"message"}
{"nick":"mafintosh","message":"jameskyburz: shoot","date":"2016-04-25T19:13:38.988Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: thanks! I am writing an rpc event emitter using ltgt ranges. Using the same internals as multileveldown....subscribe(range, (key) => {}) publish(key)....","date":"2016-04-25T19:23:40.421Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: All is going the plan except for encoding. When the messages are encoding uses level-protocol ranges and keys end up as buffers when decoded by the server.","date":"2016-04-25T19:27:17.655Z","type":"message"}
{"nick":"mafintosh","message":"jameskyburz: is your schema using bytes as the type?","date":"2016-04-25T19:28:47.320Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: I have looked into level-codec for using the same key encoding as the leveldb instance however on the server side there is no decode for buffers.","date":"2016-04-25T19:28:53.807Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: yes the schema is using bytes","date":"2016-04-25T19:29:05.182Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: I will only be using utf-8 keys but thought it would be nice to publish a module where binary key pubsub was also possible.","date":"2016-04-25T19:30:05.958Z","type":"message"}
{"nick":"mafintosh","message":"jameskyburz: in multileveldown i just always use buffers and use a leveldown instead levelup","date":"2016-04-25T19:30:51.865Z","type":"message"}
{"nick":"mafintosh","message":"jameskyburz: then you dont need to worry about any encoding","date":"2016-04-25T19:31:17.810Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: :) My idea is to have this publish/subscribe using ltgt ranges but not tied to level. The problem is decoding publish(key) and subscribe(range) messages when they are sent as buffers.","date":"2016-04-25T19:35:19.859Z","type":"message"}
{"nick":"mafintosh","message":"ah okay.","date":"2016-04-25T19:36:38.290Z","type":"message"}
{"nick":"mafintosh","message":"jameskyburz: and you cannot just tostring the buffer? :)","date":"2016-04-25T19:37:13.891Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: I can with the exception of json and custom encoding. Am wondering wether the best solution isn't using string types :)","date":"2016-04-25T19:39:11.596Z","type":"message"}
{"nick":"mafintosh","message":"jameskyburz: so cool thing about protobuf is that strings and bytes are encoded the same way on the wire","date":"2016-04-25T19:41:45.755Z","type":"message"}
{"nick":"mafintosh","message":"jameskyburz: so you can always use strings now and change it later","date":"2016-04-25T19:42:11.438Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: awesome thanks!!!!","date":"2016-04-25T19:42:28.619Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: Am loving playing with multileveldown. I tried an experiment using the server code in the browser (using memdown) and the client code in the server. Worked like a charm :) just for fun....Also thanks to your length-prefixed-stream and such I managed to have a double duplex stream over the same websocket. Not sure if it works 100% yet but the fact it worked at all is pretty cool!","date":"2016-04-25T19:45:42.016Z","type":"message"}
{"nick":"jameskyburz","message":"mafintosh: Thanks for your help will use string types for now and forget about encoding until I need to!!","date":"2016-04-25T19:46:50.656Z","type":"message"}
{"nick":"jerrysv_","date":"2016-04-25T23:45:35.909Z","type":"join"}
