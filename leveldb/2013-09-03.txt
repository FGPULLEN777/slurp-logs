{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-09-03T00:10:07.453Z","type":"quit"}
{"nick":"werle","date":"2013-09-03T00:10:39.488Z","type":"join"}
{"nick":"mikeal","date":"2013-09-03T00:12:16.577Z","type":"join"}
{"nick":"timoxley","reason":"Remote host closed the connection","date":"2013-09-03T00:15:54.709Z","type":"quit"}
{"nick":"timoxley","date":"2013-09-03T00:16:10.986Z","type":"join"}
{"nick":"kenansulayman","reason":"Quit: ≈ and thus my mac took a subtle yet profound nap ≈","date":"2013-09-03T00:33:48.094Z","type":"quit"}
{"nick":"werle","reason":"Ping timeout: 264 seconds","date":"2013-09-03T00:38:30.226Z","type":"quit"}
{"nick":"levelbot","message":"[npm] level-sleep@0.3.0 <http://npm.im/level-sleep>: Database for storing, cloneing and replicating SLEEP compatible data stores. (@mikeal)","date":"2013-09-03T01:21:21.310Z","type":"message"}
{"nick":"mikeal","message":"haha","date":"2013-09-03T01:46:43.526Z","type":"message"}
{"nick":"mikeal","message":"someone doesn't like typos in this channel :)","date":"2013-09-03T01:46:49.775Z","type":"message"}
{"nick":"st_luke","date":"2013-09-03T01:48:38.335Z","type":"join"}
{"nick":"thlorenz","date":"2013-09-03T01:53:12.993Z","type":"join"}
{"nick":"st_luke","reason":"Read error: Connection reset by peer","date":"2013-09-03T02:02:15.247Z","type":"quit"}
{"nick":"st_luke","date":"2013-09-03T02:02:43.840Z","type":"join"}
{"nick":"davidstrauss","reason":"Quit: No Ping reply in 180 seconds.","date":"2013-09-03T03:01:30.210Z","type":"quit"}
{"nick":"davidstrauss","date":"2013-09-03T03:01:53.863Z","type":"join"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-09-03T03:08:30.192Z","type":"quit"}
{"nick":"mikeal","message":"is there a number of keys at which leveldb just falls over?","date":"2013-09-03T04:06:24.723Z","type":"message"}
{"nick":"rvagg","message":"not that I'm aware of","date":"2013-09-03T04:29:58.361Z","type":"message"}
{"nick":"mbalho","message":"mikeal: whatever teh max number of files in a folder is would be the limit","date":"2013-09-03T04:32:06.024Z","type":"message"}
{"nick":"mbalho","message":"mikeal: 2mb per file","date":"2013-09-03T04:32:17.193Z","type":"message"}
{"nick":"mbalho","message":"http://stackoverflow.com/questions/7722130/what-is-the-max-number-of-files-that-can-be-kept-in-a-single-folder-on-win7-mac","date":"2013-09-03T04:32:39.671Z","type":"message"}
{"nick":"mikeal","message":"after a repair it works much better","date":"2013-09-03T04:38:50.076Z","type":"message"}
{"nick":"mikeal","message":"was seeing some slowness","date":"2013-09-03T04:38:55.859Z","type":"message"}
{"nick":"st_luke","reason":"Remote host closed the connection","date":"2013-09-03T05:01:23.375Z","type":"quit"}
{"nick":"st_luke","date":"2013-09-03T05:42:18.336Z","type":"join"}
{"nick":"mcollina","reason":"Remote host closed the connection","date":"2013-09-03T06:05:48.689Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T06:13:22.371Z","type":"join"}
{"nick":"levelbot","message":"[npm] level-assoc@0.8.0 <http://npm.im/level-assoc>: relational foreign key associations (hasMany, belongsTo) for leveldb (@substack)","date":"2013-09-03T06:28:50.681Z","type":"message"}
{"nick":"levelbot","message":"[npm] level-batcher@0.0.1 <http://npm.im/level-batcher>: stream designed for leveldb that you write objects to, and it emits batches of objects that are under a byte size limit (@maxogden)","date":"2013-09-03T06:32:52.119Z","type":"message"}
{"nick":"mbalho","message":"yayay","date":"2013-09-03T06:33:11.486Z","type":"message"}
{"nick":"mbalho","message":"o/","date":"2013-09-03T06:33:13.686Z","type":"message"}
{"nick":"substack","message":"nice","date":"2013-09-03T06:41:07.168Z","type":"message"}
{"nick":"wolfeidau","reason":"Remote host closed the connection","date":"2013-09-03T06:52:51.028Z","type":"quit"}
{"nick":"wolfeidau","date":"2013-09-03T06:53:06.010Z","type":"join"}
{"nick":"wolfeidau","reason":"Remote host closed the connection","date":"2013-09-03T06:53:15.764Z","type":"quit"}
{"nick":"mcollina","reason":"Remote host closed the connection","date":"2013-09-03T06:58:56.970Z","type":"quit"}
{"nick":"Acconut","date":"2013-09-03T07:06:13.795Z","type":"join"}
{"nick":"Acconut","reason":"Remote host closed the connection","date":"2013-09-03T07:08:53.344Z","type":"quit"}
{"nick":"st_luke","reason":"Remote host closed the connection","date":"2013-09-03T07:09:36.191Z","type":"quit"}
{"nick":"wolfeidau","date":"2013-09-03T07:26:07.128Z","type":"join"}
{"nick":"dominictarr","date":"2013-09-03T07:28:43.767Z","type":"join"}
{"nick":"substack","reason":"Remote host closed the connection","date":"2013-09-03T07:38:08.259Z","type":"quit"}
{"nick":"jcrugzz","reason":"Ping timeout: 245 seconds","date":"2013-09-03T07:41:48.168Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T07:48:11.840Z","type":"join"}
{"nick":"wolfeidau","reason":"Remote host closed the connection","date":"2013-09-03T07:49:24.122Z","type":"quit"}
{"nick":"wolfeidau","date":"2013-09-03T07:51:48.639Z","type":"join"}
{"nick":"mcollina","reason":"Read error: Connection reset by peer","date":"2013-09-03T07:54:19.843Z","type":"quit"}
{"nick":"mcollina_","date":"2013-09-03T07:54:25.368Z","type":"join"}
{"nick":"mcollina_","reason":"Read error: Connection reset by peer","date":"2013-09-03T07:56:54.973Z","type":"quit"}
{"nick":"dominictarr","reason":"Quit: dominictarr","date":"2013-09-03T07:57:39.088Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T08:00:54.329Z","type":"join"}
{"nick":"mcollina","reason":"Read error: No route to host","date":"2013-09-03T08:08:04.144Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T08:08:20.993Z","type":"join"}
{"nick":"dominictarr","date":"2013-09-03T08:22:24.798Z","type":"join"}
{"nick":"jcrugzz","date":"2013-09-03T08:31:19.895Z","type":"join"}
{"nick":"mcollina_","date":"2013-09-03T08:38:34.423Z","type":"join"}
{"nick":"mcollina","reason":"Ping timeout: 264 seconds","date":"2013-09-03T08:42:14.781Z","type":"quit"}
{"nick":"mcollina_","reason":"Remote host closed the connection","date":"2013-09-03T08:45:45.594Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T09:01:45.255Z","type":"join"}
{"nick":"kenansulayman","date":"2013-09-03T09:17:32.345Z","type":"join"}
{"nick":"mcollina","reason":"Remote host closed the connection","date":"2013-09-03T09:33:26.132Z","type":"quit"}
{"nick":"jcrugzz","reason":"Ping timeout: 245 seconds","date":"2013-09-03T09:50:26.655Z","type":"quit"}
{"nick":"wolfeidau","reason":"Remote host closed the connection","date":"2013-09-03T10:03:15.484Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T10:27:19.859Z","type":"join"}
{"nick":"mcollina","reason":"Ping timeout: 245 seconds","date":"2013-09-03T10:52:31.650Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T10:59:06.754Z","type":"join"}
{"nick":"rud","reason":"Quit: rud","date":"2013-09-03T11:02:14.446Z","type":"quit"}
{"nick":"wolfeidau","date":"2013-09-03T11:41:25.109Z","type":"join"}
{"nick":"werle","date":"2013-09-03T11:56:45.503Z","type":"join"}
{"nick":"mcollina","reason":"Remote host closed the connection","date":"2013-09-03T11:57:46.511Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T12:20:25.684Z","type":"join"}
{"nick":"mcollina","reason":"Ping timeout: 245 seconds","date":"2013-09-03T12:27:31.644Z","type":"quit"}
{"nick":"thlorenz","date":"2013-09-03T12:34:28.369Z","type":"join"}
{"nick":"rud","date":"2013-09-03T12:52:34.797Z","type":"join"}
{"nick":"mcollina","date":"2013-09-03T13:06:15.049Z","type":"join"}
{"nick":"mcollina","reason":"Read error: Connection reset by peer","date":"2013-09-03T13:12:11.969Z","type":"quit"}
{"nick":"mcollina","date":"2013-09-03T13:12:34.977Z","type":"join"}
{"nick":"mcollina","reason":"Remote host closed the connection","date":"2013-09-03T13:12:43.948Z","type":"quit"}
{"nick":"kenansulayman","reason":"Quit: ≈ and thus my mac took a subtle yet profound nap ≈","date":"2013-09-03T13:33:03.270Z","type":"quit"}
{"nick":"kenansulayman","date":"2013-09-03T13:35:16.384Z","type":"join"}
{"nick":"kenansulayman","reason":"Client Quit","date":"2013-09-03T13:35:30.961Z","type":"quit"}
{"nick":"tmcw","date":"2013-09-03T13:49:42.655Z","type":"join"}
{"nick":"tmcw","reason":"Remote host closed the connection","date":"2013-09-03T13:50:00.688Z","type":"quit"}
{"nick":"tmcw","date":"2013-09-03T13:50:15.144Z","type":"join"}
{"nick":"julianduque","reason":"Quit: leaving","date":"2013-09-03T13:52:35.903Z","type":"quit"}
{"nick":"fallsemo","date":"2013-09-03T13:56:18.492Z","type":"join"}
{"nick":"rud","reason":"Quit: rud","date":"2013-09-03T14:36:15.982Z","type":"quit"}
{"nick":"jerrysv","date":"2013-09-03T15:01:59.828Z","type":"join"}
{"nick":"ramitos","date":"2013-09-03T15:08:44.006Z","type":"join"}
{"nick":"mikeal1","date":"2013-09-03T15:12:44.985Z","type":"join"}
{"nick":"mikeal","reason":"Ping timeout: 256 seconds","date":"2013-09-03T15:13:35.533Z","type":"quit"}
{"nick":"rickbergfalk","date":"2013-09-03T15:16:06.291Z","type":"join"}
{"nick":"rud","date":"2013-09-03T15:19:11.119Z","type":"join"}
{"nick":"rud","reason":"Changing host","date":"2013-09-03T15:19:11.333Z","type":"quit"}
{"nick":"rud","date":"2013-09-03T15:19:11.333Z","type":"join"}
{"nick":"mikeal1","reason":"Quit: Leaving.","date":"2013-09-03T15:21:03.564Z","type":"quit"}
{"nick":"mikeal","date":"2013-09-03T15:21:51.292Z","type":"join"}
{"nick":"jcrugzz","date":"2013-09-03T15:25:19.261Z","type":"join"}
{"nick":"rud","reason":"Quit: rud","date":"2013-09-03T15:26:35.009Z","type":"quit"}
{"nick":"jerrysv","reason":"Read error: Connection reset by peer","date":"2013-09-03T15:34:23.531Z","type":"quit"}
{"nick":"jerrysv_","date":"2013-09-03T15:34:26.601Z","type":"join"}
{"nick":"dominictarr","reason":"Quit: dominictarr","date":"2013-09-03T15:50:03.747Z","type":"quit"}
{"nick":"levelbot","message":"[npm] tik@0.0.5 <http://npm.im/tik>: command line key/value store (@jarofghosts)","date":"2013-09-03T15:58:19.367Z","type":"message"}
{"nick":"ryan_ramage","date":"2013-09-03T15:59:23.044Z","type":"join"}
{"nick":"esundahl","date":"2013-09-03T16:03:10.652Z","type":"join"}
{"nick":"rud","date":"2013-09-03T16:06:32.854Z","type":"join"}
{"nick":"jerrysv_","new_nick":"jerrysv","date":"2013-09-03T16:43:03.095Z","type":"nick"}
{"nick":"ednapiranha","date":"2013-09-03T16:46:57.442Z","type":"join"}
{"nick":"ednapiranha","reason":"Remote host closed the connection","date":"2013-09-03T16:47:05.125Z","type":"quit"}
{"nick":"ednapiranha","date":"2013-09-03T16:47:18.502Z","type":"join"}
{"nick":"dominictarr","date":"2013-09-03T16:56:18.329Z","type":"join"}
{"nick":"dguttman","date":"2013-09-03T16:57:56.002Z","type":"join"}
{"nick":"substack","date":"2013-09-03T16:58:03.053Z","type":"join"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-09-03T17:05:17.834Z","type":"quit"}
{"nick":"jerrysv","reason":"Read error: Connection reset by peer","date":"2013-09-03T17:05:18.046Z","type":"quit"}
{"nick":"jxson","date":"2013-09-03T17:10:54.985Z","type":"join"}
{"nick":"jxson","reason":"Remote host closed the connection","date":"2013-09-03T17:11:57.629Z","type":"quit"}
{"nick":"jxson","date":"2013-09-03T17:12:04.785Z","type":"join"}
{"nick":"dguttman","reason":"Quit: dguttman","date":"2013-09-03T17:43:36.168Z","type":"quit"}
{"nick":"dguttman","date":"2013-09-03T17:54:41.228Z","type":"join"}
{"nick":"mikeal","date":"2013-09-03T18:01:01.995Z","type":"join"}
{"nick":"jxson","reason":"Remote host closed the connection","date":"2013-09-03T18:01:26.009Z","type":"quit"}
{"nick":"jxson","date":"2013-09-03T18:06:28.182Z","type":"join"}
{"nick":"jxson","reason":"Remote host closed the connection","date":"2013-09-03T18:06:40.004Z","type":"quit"}
{"nick":"jxson","date":"2013-09-03T18:07:46.291Z","type":"join"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-09-03T18:11:36.669Z","type":"quit"}
{"nick":"ryan_ramage","reason":"Quit: ryan_ramage","date":"2013-09-03T18:12:24.193Z","type":"quit"}
{"nick":"mikeal","date":"2013-09-03T18:14:39.571Z","type":"join"}
{"nick":"dguttman","reason":"Ping timeout: 260 seconds","date":"2013-09-03T18:19:43.957Z","type":"quit"}
{"nick":"rud","reason":"Quit: rud","date":"2013-09-03T18:24:53.043Z","type":"quit"}
{"nick":"rud","date":"2013-09-03T18:59:17.782Z","type":"join"}
{"nick":"rud","reason":"Changing host","date":"2013-09-03T18:59:18.031Z","type":"quit"}
{"nick":"rud","date":"2013-09-03T18:59:18.031Z","type":"join"}
{"nick":"ryan_ramage","date":"2013-09-03T19:10:29.928Z","type":"join"}
{"nick":"mbalho","message":"muahaha https://github.com/maxogden/level-batcher/blob/master/index.js#L15","date":"2013-09-03T19:39:06.566Z","type":"message"}
{"nick":"julianduque","date":"2013-09-03T19:46:55.936Z","type":"join"}
{"nick":"rescrv","message":"mbalho: does level-batcher block until the batch is full?","date":"2013-09-03T19:47:13.978Z","type":"message"}
{"nick":"mbalho","message":"rescrv: it buffers until the batch is full","date":"2013-09-03T19:47:49.917Z","type":"message"}
{"nick":"mbalho","message":"rescrv: also, and i am not sure this is the best behavior, it wont emit another batch until youve called batcher.next()","date":"2013-09-03T19:48:24.502Z","type":"message"}
{"nick":"mbalho","message":"rescrv: the goal was to make sure backpressuve events propagate, so when it emits a batch it then pauses itself so writes from the source stream will receive backpressure signals","date":"2013-09-03T19:48:53.823Z","type":"message"}
{"nick":"mbalho","message":"rescrv: then when you call .next() it calls .resume() internally and will emit another batch when it fills up (which may be immediately if there was enough buffered data while it was paused)","date":"2013-09-03T19:49:28.138Z","type":"message"}
{"nick":"mbalho","message":"rescrv: while it is paused anything that writes to it will receive a return value of false which is an advisory 'stop sending me data' signal","date":"2013-09-03T19:50:03.877Z","type":"message"}
{"nick":"mbalho","message":"mikeal: do you have a recommendation on how to do batches with read on write semantics?","date":"2013-09-03T19:52:37.255Z","type":"message"}
{"nick":"mikeal","message":"level-mutex shows pretty much how to do read on write locks properly","date":"2013-09-03T19:53:05.744Z","type":"message"}
{"nick":"mikeal","message":"it batches *everything* pending together","date":"2013-09-03T19:53:16.374Z","type":"message"}
{"nick":"mikeal","message":"if you want to break it up by the ideal length","date":"2013-09-03T19:53:22.767Z","type":"message"}
{"nick":"mikeal","message":"or wait some static number of ms before write waiting for more to buffer","date":"2013-09-03T19:53:35.336Z","type":"message"}
{"nick":"mikeal","message":"i would add those semantics to the write in level-mutex","date":"2013-09-03T19:53:44.150Z","type":"message"}
{"nick":"mikeal","message":"there are a lot of \"gotchas\" in the mutex","date":"2013-09-03T19:53:55.849Z","type":"message"}
{"nick":"mbalho","message":"mikeal: if i have 1000 documents to write and i need to check if they exist first and compare revs do i need to do 1000 gets before i create my batch?","date":"2013-09-03T19:54:02.658Z","type":"message"}
{"nick":"mikeal","message":"which is why is say \"just look at level-mutex\"","date":"2013-09-03T19:54:06.082Z","type":"message"}
{"nick":"mikeal","message":"not saying you can't write your own, but you should steal most of the code there first","date":"2013-09-03T19:54:15.711Z","type":"message"}
{"nick":"mikeal","message":"so, level-mutex doesn't care if the reads are *required*, some higher order library is readings before writing and making that judement","date":"2013-09-03T19:54:44.556Z","type":"message"}
{"nick":"mbalho","message":"mikeal: i have a thing that breaks up an object into arrays of objects at optimal batch sizes https://github.com/maxogden/level-batcher","date":"2013-09-03T19:54:47.061Z","type":"message"}
{"nick":"rescrv","message":"from my experience with HyperLevelDB, you should write what you have when you have it.  batch only what comes in while you're writing.  Based on the internals, it'll not be a loss of performance.","date":"2013-09-03T19:55:02.143Z","type":"message"}
{"nick":"mikeal","message":"the trick is, all the reads need to be done at once, then all the writes, to insure that no write happens between my read and my write","date":"2013-09-03T19:55:04.209Z","type":"message"}
{"nick":"mbalho","message":"mikeal: so do you get one at a time until youve got them all and then write all puts in one batch before doing anything else/","date":"2013-09-03T19:56:41.018Z","type":"message"}
{"nick":"mikeal","message":"no no no","date":"2013-09-03T19:57:00.481Z","type":"message"}
{"nick":"mikeal","message":"all the reads happen in a block, then return them in the order the mutex got them, then *all* the writes","date":"2013-09-03T19:57:16.918Z","type":"message"}
{"nick":"mikeal","message":"so if you want atomicity at the document level you write that in your library, the mutex just insures that writes don't happen between your last read and the write you're scheduling","date":"2013-09-03T19:57:48.433Z","type":"message"}
{"nick":"mikeal","message":"keep in mind that, you'll still need to proactively decline any more writes to the same document after you've scheduled one","date":"2013-09-03T19:58:05.922Z","type":"message"}
{"nick":"mikeal","message":"as an example you can look at couchup","date":"2013-09-03T19:58:20.170Z","type":"message"}
{"nick":"mikeal","message":"but you'll have to sort through all of couch's revision checking/setting and newWrites:false logic","date":"2013-09-03T19:58:48.022Z","type":"message"}
{"nick":"mbalho","message":"my .put is here https://github.com/maxogden/dat/blob/master/lib/storage.js#L82","date":"2013-09-03T19:59:30.923Z","type":"message"}
{"nick":"mikeal","message":"that is really similar to mine","date":"2013-09-03T20:00:27.745Z","type":"message"}
{"nick":"mikeal","message":"yeah, you need to protect against concurrent writes in the same mutex lock","date":"2013-09-03T20:00:37.934Z","type":"message"}
{"nick":"mikeal","message":"mbalho: https://github.com/mikeal/couchup/blob/master/index.js#L128","date":"2013-09-03T20:01:49.777Z","type":"message"}
{"nick":"mikeal","message":"https://github.com/mikeal/couchup/blob/master/index.js#L311","date":"2013-09-03T20:02:01.263Z","type":"message"}
{"nick":"mbalho","message":"mikeal: i think what i will do is have 2 code paths, one for data without _ids (where i know they dont exist yet) that just inserts as batches as fast as possible","date":"2013-09-03T20:02:01.693Z","type":"message"}
{"nick":"mbalho","message":"and another for ones that need read on write","date":"2013-09-03T20:02:07.017Z","type":"message"}
{"nick":"mikeal","message":"https://github.com/mikeal/couchup/blob/master/index.js#L328","date":"2013-09-03T20:02:11.328Z","type":"message"}
{"nick":"mbalho","message":"mikeal: oh right","date":"2013-09-03T20:02:19.919Z","type":"message"}
{"nick":"mbalho","message":"mikeal: i wonder if ids can be optimized for read on write insert speed","date":"2013-09-03T20:03:55.227Z","type":"message"}
{"nick":"ednapiranha","message":"mbalho: !","date":"2013-09-03T20:03:55.647Z","type":"message"}
{"nick":"mbalho","message":"ednapiranha: YO","date":"2013-09-03T20:04:00.380Z","type":"message"}
{"nick":"ednapiranha","message":"mbalho: I AM IN PDX","date":"2013-09-03T20:04:03.997Z","type":"message"}
{"nick":"mbalho","message":"ednapiranha: wassup pdx hipsster","date":"2013-09-03T20:04:06.897Z","type":"message"}
{"nick":"ednapiranha","message":"in the moz office","date":"2013-09-03T20:04:08.516Z","type":"message"}
{"nick":"ednapiranha","message":"lol","date":"2013-09-03T20:04:11.610Z","type":"message"}
{"nick":"mbalho","message":"ednapiranha: high five dietrich for me","date":"2013-09-03T20:04:19.627Z","type":"message"}
{"nick":"mbalho","message":"ednapiranha: and mikeal","date":"2013-09-03T20:04:21.961Z","type":"message"}
{"nick":"ednapiranha","message":"i dont think he's in today","date":"2013-09-03T20:04:25.964Z","type":"message"}
{"nick":"mbalho","message":"(for me and mikeal)","date":"2013-09-03T20:04:27.834Z","type":"message"}
{"nick":"mikeal","message":"mbalho: what do you mean?","date":"2013-09-03T20:04:28.051Z","type":"message"}
{"nick":"mbalho","message":"ah","date":"2013-09-03T20:04:28.260Z","type":"message"}
{"nick":"ednapiranha","message":"haha","date":"2013-09-03T20:04:37.477Z","type":"message"}
{"nick":"mikeal","message":"how would you optimize the id?","date":"2013-09-03T20:04:45.118Z","type":"message"}
{"nick":"mbalho","message":"mikeal: so you can use a read stream to get the ids more efficiently than a buncha random reads","date":"2013-09-03T20:05:08.865Z","type":"message"}
{"nick":"mikeal","message":"i store the sequence in the key, so a get() is really a peekLast","date":"2013-09-03T20:05:09.080Z","type":"message"}
{"nick":"mbalho","message":"mikeal: but i guess you dont know how sparse the bulk insert data is","date":"2013-09-03T20:05:22.563Z","type":"message"}
{"nick":"jcrugzz","reason":"Ping timeout: 260 seconds","date":"2013-09-03T20:05:40.018Z","type":"quit"}
{"nick":"mikeal","message":"oh i see, so like, rather than do a bunch of gets for level keys in the mutex block you want to get a readstream?","date":"2013-09-03T20:05:57.617Z","type":"message"}
{"nick":"Acconut","date":"2013-09-03T20:06:13.368Z","type":"join"}
{"nick":"mikeal","message":"i talked with rvagg about optimizing reads at one point","date":"2013-09-03T20:06:19.458Z","type":"message"}
{"nick":"mbalho","message":"yea but full table scans are slow on large datasets so i dont think that would make sense for batches where you are inserting two rows like ['a', 'z']","date":"2013-09-03T20:06:32.959Z","type":"message"}
{"nick":"mikeal","message":"and i think i remember him saying that it's best to just do them concurrently","date":"2013-09-03T20:06:36.562Z","type":"message"}
{"nick":"mikeal","message":"i wonder how well that is optimized","date":"2013-09-03T20:07:05.136Z","type":"message"}
{"nick":"mbalho","message":"mikeal: like i if i sort the batch and then use a series of readstreams...","date":"2013-09-03T20:07:08.913Z","type":"message"}
{"nick":"mbalho","message":"hmm but theres still no way to know how sparse the data is","date":"2013-09-03T20:07:37.881Z","type":"message"}
{"nick":"mikeal","message":"i'm willing to bet that 1) it is still faster to do them concurrently 2) it is probably faster to sort them before asking for them","date":"2013-09-03T20:08:01.801Z","type":"message"}
{"nick":"mikeal","message":"so long as you return them in order out of the mutex","date":"2013-09-03T20:08:14.158Z","type":"message"}
{"nick":"mbalho","message":"like if i sort and then do all the individual .gets in sorted order?","date":"2013-09-03T20:08:32.637Z","type":"message"}
{"nick":"mikeal","message":"if you don't return them in out of the mutex you aren't being \"fair\" about who wins in a concurrent udpate","date":"2013-09-03T20:08:34.635Z","type":"message"}
{"nick":"mikeal","message":"mbalho: yeah, i'm willing to bet it's faster","date":"2013-09-03T20:08:52.422Z","type":"message"}
{"nick":"mbalho","message":"rescrv: do you think that random reads executed in order sorted by key are faster than random reads executed in random order?","date":"2013-09-03T20:09:44.850Z","type":"message"}
{"nick":"rescrv","message":"mikeal: I was not talking about a r/w case. mbalho has made a batcher based on my feedback, and I was suggesting that he not wait for a full batch size unless he has to","date":"2013-09-03T20:10:43.817Z","type":"message"}
{"nick":"rescrv","message":"mbalho: I don't think it'll matter unless doing so keeps things fresh in cache (e.g., adjacent keys land in an SST).","date":"2013-09-03T20:11:11.638Z","type":"message"}
{"nick":"rescrv","message":"I certainly don't think it'll be like it was for writes","date":"2013-09-03T20:11:20.020Z","type":"message"}
{"nick":"rescrv","message":"that's just an extreme case","date":"2013-09-03T20:11:24.011Z","type":"message"}
{"nick":"mbalho","message":"gotcha","date":"2013-09-03T20:12:00.240Z","type":"message"}
{"nick":"mbalho","message":"rescrv: also good point on the batcher semantics","date":"2013-09-03T20:12:11.541Z","type":"message"}
{"nick":"mikeal","message":"right","date":"2013-09-03T20:14:56.279Z","type":"message"}
{"nick":"thlorenz","message":"so my level writes are getting backed up and cause memory to grow somehow","date":"2013-09-03T20:15:35.011Z","type":"message"}
{"nick":"mikeal","message":"rescrv: what i'm interested in this for is level-mutex, which already holds on to a batch until the reads are completed, and it would be best to restrict the writes to a certain size if it is faster to wait","date":"2013-09-03T20:15:36.735Z","type":"message"}
{"nick":"mikeal","message":"thlorenz: yeah, same issue, we need streams2 pull style streams","date":"2013-09-03T20:15:54.279Z","type":"message"}
{"nick":"thlorenz","message":"switching to leveldown-hyper brought only small relief","date":"2013-09-03T20:15:59.284Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: how are you writing","date":"2013-09-03T20:16:09.872Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: batches","date":"2013-09-03T20:16:16.493Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: one at a time or multiple, also how big are the batches","date":"2013-09-03T20:16:25.080Z","type":"message"}
{"nick":"thlorenz","message":"but pretty big ones and lots - downloading half of github ;)","date":"2013-09-03T20:16:31.473Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: read through https://github.com/maxogden/level-bulk-load/issues/1","date":"2013-09-03T20:16:55.133Z","type":"message"}
{"nick":"thlorenz","message":"haven't measured, but they'd contain data for all the repos of a user","date":"2013-09-03T20:16:58.818Z","type":"message"}
{"nick":"mikeal","message":"thlorenz: i'm downloading *so* much of GitHub :)","date":"2013-09-03T20:17:00.708Z","type":"message"}
{"nick":"mikeal","message":"i'm 300K through 3M commit details i need to get","date":"2013-09-03T20:17:13.890Z","type":"message"}
{"nick":"mikeal","message":"i don't think it'll complete before my talk actually","date":"2013-09-03T20:17:20.345Z","type":"message"}
{"nick":"Acconut","reason":"Quit: Acconut","date":"2013-09-03T20:17:23.702Z","type":"quit"}
{"nick":"thlorenz","message":"mikeal: :) it's for valuepack  I need all that data to make decisions about how good a package is","date":"2013-09-03T20:17:27.773Z","type":"message"}
{"nick":"thlorenz","message":"mikeal: funny thing is I blamed request since in the heapdump all those ClientRequests were retained","date":"2013-09-03T20:17:59.638Z","type":"message"}
{"nick":"mikeal","message":"i've identfified a little over 3M commits in the larger node community","date":"2013-09-03T20:18:17.608Z","type":"message"}
{"nick":"thlorenz","message":"so I switched to hyperquest, but then realized that they were retained due to level being backed up ;)","date":"2013-09-03T20:18:26.324Z","type":"message"}
{"nick":"mikeal","message":"getting the basic commit data you can get 30 in a response","date":"2013-09-03T20:18:38.966Z","type":"message"}
{"nick":"mikeal","message":"but commit details are a request per commit","date":"2013-09-03T20:18:45.342Z","type":"message"}
{"nick":"thlorenz","message":"so not request's fault as far as I can tell","date":"2013-09-03T20:18:47.517Z","type":"message"}
{"nick":"ednapiranha","reason":"Read error: Connection reset by peer","date":"2013-09-03T20:18:49.175Z","type":"quit"}
{"nick":"mikeal","message":"and i don't think it'll finish in time","date":"2013-09-03T20:18:49.970Z","type":"message"}
{"nick":"mikeal","message":"thlorenz: you should use requestdb","date":"2013-09-03T20:18:56.236Z","type":"message"}
{"nick":"ednapiranha","date":"2013-09-03T20:19:00.396Z","type":"join"}
{"nick":"mbalho","message":"thlorenz: biggest problem is probably your batch size","date":"2013-09-03T20:19:04.771Z","type":"message"}
{"nick":"mikeal","message":"it'll just make sure you never ask github for the same thing twice","date":"2013-09-03T20:19:08.003Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: but if you read that thread you'll understand all the nuance","date":"2013-09-03T20:19:15.654Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: you mean too big?","date":"2013-09-03T20:19:16.799Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: yes","date":"2013-09-03T20:19:23.760Z","type":"message"}
{"nick":"thlorenz","message":"ok will read it and look into requestdb as well","date":"2013-09-03T20:19:36.814Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: I thought the less batches you do, the better? therefore the bigger the better?","date":"2013-09-03T20:19:55.693Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: nope","date":"2013-09-03T20:20:48.062Z","type":"message"}
{"nick":"Acconut","date":"2013-09-03T20:21:26.987Z","type":"join"}
{"nick":"thlorenz","message":"mbalho: so level-batcher will chunk them properly?","date":"2013-09-03T20:21:54.717Z","type":"message"}
{"nick":"mbalho","message":"its a work in progress, but thats the idea","date":"2013-09-03T20:22:12.895Z","type":"message"}
{"nick":"thlorenz","message":"ah, ok, well I'm getting most of my data down over the course of a few hours, but there is no way I could run this as a job on some server","date":"2013-09-03T20:22:49.536Z","type":"message"}
{"nick":"mbalho","message":"what?","date":"2013-09-03T20:23:22.424Z","type":"message"}
{"nick":"thlorenz","message":"so I'm gonna move on to analyze the data for now and maybe we can sit together at nodeconf and look at some heapdumps, etc. in order to find the best solution","date":"2013-09-03T20:23:24.614Z","type":"message"}
{"nick":"mbalho","message":"oh youre talkign to mikeal","date":"2013-09-03T20:23:39.948Z","type":"message"}
{"nick":"Acconut","reason":"Client Quit","date":"2013-09-03T20:23:50.377Z","type":"quit"}
{"nick":"thlorenz","message":"to whoever is interested in helping to fix these issues ;)","date":"2013-09-03T20:23:57.447Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: the memory usage + crashing issues?","date":"2013-09-03T20:24:10.435Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: the fix is to not do huge batches","date":"2013-09-03T20:24:18.115Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: also upgrade to the newest level release, there were memory leak fixes a couple of days ago","date":"2013-09-03T20:24:47.819Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: so really just split one up into say ten and batch them right after another?","date":"2013-09-03T20:24:56.445Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: dont just pick random values, do it based on the write buffer size","date":"2013-09-03T20:25:18.669Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: of course, i.e. try to get as close to some ideal buffer size -- which is what btw?","date":"2013-09-03T20:26:12.726Z","type":"message"}
{"nick":"mbalho","message":"the size is less important than making sure your batches arent bigger than the size","date":"2013-09-03T20:27:09.029Z","type":"message"}
{"nick":"thlorenz","message":"runnning leveldown 0.8.0 should I upgrade to 0.8.2? (actually I was using leveldown-hyper 0.8.2)","date":"2013-09-03T20:27:19.908Z","type":"message"}
{"nick":"mbalho","message":"whatever newest level uses","date":"2013-09-03T20:27:32.435Z","type":"message"}
{"nick":"thlorenz","message":"ok","date":"2013-09-03T20:27:35.368Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: yeah, I'm on those versions","date":"2013-09-03T20:28:24.500Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: still not clear which size my batches should not exceed","date":"2013-09-03T20:28:56.202Z","type":"message"}
{"nick":"mbalho","message":"the write buffer size","date":"2013-09-03T20:29:13.091Z","type":"message"}
{"nick":"mbalho","message":"https://github.com/rvagg/node-leveldown/#leveldownopenoptions-callback","date":"2013-09-03T20:30:22.583Z","type":"message"}
{"nick":"kenansulayman","date":"2013-09-03T20:30:46.430Z","type":"join"}
{"nick":"thlorenz","message":"mbalho: thanks got it","date":"2013-09-03T20:30:56.916Z","type":"message"}
{"nick":"Acconut","date":"2013-09-03T20:34:37.190Z","type":"join"}
{"nick":"thlorenz","message":"mikeal: btw the lastModified github things only work moderately","date":"2013-09-03T20:36:37.902Z","type":"message"}
{"nick":"thlorenz","message":"I feel like I'm getting modified when I know it wasn't, so I end up pulling down lots of data I already pulled during a previous run :(","date":"2013-09-03T20:37:23.636Z","type":"message"}
{"nick":"thlorenz","message":"works good enough to pass it along though - still saves some requests at least ;)(","date":"2013-09-03T20:38:05.232Z","type":"message"}
{"nick":"thlorenz","message":"mikeal: what are you using to follow the paged data? I wrote https://github.com/thlorenz/request-all-pages for that","date":"2013-09-03T20:43:36.537Z","type":"message"}
{"nick":"thlorenz","message":"really annoying you can get max 100 only and each time it costs you a request","date":"2013-09-03T20:43:57.180Z","type":"message"}
{"nick":"wolfeidau","reason":"Remote host closed the connection","date":"2013-09-03T20:44:18.073Z","type":"quit"}
{"nick":"wolfeidau","date":"2013-09-03T20:44:26.567Z","type":"join"}
{"nick":"kenansulayman","reason":"Quit: ≈ and thus my mac took a subtle yet profound nap ≈","date":"2013-09-03T20:51:37.453Z","type":"quit"}
{"nick":"Acconut","reason":"Quit: Acconut","date":"2013-09-03T20:55:18.474Z","type":"quit"}
{"nick":"mikeal","message":"thlorenz: i'm not doing lastModified, i'm literally holding the whole response and returning it if i have it.","date":"2013-09-03T21:03:21.103Z","type":"message"}
{"nick":"thlorenz","message":"mikeal: you mean you to ETAG?","date":"2013-09-03T21:04:39.523Z","type":"message"}
{"nick":"thlorenz","message":"cause I tried that before I think with even worse results and it doesn't work on paged data (unless you do it per page which gets messy)","date":"2013-09-03T21:05:15.948Z","type":"message"}
{"nick":"mikeal","message":"thlorenz: https://gist.github.com/mikeal/6429576","date":"2013-09-03T21:05:41.999Z","type":"message"}
{"nick":"mikeal","message":"thlorenz: no no no, requestdb stores the response, i never make the same request twice, if i got a response, ever, i just return that.","date":"2013-09-03T21:06:05.747Z","type":"message"}
{"nick":"ryan_ramage","reason":"Quit: ryan_ramage","date":"2013-09-03T21:06:21.839Z","type":"quit"}
{"nick":"thlorenz","message":"mikeal: well I have to make same request twice to update my data, i.e. if someone starred another repo","date":"2013-09-03T21:06:32.616Z","type":"message"}
{"nick":"thlorenz","message":"like a day later or so","date":"2013-09-03T21:06:40.374Z","type":"message"}
{"nick":"mikeal","message":"right, i'm not doing any of those yet","date":"2013-09-03T21:06:45.216Z","type":"message"}
{"nick":"mikeal","message":"i'm going to time bound all my data to exclude the last month anyway","date":"2013-09-03T21:07:05.186Z","type":"message"}
{"nick":"mikeal","message":"requestdb needs an option to re-fetch using cache headers","date":"2013-09-03T21:07:37.591Z","type":"message"}
{"nick":"mikeal","message":"but some things you shouldn't use it on","date":"2013-09-03T21:07:42.144Z","type":"message"}
{"nick":"thlorenz","message":"ah, that makes sense, but I wanna stay more up to date, otherwise people will not get the results they expect","date":"2013-09-03T21:07:47.287Z","type":"message"}
{"nick":"mikeal","message":"like urls to git commits","date":"2013-09-03T21:07:47.523Z","type":"message"}
{"nick":"thlorenz","message":"mikeal: I'm munging data before I store it (i.e. remove things I don't need), so I don't think requestdb as it is would work for me","date":"2013-09-03T21:08:32.532Z","type":"message"}
{"nick":"thlorenz","message":"I also index stuff while I store it","date":"2013-09-03T21:08:50.003Z","type":"message"}
{"nick":"mikeal","message":"i do both","date":"2013-09-03T21:09:57.284Z","type":"message"}
{"nick":"mikeal","message":"i use requestdb to get data, but then i store what i want in a secondary store","date":"2013-09-03T21:10:10.183Z","type":"message"}
{"nick":"mikeal","message":"that way, if i want to build a new dataset from requests i've already made i can do it very quickly","date":"2013-09-03T21:10:26.599Z","type":"message"}
{"nick":"mikeal","message":"i don't use requestdb as my primary store, i just use it as a drop-in replacement for request to get the data","date":"2013-09-03T21:10:50.399Z","type":"message"}
{"nick":"jcrugzz","date":"2013-09-03T21:23:54.160Z","type":"join"}
{"nick":"thlorenz","message":"mikeal: that makes a lot of sense actually","date":"2013-09-03T21:26:12.198Z","type":"message"}
{"nick":"thlorenz","message":"I had figured that my bottleneck would be github and the requests, so munging data in the meantime made sense","date":"2013-09-03T21:27:00.169Z","type":"message"}
{"nick":"thlorenz","message":"now level turns out to be my bottleneck :)","date":"2013-09-03T21:27:13.619Z","type":"message"}
{"nick":"mikeal","message":"it all depends","date":"2013-09-03T21:29:10.105Z","type":"message"}
{"nick":"mikeal","message":"i hit write limitations when 1 github request equalled 30 level writes","date":"2013-09-03T21:29:23.867Z","type":"message"}
{"nick":"mikeal","message":"that was enough, over time, to overload it","date":"2013-09-03T21:29:34.426Z","type":"message"}
{"nick":"mikeal","message":"but now, i'm averaging like 2 requests a second","date":"2013-09-03T21:29:52.581Z","type":"message"}
{"nick":"mikeal","message":"and each one is only a single write, so github is my bottleneck :)","date":"2013-09-03T21:30:04.207Z","type":"message"}
{"nick":"mikeal","message":"i hit some kinda special rate limiting","date":"2013-09-03T21:30:22.233Z","type":"message"}
{"nick":"mikeal","message":"they are actually restricting the number of requests they let in from me","date":"2013-09-03T21:30:36.576Z","type":"message"}
{"nick":"ryan_ramage","date":"2013-09-03T21:34:33.349Z","type":"join"}
{"nick":"dguttman","date":"2013-09-03T21:55:56.122Z","type":"join"}
{"nick":"ryan_ramage","reason":"Quit: ryan_ramage","date":"2013-09-03T22:03:22.925Z","type":"quit"}
{"nick":"esundahl","reason":"Remote host closed the connection","date":"2013-09-03T22:08:28.217Z","type":"quit"}
{"nick":"esundahl","date":"2013-09-03T22:08:54.846Z","type":"join"}
{"nick":"esundahl_","date":"2013-09-03T22:10:05.525Z","type":"join"}
{"nick":"esundahl","reason":"Ping timeout: 276 seconds","date":"2013-09-03T22:13:33.629Z","type":"quit"}
{"nick":"jcrugzz","reason":"Ping timeout: 260 seconds","date":"2013-09-03T22:19:08.123Z","type":"quit"}
{"nick":"thlorenz","reason":"Remote host closed the connection","date":"2013-09-03T22:27:52.561Z","type":"quit"}
{"nick":"disordinary","date":"2013-09-03T22:30:51.408Z","type":"join"}
{"nick":"levelbot","message":"[npm] tik@0.0.6 <http://npm.im/tik>: command line key/value store (@jarofghosts)","date":"2013-09-03T22:44:19.612Z","type":"message"}
{"nick":"thlorenz","date":"2013-09-03T22:45:36.998Z","type":"join"}
{"nick":"mikeal","message":"if you write an identical key/value is level smart enough to disregard it?","date":"2013-09-03T22:55:59.546Z","type":"message"}
{"nick":"mbalho","message":"it will override","date":"2013-09-03T22:56:34.320Z","type":"message"}
{"nick":"thlorenz","message":"mikeal: I'd say it would overwrite it","date":"2013-09-03T22:56:36.826Z","type":"message"}
{"nick":"thlorenz","message":":)","date":"2013-09-03T22:56:39.364Z","type":"message"}
{"nick":"mikeal","message":"overwrite it with the exact same data?","date":"2013-09-03T22:56:48.010Z","type":"message"}
{"nick":"mbalho","message":"yea","date":"2013-09-03T22:57:05.421Z","type":"message"}
{"nick":"mikeal","message":"ok","date":"2013-09-03T22:57:19.863Z","type":"message"}
{"nick":"mikeal","message":"so if there's a high probability of writing same data twice","date":"2013-09-03T22:57:32.972Z","type":"message"}
{"nick":"brycebaril","message":"that's a good question, given the compaction stuff it might be an optimization they considered","date":"2013-09-03T22:57:39.570Z","type":"message"}
{"nick":"mikeal","message":"it's faster to read-before-write","date":"2013-09-03T22:57:40.762Z","type":"message"}
{"nick":"mikeal","message":"easy enough to write a benchmark for","date":"2013-09-03T22:58:08.550Z","type":"message"}
{"nick":"brycebaril","message":"I bet rescrv might know","date":"2013-09-03T22:58:23.258Z","type":"message"}
{"nick":"rescrv","message":"mikeal: it depends how much data is in the \"hot\" set.  If you're likely to write the same key twice within a write buffer, don't bother doing the read-before-write.","date":"2013-09-03T22:58:51.636Z","type":"message"}
{"nick":"mikeal","message":"hrm....","date":"2013-09-03T22:59:07.107Z","type":"message"}
{"nick":"mikeal","message":"what is the scope of the write buffer?","date":"2013-09-03T22:59:21.122Z","type":"message"}
{"nick":"mikeal","message":"like, the last hundred keys? thousand keys?","date":"2013-09-03T22:59:27.629Z","type":"message"}
{"nick":"rescrv","message":"I'm sure there's a sweet spot where read-before-write will win, but for extremely large datasets (10x ram), and extremely skewed datasets, you should just write and not try to read before write.","date":"2013-09-03T22:59:41.633Z","type":"message"}
{"nick":"rescrv","message":"mikeal: the last write_buffer/avg-key-size keys","date":"2013-09-03T22:59:59.386Z","type":"message"}
{"nick":"mikeal","message":"my keys tend to be large since they are binary","date":"2013-09-03T23:00:16.582Z","type":"message"}
{"nick":"mikeal","message":"with this i could get away with strings tho","date":"2013-09-03T23:00:33.623Z","type":"message"}
{"nick":"mikeal","message":"i'm writing a set implementation","date":"2013-09-03T23:00:46.327Z","type":"message"}
{"nick":"mikeal","message":"kind of","date":"2013-09-03T23:00:48.004Z","type":"message"}
{"nick":"levelbot","message":"[npm] valuepack-core@0.3.14 <http://npm.im/valuepack-core>: Core utils and configurations for valuepack, not at all useful by itself. (@thlorenz)","date":"2013-09-03T23:00:52.887Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: btw I was wondering if I use json encoding for my values, how would I know what size my batch will have?","date":"2013-09-03T23:01:07.826Z","type":"message"}
{"nick":"ednapiranha","reason":"Remote host closed the connection","date":"2013-09-03T23:01:16.186Z","type":"quit"}
{"nick":"rescrv","message":"mikeal: how often are you overwriting things?","date":"2013-09-03T23:01:54.674Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: https://github.com/maxogden/level-batcher/blob/master/index.js#L51","date":"2013-09-03T23:02:00.633Z","type":"message"}
{"nick":"thlorenz","message":"thanks :)","date":"2013-09-03T23:02:19.929Z","type":"message"}
{"nick":"mikeal","message":"well, with a set you can't put the same thing in twice","date":"2013-09-03T23:02:32.836Z","type":"message"}
{"nick":"mikeal","message":"so the way i was thinking of working it is to encode the bucket and value in to the key","date":"2013-09-03T23:02:49.874Z","type":"message"}
{"nick":"mikeal","message":"so [\"bucket\", \"key1\"]","date":"2013-09-03T23:03:04.794Z","type":"message"}
{"nick":"mikeal","message":"no value","date":"2013-09-03T23:03:08.335Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: this https://github.com/maxogden/level-batcher/blob/master/index.js#L15 LOL :)","date":"2013-09-03T23:03:25.827Z","type":"message"}
{"nick":"mikeal","message":"the easiest thing to do would be to just call write() no matter what","date":"2013-09-03T23:03:40.441Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: ;)","date":"2013-09-03T23:03:51.423Z","type":"message"}
{"nick":"brycebaril","message":"mikeal: for a set it might be handy to do the read first anyway, e.g. with Redis sets, the return value is bool/false as to whether the member was added or not.","date":"2013-09-03T23:04:01.672Z","type":"message"}
{"nick":"rescrv","message":"so you want to take a 2-tuple and insert it into a set iff it doesn't already exist; else do nothing","date":"2013-09-03T23:04:02.465Z","type":"message"}
{"nick":"mikeal","message":"good call","date":"2013-09-03T23:04:24.534Z","type":"message"}
{"nick":"rescrv","message":"brycebaril: unless he's doing overwrites often within a short time frame, or his set is so small he could keep it in memory, or he wants the true/false semantics, he'd be best to just \"write\"","date":"2013-09-03T23:04:47.354Z","type":"message"}
{"nick":"mikeal","message":"i can optimize the reads in a short time frame just as easily tho","date":"2013-09-03T23:05:34.975Z","type":"message"}
{"nick":"brycebaril","message":"right, the true/false thing would be a feature that might be useful enough to pay the cost of the read","date":"2013-09-03T23:05:36.515Z","type":"message"}
{"nick":"mikeal","message":"in fact, i could just keep an LRU around","date":"2013-09-03T23:05:42.333Z","type":"message"}
{"nick":"rescrv","message":"mikeal: level does that by default (sort-of)","date":"2013-09-03T23:05:55.908Z","type":"message"}
{"nick":"mikeal","message":"that would be faster than relying on level to optimize the noop writes","date":"2013-09-03T23:05:56.345Z","type":"message"}
{"nick":"rescrv","message":"under some situations, yes","date":"2013-09-03T23:06:32.447Z","type":"message"}
{"nick":"rescrv","message":"under others, no","date":"2013-09-03T23:06:36.311Z","type":"message"}
{"nick":"rescrv","message":"most workloads I use leveldb for, the answer would be no","date":"2013-09-03T23:06:47.215Z","type":"message"}
{"nick":"rescrv","message":"unless you have a very skewed distribution","date":"2013-09-03T23:07:12.775Z","type":"message"}
{"nick":"mikeal","message":"the key distribution is basically random","date":"2013-09-03T23:07:26.026Z","type":"message"}
{"nick":"rescrv","message":"then an LRU won't take much off the top","date":"2013-09-03T23:07:37.686Z","type":"message"}
{"nick":"mikeal","message":"it's not predictable","date":"2013-09-03T23:07:38.154Z","type":"message"}
{"nick":"mikeal","message":"umn, no, that's not accurate","date":"2013-09-03T23:07:58.102Z","type":"message"}
{"nick":"mikeal","message":"some keys will show up more often than others, but the sorting of those keys are going to be wide","date":"2013-09-03T23:08:17.138Z","type":"message"}
{"nick":"mikeal","message":"that's what i meant by the distribution being random","date":"2013-09-03T23:08:31.808Z","type":"message"}
{"nick":"brycebaril","message":"how would a LRU interact with multilevel? that could be problematic","date":"2013-09-03T23:08:34.846Z","type":"message"}
{"nick":"rescrv","message":"mikeal: I read \"uniformly random\".  Ugh, too much staring at a monitor","date":"2013-09-03T23:08:49.017Z","type":"message"}
{"nick":"mikeal","message":"my assumption is always that within the scope of my object i'm the only person manipulating whatever i need a consistent state on","date":"2013-09-03T23:09:28.523Z","type":"message"}
{"nick":"mikeal","message":"everyone must go through me","date":"2013-09-03T23:09:41.219Z","type":"message"}
{"nick":"rescrv","message":"that's a good assumption","date":"2013-09-03T23:09:52.534Z","type":"message"}
{"nick":"mikeal","message":"that's the only way that level-mutex can actually guarantee anything","date":"2013-09-03T23:09:55.073Z","type":"message"}
{"nick":"dominictarr","reason":"Quit: dominictarr","date":"2013-09-03T23:10:30.529Z","type":"quit"}
{"nick":"mikeal","message":"you can stick multiple mutexes around a level instance, the assumption is that they are managing individual and separate consistency guarantees.","date":"2013-09-03T23:10:33.690Z","type":"message"}
{"nick":"brycebaril","message":"ahh, ok, I had the impression you were making a more generic level-set implementation that people might want to use in a broader sense","date":"2013-09-03T23:10:34.814Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: do I also have to ensure I only got one batch going at a time, i.e do them in series?","date":"2013-09-03T23:10:36.695Z","type":"message"}
{"nick":"mikeal","message":"you could use it to store data in sets :)","date":"2013-09-03T23:10:49.931Z","type":"message"}
{"nick":"mikeal","message":"i'm not entirely sold on mutli-level TBH","date":"2013-09-03T23:11:00.334Z","type":"message"}
{"nick":"mikeal","message":"it only really works if you don't have a data model above the level layer","date":"2013-09-03T23:11:44.150Z","type":"message"}
{"nick":"mbalho","message":"thlorenz: not 100% sure but i think so","date":"2013-09-03T23:11:47.059Z","type":"message"}
{"nick":"mikeal","message":"cause almost any data model will need a consistency scheme","date":"2013-09-03T23:12:02.442Z","type":"message"}
{"nick":"fallsemo","reason":"Quit: Leaving.","date":"2013-09-03T23:12:08.637Z","type":"quit"}
{"nick":"mikeal","message":"that's why all of my modules expose data and transfer it over SLEEP","date":"2013-09-03T23:12:30.833Z","type":"message"}
{"nick":"thlorenz","message":"mbalho: ah, you'd think leveldown would do this for you if that is the case - maybe it is rvagg do you know anything about that?","date":"2013-09-03T23:13:24.369Z","type":"message"}
{"nick":"fallsemo","date":"2013-09-03T23:13:34.601Z","type":"join"}
{"nick":"mbalho","message":"thlorenz: leveldown isnt optimized for this stuff yet","date":"2013-09-03T23:14:04.244Z","type":"message"}
{"nick":"thlorenz","message":"ok, thanks - I'll keep that in mind","date":"2013-09-03T23:14:37.012Z","type":"message"}
{"nick":"rescrv","message":"what consistency guarantees are you guys looking for?","date":"2013-09-03T23:15:00.917Z","type":"message"}
{"nick":"mikeal","message":"i think i'm going to start using sublevel for my internal slicing instead of bytewise slices","date":"2013-09-03T23:15:17.497Z","type":"message"}
{"nick":"tmcw","reason":"Remote host closed the connection","date":"2013-09-03T23:17:49.308Z","type":"quit"}
{"nick":"tmcw","date":"2013-09-03T23:18:22.080Z","type":"join"}
{"nick":"mikeal","message":"if i don't care about the value i'm setting, what is the most efficient one to use?","date":"2013-09-03T23:18:52.218Z","type":"message"}
{"nick":"disordinary","reason":"Quit: Konversation terminated!","date":"2013-09-03T23:22:35.309Z","type":"quit"}
{"nick":"tmcw","reason":"Ping timeout: 248 seconds","date":"2013-09-03T23:22:45.723Z","type":"quit"}
{"nick":"jcrugzz","date":"2013-09-03T23:24:54.798Z","type":"join"}
{"nick":"soldair","date":"2013-09-03T23:27:10.339Z","type":"join"}
{"nick":"esundahl_","reason":"Remote host closed the connection","date":"2013-09-03T23:28:15.711Z","type":"quit"}
{"nick":"esundahl","date":"2013-09-03T23:28:42.926Z","type":"join"}
{"nick":"jcrugzz","reason":"Ping timeout: 276 seconds","date":"2013-09-03T23:29:36.559Z","type":"quit"}
{"nick":"levelbot","message":"[npm] level-batcher@0.0.2 <http://npm.im/level-batcher>: stream designed for leveldb that you write objects to, and it emits batches of objects that are under a byte size limit (@maxogden)","date":"2013-09-03T23:29:49.026Z","type":"message"}
{"nick":"mbalho","message":"just updated it to always write batches when any buffered data exists","date":"2013-09-03T23:30:27.107Z","type":"message"}
{"nick":"esundahl","reason":"Ping timeout: 248 seconds","date":"2013-09-03T23:32:53.744Z","type":"quit"}
{"nick":"rickbergfalk","reason":"Quit: http://www.kiwiirc.com/ - A hand crafted IRC client","date":"2013-09-03T23:34:15.902Z","type":"quit"}
{"nick":"mbalho","message":"rvagg: throw this on your todo list after lmdb https://github.com/spotify/sparkey/blob/master/README.md#performance","date":"2013-09-03T23:35:02.585Z","type":"message"}
{"nick":"mikeal","reason":"Quit: Leaving.","date":"2013-09-03T23:35:54.206Z","type":"quit"}
{"nick":"mbalho","message":"rvagg: also https://github.com/bureau14/leveldb","date":"2013-09-03T23:36:09.983Z","type":"message"}
{"nick":"mikeal","date":"2013-09-03T23:37:37.203Z","type":"join"}
{"nick":"rescrv","message":"mbalho: does bureau14's fork do anything more than windows support?","date":"2013-09-03T23:40:19.082Z","type":"message"}
{"nick":"mbalho","message":"rescrv: not sure","date":"2013-09-03T23:41:28.477Z","type":"message"}
{"nick":"mbalho","message":"lol \"Is not 100% compliant with ugly Google coding style;\"","date":"2013-09-03T23:42:17.376Z","type":"message"}
{"nick":"fallsemo","reason":"Quit: Leaving.","date":"2013-09-03T23:42:19.774Z","type":"quit"}
{"nick":"rescrv","message":"it sounds to me like they added windows support and decided to change the code to C++11 for the cargo-cult of it.  The description doesn't say why they changed things, except for things like adding c++11 support (which doesn't actually change the function and likely doesn't make it faster).","date":"2013-09-03T23:47:09.601Z","type":"message"}
{"nick":"levelbot","message":"[npm] dat@0.2.1 <http://npm.im/dat>: data sharing and replication tool (@maxogden)","date":"2013-09-03T23:59:20.126Z","type":"message"}
